<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/project/data/load_cohort.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/project/data/load_cohort.py" />
              <option name="updatedContent" value="import pandas as pd&#10;&#10;# Path to the initial cohort file&#10;data_path = &quot;./data/initial_cohort.csv&quot;&#10;&#10;def load_initial_cohort(path):&#10;    &quot;&quot;&quot;Load the initial cohort subject IDs.&quot;&quot;&quot;&#10;    return pd.read_csv(path)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    cohort = load_initial_cohort(data_path)&#10;    print(f&quot;Loaded {len(cohort)} patients from initial cohort.&quot;)&#10;    print(cohort.head())&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/project/load_cohort.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/project/load_cohort.py" />
              <option name="updatedContent" value="import pandas as pd&#10;&#10;DATA_PATH = &quot;./data/initial_cohort.csv&quot;&#10;&#10;def load_initial_cohort(path=DATA_PATH):&#10;    &quot;&quot;&quot;Load the initial cohort subject IDs.&quot;&quot;&quot;&#10;    return pd.read_csv(path)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    cohort = load_initial_cohort()&#10;    print(f&quot;Loaded {len(cohort)} patients from initial cohort.&quot;)&#10;    print(cohort.head())&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/project/mlhc_final_project.ipynb">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/project/mlhc_final_project.ipynb" />
              <option name="originalContent" value="{&#10; &quot;cells&quot;: [&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;# MLHC Final Project: ICU Outcome Prediction\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;## Objective\n&quot;,&#10;    &quot;Build prediction models for three ICU outcomes using MIMIC-III data:\n&quot;,&#10;    &quot;1. **Mortality** - Death during hospitalization or within 30 days after discharge\n&quot;,&#10;    &quot;2. **Prolonged Stay** - Length of stay &gt; 7 days\n&quot;,&#10;    &quot;3. **Hospital Readmission** - Readmission within 30 days after discharge\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Data Window**: First 48 hours of first hospital admission (with 6-hour gap)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Cohort**: Patients from initial_cohort.csv only&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 1. Setup and Imports&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Core libraries\n&quot;,&#10;    &quot;import pandas as pd\n&quot;,&#10;    &quot;import numpy as np\n&quot;,&#10;    &quot;import matplotlib.pyplot as plt\n&quot;,&#10;    &quot;import seaborn as sns\n&quot;,&#10;    &quot;from datetime import datetime, timedelta\n&quot;,&#10;    &quot;import warnings\n&quot;,&#10;    &quot;warnings.filterwarnings('ignore')\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Machine learning libraries\n&quot;,&#10;    &quot;from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n&quot;,&#10;    &quot;from sklearn.preprocessing import StandardScaler, LabelEncoder\n&quot;,&#10;    &quot;from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n&quot;,&#10;    &quot;from sklearn.linear_model import LogisticRegression\n&quot;,&#10;    &quot;from sklearn.metrics import (\n&quot;,&#10;    &quot;    accuracy_score, precision_score, recall_score, f1_score, \n&quot;,&#10;    &quot;    roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix,\n&quot;,&#10;    &quot;    classification_report\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;from sklearn.impute import SimpleImputer\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Set display options\n&quot;,&#10;    &quot;pd.set_option('display.max_columns', None)\n&quot;,&#10;    &quot;pd.set_option('display.max_rows', 100)\n&quot;,&#10;    &quot;plt.style.use('seaborn-v0_8')\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Random seed for reproducibility\n&quot;,&#10;    &quot;RANDOM_STATE = 42\n&quot;,&#10;    &quot;np.random.seed(RANDOM_STATE)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 2. Configuration and Constants&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# File paths\n&quot;,&#10;    &quot;DATA_PATH = \&quot;./data/\&quot;\n&quot;,&#10;    &quot;COHORT_FILE = DATA_PATH + \&quot;initial_cohort.csv\&quot;\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Time windows (in hours)\n&quot;,&#10;    &quot;PREDICTION_WINDOW = 48  # First 48 hours\n&quot;,&#10;    &quot;GAP_HOURS = 6  # 6-hour gap before prediction\n&quot;,&#10;    &quot;EFFECTIVE_WINDOW = PREDICTION_WINDOW - GAP_HOURS  # 42 hours of actual data\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Outcome definitions\n&quot;,&#10;    &quot;PROLONGED_STAY_THRESHOLD = 7  # days\n&quot;,&#10;    &quot;READMISSION_WINDOW = 30  # days\n&quot;,&#10;    &quot;MORTALITY_WINDOW = 30  # days post-discharge\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Model parameters\n&quot;,&#10;    &quot;TEST_SIZE = 0.2\n&quot;,&#10;    &quot;VALIDATION_SIZE = 0.2\n&quot;,&#10;    &quot;CV_FOLDS = 5\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;Configuration loaded:\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;- Effective data window: {EFFECTIVE_WINDOW} hours\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;- Prolonged stay threshold: {PROLONGED_STAY_THRESHOLD} days\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;- Readmission window: {READMISSION_WINDOW} days\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;- Mortality window: {MORTALITY_WINDOW} days post-discharge\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 3. Data Loading and Initial Exploration&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def load_initial_cohort(filepath=COHORT_FILE):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Load the initial cohort of patients.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    try:\n&quot;,&#10;    &quot;        cohort = pd.read_csv(filepath)\n&quot;,&#10;    &quot;        print(f\&quot;Successfully loaded {len(cohort)} patients from initial cohort\&quot;)\n&quot;,&#10;    &quot;        return cohort\n&quot;,&#10;    &quot;    except FileNotFoundError:\n&quot;,&#10;    &quot;        print(f\&quot;Error: File {filepath} not found\&quot;)\n&quot;,&#10;    &quot;        return None\n&quot;,&#10;    &quot;    except Exception as e:\n&quot;,&#10;    &quot;        print(f\&quot;Error loading cohort: {e}\&quot;)\n&quot;,&#10;    &quot;        return None\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Load initial cohort\n&quot;,&#10;    &quot;cohort_df = load_initial_cohort()\n&quot;,&#10;    &quot;if cohort_df is not None:\n&quot;,&#10;    &quot;    print(f\&quot;\\nCohort shape: {cohort_df.shape}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Columns: {list(cohort_df.columns)}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;\\nFirst 10 subject IDs:\&quot;)\n&quot;,&#10;    &quot;    print(cohort_df.head(10))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Get list of subject IDs for filtering\n&quot;,&#10;    &quot;    subject_ids = cohort_df['subject_id'].tolist()\n&quot;,&#10;    &quot;    print(f\&quot;\\nTotal subjects in cohort: {len(subject_ids)}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 4. Data Loading Functions\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Note**: These functions assume you have access to MIMIC-III tables. You'll need to adapt the data loading based on your actual data source (CSV files, database, etc.).&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def load_mimic_data():\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    Load MIMIC-III data tables.\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    TODO: Adapt this function based on your actual data source:\n&quot;,&#10;    &quot;    - If you have CSV files, load them with pd.read_csv()\n&quot;,&#10;    &quot;    - If you have a database connection, use SQL queries\n&quot;,&#10;    &quot;    - If you have a different format, adapt accordingly\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Example structure - adapt to your data source\n&quot;,&#10;    &quot;    try:\n&quot;,&#10;    &quot;        # Core tables needed for the project\n&quot;,&#10;    &quot;        tables = {\n&quot;,&#10;    &quot;            'admissions': None,     # Hospital admissions\n&quot;,&#10;    &quot;            'icustays': None,       # ICU stays\n&quot;,&#10;    &quot;            'patients': None,       # Patient demographics\n&quot;,&#10;    &quot;            'chartevents': None,    # Charted vital signs, lab values\n&quot;,&#10;    &quot;            'labevents': None,      # Laboratory measurements\n&quot;,&#10;    &quot;            'prescriptions': None,  # Medication prescriptions\n&quot;,&#10;    &quot;            'procedures_icd': None, # Procedures\n&quot;,&#10;    &quot;            'diagnoses_icd': None,  # Diagnoses\n&quot;,&#10;    &quot;        }\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        # TODO: Replace with actual data loading\n&quot;,&#10;    &quot;        # Example for CSV files:\n&quot;,&#10;    &quot;        # tables['admissions'] = pd.read_csv(DATA_PATH + 'ADMISSIONS.csv')\n&quot;,&#10;    &quot;        # tables['icustays'] = pd.read_csv(DATA_PATH + 'ICUSTAYS.csv')\n&quot;,&#10;    &quot;        # etc.\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        print(\&quot;Data loading function ready - please implement based on your data source\&quot;)\n&quot;,&#10;    &quot;        return tables\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;    except Exception as e:\n&quot;,&#10;    &quot;        print(f\&quot;Error loading MIMIC data: {e}\&quot;)\n&quot;,&#10;    &quot;        return None\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Placeholder for data loading\n&quot;,&#10;    &quot;print(\&quot;Data loading functions defined\&quot;)\n&quot;,&#10;    &quot;print(\&quot;TODO: Implement load_mimic_data() based on your actual data source\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 5. Data Preprocessing and Feature Engineering&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def filter_to_cohort(df, cohort_subject_ids, subject_col='subject_id'):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Filter dataframe to only include patients in our cohort.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    if subject_col not in df.columns:\n&quot;,&#10;    &quot;        print(f\&quot;Warning: {subject_col} not found in dataframe columns\&quot;)\n&quot;,&#10;    &quot;        return df\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    initial_count = len(df)\n&quot;,&#10;    &quot;    filtered_df = df[df[subject_col].isin(cohort_subject_ids)]\n&quot;,&#10;    &quot;    final_count = len(filtered_df)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;Filtered from {initial_count} to {final_count} rows\&quot;)\n&quot;,&#10;    &quot;    return filtered_df\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def get_first_admission_time(admissions_df, subject_ids):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Get the first admission time for each patient in our cohort.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    # Filter to cohort patients\n&quot;,&#10;    &quot;    cohort_admissions = filter_to_cohort(admissions_df, subject_ids)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Convert admission time to datetime\n&quot;,&#10;    &quot;    cohort_admissions['admittime'] = pd.to_datetime(cohort_admissions['admittime'])\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Get first admission for each patient\n&quot;,&#10;    &quot;    first_admissions = cohort_admissions.groupby('subject_id')['admittime'].min().reset_index()\n&quot;,&#10;    &quot;    first_admissions.columns = ['subject_id', 'first_admittime']\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return first_admissions\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def extract_time_window_data(df, first_admissions, time_col, window_hours=EFFECTIVE_WINDOW):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Extract data within the specified time window from first admission.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    # Merge with first admission times\n&quot;,&#10;    &quot;    df_with_admit = df.merge(first_admissions, on='subject_id', how='inner')\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Convert time column to datetime\n&quot;,&#10;    &quot;    df_with_admit[time_col] = pd.to_datetime(df_with_admit[time_col])\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Calculate time from admission\n&quot;,&#10;    &quot;    df_with_admit['hours_from_admit'] = (\n&quot;,&#10;    &quot;        df_with_admit[time_col] - df_with_admit['first_admittime']\n&quot;,&#10;    &quot;    ).dt.total_seconds() / 3600\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Filter to time window (0 to window_hours)\n&quot;,&#10;    &quot;    window_data = df_with_admit[\n&quot;,&#10;    &quot;        (df_with_admit['hours_from_admit'] &gt;= 0) &amp; \n&quot;,&#10;    &quot;        (df_with_admit['hours_from_admit'] &lt;= window_hours)\n&quot;,&#10;    &quot;    ]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;Extracted {len(window_data)} records within {window_hours}-hour window\&quot;)\n&quot;,&#10;    &quot;    return window_data\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Data preprocessing functions defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 6. Feature Engineering Functions&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def create_demographic_features(patients_df, admissions_df, subject_ids):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Create demographic features.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    # Filter to cohort\n&quot;,&#10;    &quot;    patients = filter_to_cohort(patients_df, subject_ids)\n&quot;,&#10;    &quot;    admissions = filter_to_cohort(admissions_df, subject_ids)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Calculate age at admission\n&quot;,&#10;    &quot;    patients['dob'] = pd.to_datetime(patients['dob'])\n&quot;,&#10;    &quot;    admissions['admittime'] = pd.to_datetime(admissions['admittime'])\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Get first admission for each patient\n&quot;,&#10;    &quot;    first_admissions = admissions.groupby('subject_id')['admittime'].min().reset_index()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Merge and calculate age\n&quot;,&#10;    &quot;    demo_features = patients.merge(first_admissions, on='subject_id')\n&quot;,&#10;    &quot;    demo_features['age_at_admission'] = (\n&quot;,&#10;    &quot;        demo_features['admittime'] - demo_features['dob']\n&quot;,&#10;    &quot;    ).dt.days / 365.25\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Create gender dummy\n&quot;,&#10;    &quot;    demo_features['gender_M'] = (demo_features['gender'] == 'M').astype(int)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Select final features\n&quot;,&#10;    &quot;    features = demo_features[['subject_id', 'age_at_admission', 'gender_M']]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return features\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def create_vital_signs_features(chartevents_df, first_admissions):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Create vital signs features from chartevents.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    # Extract vital signs within time window\n&quot;,&#10;    &quot;    vitals_window = extract_time_window_data(\n&quot;,&#10;    &quot;        chartevents_df, first_admissions, 'charttime'\n&quot;,&#10;    &quot;    )\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Common vital signs item IDs (adapt based on your data)\n&quot;,&#10;    &quot;    vital_items = {\n&quot;,&#10;    &quot;        'heart_rate': [211, 220045],\n&quot;,&#10;    &quot;        'systolic_bp': [51, 442, 455, 6701, 220179, 220050],\n&quot;,&#10;    &quot;        'diastolic_bp': [8368, 8440, 8441, 8555, 220180, 220051],\n&quot;,&#10;    &quot;        'mean_bp': [52, 6702, 443, 456, 220052, 220181],\n&quot;,&#10;    &quot;        'resp_rate': [615, 618, 220210, 224690],\n&quot;,&#10;    &quot;        'temperature': [223762, 676, 223761, 678],\n&quot;,&#10;    &quot;        'spo2': [646, 220277]\n&quot;,&#10;    &quot;    }\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    vital_features = []\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    for vital_name, item_ids in vital_items.items():\n&quot;,&#10;    &quot;        vital_data = vitals_window[vitals_window['itemid'].isin(item_ids)]\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        if len(vital_data) &gt; 0:\n&quot;,&#10;    &quot;            # Convert to numeric\n&quot;,&#10;    &quot;            vital_data['valuenum'] = pd.to_numeric(vital_data['valuenum'], errors='coerce')\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Calculate statistics\n&quot;,&#10;    &quot;            vital_stats = vital_data.groupby('subject_id')['valuenum'].agg([\n&quot;,&#10;    &quot;                'mean', 'std', 'min', 'max', 'count'\n&quot;,&#10;    &quot;            ]).reset_index()\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Rename columns\n&quot;,&#10;    &quot;            vital_stats.columns = ['subject_id'] + [\n&quot;,&#10;    &quot;                f'{vital_name}_{stat}' for stat in ['mean', 'std', 'min', 'max', 'count']\n&quot;,&#10;    &quot;            ]\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            vital_features.append(vital_stats)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return vital_features\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def create_lab_features(labevents_df, first_admissions):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Create laboratory features.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    # Extract lab events within time window\n&quot;,&#10;    &quot;    labs_window = extract_time_window_data(\n&quot;,&#10;    &quot;        labevents_df, first_admissions, 'charttime'\n&quot;,&#10;    &quot;    )\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Common lab item IDs (adapt based on your data)\n&quot;,&#10;    &quot;    lab_items = {\n&quot;,&#10;    &quot;        'hemoglobin': [51222, 220228],\n&quot;,&#10;    &quot;        'hematocrit': [51221, 220615],\n&quot;,&#10;    &quot;        'platelets': [51265, 220621],\n&quot;,&#10;    &quot;        'wbc': [51301, 220546],\n&quot;,&#10;    &quot;        'creatinine': [50912, 220615],\n&quot;,&#10;    &quot;        'bun': [51006, 220546],\n&quot;,&#10;    &quot;        'glucose': [50931, 220621],\n&quot;,&#10;    &quot;        'sodium': [50983, 220615],\n&quot;,&#10;    &quot;        'potassium': [50971, 220640],\n&quot;,&#10;    &quot;        'chloride': [50902, 220602]\n&quot;,&#10;    &quot;    }\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    lab_features = []\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    for lab_name, item_ids in lab_items.items():\n&quot;,&#10;    &quot;        lab_data = labs_window[labs_window['itemid'].isin(item_ids)]\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        if len(lab_data) &gt; 0:\n&quot;,&#10;    &quot;            # Convert to numeric\n&quot;,&#10;    &quot;            lab_data['valuenum'] = pd.to_numeric(lab_data['valuenum'], errors='coerce')\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Calculate statistics\n&quot;,&#10;    &quot;            lab_stats = lab_data.groupby('subject_id')['valuenum'].agg([\n&quot;,&#10;    &quot;                'mean', 'std', 'min', 'max', 'count'\n&quot;,&#10;    &quot;            ]).reset_index()\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Rename columns\n&quot;,&#10;    &quot;            lab_stats.columns = ['subject_id'] + [\n&quot;,&#10;    &quot;                f'{lab_name}_{stat}' for stat in ['mean', 'std', 'min', 'max', 'count']\n&quot;,&#10;    &quot;            ]\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            lab_features.append(lab_stats)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return lab_features\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Feature engineering functions defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 7. Target Variable Creation&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def create_target_variables(admissions_df, patients_df, subject_ids):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Create the three target variables for prediction.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Filter to cohort patients\n&quot;,&#10;    &quot;    admissions = filter_to_cohort(admissions_df, subject_ids)\n&quot;,&#10;    &quot;    patients = filter_to_cohort(patients_df, subject_ids)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Convert datetime columns\n&quot;,&#10;    &quot;    admissions['admittime'] = pd.to_datetime(admissions['admittime'])\n&quot;,&#10;    &quot;    admissions['dischtime'] = pd.to_datetime(admissions['dischtime'])\n&quot;,&#10;    &quot;    patients['dod'] = pd.to_datetime(patients['dod'])\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Get first admission for each patient\n&quot;,&#10;    &quot;    first_admissions = admissions.groupby('subject_id').first().reset_index()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    targets = first_admissions[['subject_id', 'admittime', 'dischtime']].copy()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Merge with patient data for death dates\n&quot;,&#10;    &quot;    targets = targets.merge(patients[['subject_id', 'dod']], on='subject_id', how='left')\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # 1. Mortality target (in-hospital or within 30 days post-discharge)\n&quot;,&#10;    &quot;    targets['mortality'] = 0\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # In-hospital mortality\n&quot;,&#10;    &quot;    in_hospital_death = targets['dod'].between(\n&quot;,&#10;    &quot;        targets['admittime'], targets['dischtime']\n&quot;,&#10;    &quot;    )\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # 30-day post-discharge mortality\n&quot;,&#10;    &quot;    post_discharge_death = (\n&quot;,&#10;    &quot;        targets['dod'] &gt; targets['dischtime']\n&quot;,&#10;    &quot;    ) &amp; (\n&quot;,&#10;    &quot;        targets['dod'] &lt;= targets['dischtime'] + timedelta(days=MORTALITY_WINDOW)\n&quot;,&#10;    &quot;    )\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    targets.loc[in_hospital_death | post_discharge_death, 'mortality'] = 1\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # 2. Prolonged stay target (&gt; 7 days)\n&quot;,&#10;    &quot;    targets['los_days'] = (\n&quot;,&#10;    &quot;        targets['dischtime'] - targets['admittime']\n&quot;,&#10;    &quot;    ).dt.total_seconds() / (24 * 3600)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    targets['prolonged_stay'] = (targets['los_days'] &gt; PROLONGED_STAY_THRESHOLD).astype(int)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # 3. Readmission target (within 30 days)\n&quot;,&#10;    &quot;    targets['readmission'] = 0\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # For each patient, check if they have a subsequent admission within 30 days\n&quot;,&#10;    &quot;    all_admissions = admissions.sort_values(['subject_id', 'admittime'])\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    for _, target_row in targets.iterrows():\n&quot;,&#10;    &quot;        subject_id = target_row['subject_id']\n&quot;,&#10;    &quot;        discharge_time = target_row['dischtime']\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        # Get all admissions for this patient after discharge\n&quot;,&#10;    &quot;        patient_admissions = all_admissions[\n&quot;,&#10;    &quot;            all_admissions['subject_id'] == subject_id\n&quot;,&#10;    &quot;        ]\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        subsequent_admissions = patient_admissions[\n&quot;,&#10;    &quot;            patient_admissions['admittime'] &gt; discharge_time\n&quot;,&#10;    &quot;        ]\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        if len(subsequent_admissions) &gt; 0:\n&quot;,&#10;    &quot;            # Check if any subsequent admission is within 30 days\n&quot;,&#10;    &quot;            next_admission = subsequent_admissions.iloc[0]['admittime']\n&quot;,&#10;    &quot;            days_to_readmission = (next_admission - discharge_time).total_seconds() / (24 * 3600)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            if days_to_readmission &lt;= READMISSION_WINDOW:\n&quot;,&#10;    &quot;                targets.loc[targets['subject_id'] == subject_id, 'readmission'] = 1\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Select final columns\n&quot;,&#10;    &quot;    target_columns = ['subject_id', 'mortality', 'prolonged_stay', 'readmission']\n&quot;,&#10;    &quot;    final_targets = targets[target_columns]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Print summary statistics\n&quot;,&#10;    &quot;    print(\&quot;Target Variable Summary:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Mortality rate: {final_targets['mortality'].mean():.2%}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Prolonged stay rate: {final_targets['prolonged_stay'].mean():.2%}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Readmission rate: {final_targets['readmission'].mean():.2%}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return final_targets\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Target variable creation function defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 8. Model Training and Evaluation&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def prepare_features_and_targets(features_df, targets_df):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Prepare final feature matrix and target variables.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Merge features with targets\n&quot;,&#10;    &quot;    final_data = features_df.merge(targets_df, on='subject_id', how='inner')\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Separate features and targets\n&quot;,&#10;    &quot;    feature_cols = [col for col in final_data.columns \n&quot;,&#10;    &quot;                   if col not in ['subject_id', 'mortality', 'prolonged_stay', 'readmission']]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    X = final_data[feature_cols]\n&quot;,&#10;    &quot;    y_mortality = final_data['mortality']\n&quot;,&#10;    &quot;    y_prolonged = final_data['prolonged_stay']\n&quot;,&#10;    &quot;    y_readmission = final_data['readmission']\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;Final dataset shape: {X.shape}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Features: {len(feature_cols)}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Samples: {len(X)}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return X, y_mortality, y_prolonged, y_readmission, feature_cols\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def preprocess_features(X_train, X_test):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Preprocess features: imputation and scaling.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Handle missing values\n&quot;,&#10;    &quot;    imputer = SimpleImputer(strategy='median')\n&quot;,&#10;    &quot;    X_train_imputed = imputer.fit_transform(X_train)\n&quot;,&#10;    &quot;    X_test_imputed = imputer.transform(X_test)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Scale features\n&quot;,&#10;    &quot;    scaler = StandardScaler()\n&quot;,&#10;    &quot;    X_train_scaled = scaler.fit_transform(X_train_imputed)\n&quot;,&#10;    &quot;    X_test_scaled = scaler.transform(X_test_imputed)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return X_train_scaled, X_test_scaled, imputer, scaler\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def evaluate_model(model, X_test, y_test, model_name, target_name):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Comprehensive model evaluation.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Predictions\n&quot;,&#10;    &quot;    y_pred = model.predict(X_test)\n&quot;,&#10;    &quot;    y_pred_proba = model.predict_proba(X_test)[:, 1]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Metrics\n&quot;,&#10;    &quot;    accuracy = accuracy_score(y_test, y_pred)\n&quot;,&#10;    &quot;    precision = precision_score(y_test, y_pred)\n&quot;,&#10;    &quot;    recall = recall_score(y_test, y_pred)\n&quot;,&#10;    &quot;    f1 = f1_score(y_test, y_pred)\n&quot;,&#10;    &quot;    auc = roc_auc_score(y_test, y_pred_proba)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n{model_name} - {target_name}:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  Accuracy: {accuracy:.3f}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  Precision: {precision:.3f}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  Recall: {recall:.3f}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  F1-Score: {f1:.3f}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  AUC-ROC: {auc:.3f}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return {\n&quot;,&#10;    &quot;        'model': model_name,\n&quot;,&#10;    &quot;        'target': target_name,\n&quot;,&#10;    &quot;        'accuracy': accuracy,\n&quot;,&#10;    &quot;        'precision': precision,\n&quot;,&#10;    &quot;        'recall': recall,\n&quot;,&#10;    &quot;        'f1': f1,\n&quot;,&#10;    &quot;        'auc': auc\n&quot;,&#10;    &quot;    }\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def plot_roc_curve(models_dict, X_test, y_test, target_name):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Plot ROC curves for multiple models.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    plt.figure(figsize=(10, 6))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    for model_name, model in models_dict.items():\n&quot;,&#10;    &quot;        y_pred_proba = model.predict_proba(X_test)[:, 1]\n&quot;,&#10;    &quot;        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n&quot;,&#10;    &quot;        auc = roc_auc_score(y_test, y_pred_proba)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n&quot;,&#10;    &quot;    plt.xlabel('False Positive Rate')\n&quot;,&#10;    &quot;    plt.ylabel('True Positive Rate')\n&quot;,&#10;    &quot;    plt.title(f'ROC Curves - {target_name}')\n&quot;,&#10;    &quot;    plt.legend()\n&quot;,&#10;    &quot;    plt.grid(True)\n&quot;,&#10;    &quot;    plt.show()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Model training and evaluation functions defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 9. Main Pipeline Execution\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Note**: The following cells will execute the full pipeline. Make sure you have implemented the data loading functions based on your actual data source.&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# TODO: Implement this cell once you have your data loading functions ready\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Load MIMIC data\n&quot;,&#10;    &quot;print(\&quot;Loading MIMIC-III data...\&quot;)\n&quot;,&#10;    &quot;# mimic_tables = load_mimic_data()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Data loading step ready - please implement load_mimic_data() function first\&quot;)\n&quot;,&#10;    &quot;print(\&quot;Once implemented, this cell will:\&quot;)\n&quot;,&#10;    &quot;print(\&quot;1. Load all MIMIC-III tables\&quot;)\n&quot;,&#10;    &quot;print(\&quot;2. Filter to cohort patients\&quot;)\n&quot;,&#10;    &quot;print(\&quot;3. Extract time windows\&quot;)\n&quot;,&#10;    &quot;print(\&quot;4. Create features\&quot;)\n&quot;,&#10;    &quot;print(\&quot;5. Create targets\&quot;)\n&quot;,&#10;    &quot;print(\&quot;6. Train and evaluate models\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Example pipeline execution (uncomment and modify once data loading is implemented)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;# 1. Load and filter data\n&quot;,&#10;    &quot;mimic_tables = load_mimic_data()\n&quot;,&#10;    &quot;admissions = filter_to_cohort(mimic_tables['admissions'], subject_ids)\n&quot;,&#10;    &quot;patients = filter_to_cohort(mimic_tables['patients'], subject_ids)\n&quot;,&#10;    &quot;chartevents = filter_to_cohort(mimic_tables['chartevents'], subject_ids)\n&quot;,&#10;    &quot;labevents = filter_to_cohort(mimic_tables['labevents'], subject_ids)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# 2. Get first admission times\n&quot;,&#10;    &quot;first_admissions = get_first_admission_time(admissions, subject_ids)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# 3. Create features\n&quot;,&#10;    &quot;print(\&quot;Creating features...\&quot;)\n&quot;,&#10;    &quot;demo_features = create_demographic_features(patients, admissions, subject_ids)\n&quot;,&#10;    &quot;vital_features = create_vital_signs_features(chartevents, first_admissions)\n&quot;,&#10;    &quot;lab_features = create_lab_features(labevents, first_admissions)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Combine all features\n&quot;,&#10;    &quot;all_features = demo_features\n&quot;,&#10;    &quot;for vf in vital_features:\n&quot;,&#10;    &quot;    all_features = all_features.merge(vf, on='subject_id', how='left')\n&quot;,&#10;    &quot;for lf in lab_features:\n&quot;,&#10;    &quot;    all_features = all_features.merge(lf, on='subject_id', how='left')\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# 4. Create targets\n&quot;,&#10;    &quot;print(\&quot;Creating target variables...\&quot;)\n&quot;,&#10;    &quot;targets = create_target_variables(admissions, patients, subject_ids)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# 5. Prepare final dataset\n&quot;,&#10;    &quot;X, y_mortality, y_prolonged, y_readmission, feature_cols = prepare_features_and_targets(\n&quot;,&#10;    &quot;    all_features, targets\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Pipeline ready for model training!\&quot;)\n&quot;,&#10;    &quot;\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Pipeline execution template ready\&quot;)\n&quot;,&#10;    &quot;print(\&quot;Uncomment and run once data loading is implemented\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 10. Model Training for All Three Targets&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Example model training (uncomment once data is ready)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;# Define models\n&quot;,&#10;    &quot;models = {\n&quot;,&#10;    &quot;    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE),\n&quot;,&#10;    &quot;    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n&quot;,&#10;    &quot;    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE)\n&quot;,&#10;    &quot;}\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Train and evaluate for each target\n&quot;,&#10;    &quot;targets_dict = {\n&quot;,&#10;    &quot;    'Mortality': y_mortality,\n&quot;,&#10;    &quot;    'Prolonged Stay': y_prolonged,\n&quot;,&#10;    &quot;    'Readmission': y_readmission\n&quot;,&#10;    &quot;}\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;results = []\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;for target_name, y_target in targets_dict.items():\n&quot;,&#10;    &quot;    print(f\&quot;\\n{'='*50}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Training models for {target_name}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;{'='*50}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Split data\n&quot;,&#10;    &quot;    X_train, X_test, y_train, y_test = train_test_split(\n&quot;,&#10;    &quot;        X, y_target, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_target\n&quot;,&#10;    &quot;    )\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Preprocess features\n&quot;,&#10;    &quot;    X_train_processed, X_test_processed, imputer, scaler = preprocess_features(\n&quot;,&#10;    &quot;        X_train, X_test\n&quot;,&#10;    &quot;    )\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    trained_models = {}\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Train each model\n&quot;,&#10;    &quot;    for model_name, model in models.items():\n&quot;,&#10;    &quot;        print(f\&quot;\\nTraining {model_name}...\&quot;)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        # Train model\n&quot;,&#10;    &quot;        model.fit(X_train_processed, y_train)\n&quot;,&#10;    &quot;        trained_models[model_name] = model\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        # Evaluate model\n&quot;,&#10;    &quot;        result = evaluate_model(\n&quot;,&#10;    &quot;            model, X_test_processed, y_test, model_name, target_name\n&quot;,&#10;    &quot;        )\n&quot;,&#10;    &quot;        results.append(result)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Plot ROC curves\n&quot;,&#10;    &quot;    plot_roc_curve(trained_models, X_test_processed, y_test, target_name)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Results summary\n&quot;,&#10;    &quot;results_df = pd.DataFrame(results)\n&quot;,&#10;    &quot;print(\&quot;\\n\&quot; + \&quot;=\&quot;*50)\n&quot;,&#10;    &quot;print(\&quot;FINAL RESULTS SUMMARY\&quot;)\n&quot;,&#10;    &quot;print(\&quot;=\&quot;*50)\n&quot;,&#10;    &quot;print(results_df.to_string(index=False))\n&quot;,&#10;    &quot;\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Model training template ready\&quot;)\n&quot;,&#10;    &quot;print(\&quot;Uncomment and run once features and targets are prepared\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 11. Results Analysis and Visualization&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def plot_feature_importance(model, feature_names, target_name, top_n=20):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Plot feature importance for tree-based models.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    if hasattr(model, 'feature_importances_'):\n&quot;,&#10;    &quot;        importance_df = pd.DataFrame({\n&quot;,&#10;    &quot;            'feature': feature_names,\n&quot;,&#10;    &quot;            'importance': model.feature_importances_\n&quot;,&#10;    &quot;        }).sort_values('importance', ascending=False)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        plt.figure(figsize=(10, 8))\n&quot;,&#10;    &quot;        sns.barplot(data=importance_df.head(top_n), x='importance', y='feature')\n&quot;,&#10;    &quot;        plt.title(f'Top {top_n} Feature Importance - {target_name}')\n&quot;,&#10;    &quot;        plt.tight_layout()\n&quot;,&#10;    &quot;        plt.show()\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        return importance_df\n&quot;,&#10;    &quot;    else:\n&quot;,&#10;    &quot;        print(f\&quot;Model does not have feature_importances_ attribute\&quot;)\n&quot;,&#10;    &quot;        return None\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def create_results_summary_table(results_df):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Create a formatted results summary table.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    # Pivot table for better visualization\n&quot;,&#10;    &quot;    summary = results_df.pivot_table(\n&quot;,&#10;    &quot;        index='model', \n&quot;,&#10;    &quot;        columns='target', \n&quot;,&#10;    &quot;        values=['accuracy', 'precision', 'recall', 'f1', 'auc']\n&quot;,&#10;    &quot;    )\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return summary\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Results analysis functions defined\&quot;)\n&quot;,&#10;    &quot;print(\&quot;Use these after model training to analyze results and feature importance\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 12. Next Steps and TODO\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;### Immediate Actions Required:\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;1. **Implement Data Loading**: \n&quot;,&#10;    &quot;   - Modify `load_mimic_data()` function based on your actual data source\n&quot;,&#10;    &quot;   - Update file paths and data format as needed\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;2. **Verify Item IDs**: \n&quot;,&#10;    &quot;   - Check that the vital signs and lab item IDs match your dataset\n&quot;,&#10;    &quot;   - Update the dictionaries in feature engineering functions\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;3. **Test Pipeline**: \n&quot;,&#10;    &quot;   - Run the pipeline on a small subset first\n&quot;,&#10;    &quot;   - Debug any data type or format issues\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;### Optional Improvements:\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;4. **Advanced Features**:\n&quot;,&#10;    &quot;   - Add medication features\n&quot;,&#10;    &quot;   - Include diagnosis/procedure codes\n&quot;,&#10;    &quot;   - Create time-series features (trends, slopes)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;5. **Model Improvements**:\n&quot;,&#10;    &quot;   - Hyperparameter tuning\n&quot;,&#10;    &quot;   - Advanced models (XGBoost, Neural Networks)\n&quot;,&#10;    &quot;   - Ensemble methods\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;6. **Validation**:\n&quot;,&#10;    &quot;   - Cross-validation\n&quot;,&#10;    &quot;   - Temporal validation\n&quot;,&#10;    &quot;   - Clinical validation with domain experts\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;### For Your Paper:\n&quot;,&#10;    &quot;- Document methodology and feature engineering choices\n&quot;,&#10;    &quot;- Include performance metrics and comparisons\n&quot;,&#10;    &quot;- Discuss clinical relevance and limitations\n&quot;,&#10;    &quot;- Add visualizations and result interpretations&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 13. Helper Functions and Utilities&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def save_results(results_df, filepath='./results/'):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Save results to CSV file.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    import os\n&quot;,&#10;    &quot;    os.makedirs(filepath, exist_ok=True)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n&quot;,&#10;    &quot;    filename = f'{filepath}model_results_{timestamp}.csv'\n&quot;,&#10;    &quot;    results_df.to_csv(filename, index=False)\n&quot;,&#10;    &quot;    print(f\&quot;Results saved to {filename}\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def check_data_quality(df, df_name):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Perform basic data quality checks.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    print(f\&quot;\\nData Quality Report - {df_name}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Shape: {df.shape}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Missing values:\&quot;)\n&quot;,&#10;    &quot;    missing = df.isnull().sum()\n&quot;,&#10;    &quot;    missing_pct = (missing / len(df)) * 100\n&quot;,&#10;    &quot;    missing_df = pd.DataFrame({\n&quot;,&#10;    &quot;        'Missing Count': missing,\n&quot;,&#10;    &quot;        'Missing %': missing_pct\n&quot;,&#10;    &quot;    })\n&quot;,&#10;    &quot;    print(missing_df[missing_df['Missing Count'] &gt; 0].to_string())\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def validate_cohort_coverage(df, subject_ids, df_name):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Check how many cohort patients are covered in a dataset.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    if 'subject_id' in df.columns:\n&quot;,&#10;    &quot;        covered_subjects = set(df['subject_id'].unique())\n&quot;,&#10;    &quot;        cohort_subjects = set(subject_ids)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        coverage = len(covered_subjects.intersection(cohort_subjects))\n&quot;,&#10;    &quot;        total_cohort = len(cohort_subjects)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        print(f\&quot;\\n{df_name} Coverage:\&quot;)\n&quot;,&#10;    &quot;        print(f\&quot;Cohort patients covered: {coverage}/{total_cohort} ({coverage/total_cohort:.1%})\&quot;)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        missing_subjects = cohort_subjects - covered_subjects\n&quot;,&#10;    &quot;        if missing_subjects:\n&quot;,&#10;    &quot;            print(f\&quot;Missing subjects: {len(missing_subjects)}\&quot;)\n&quot;,&#10;    &quot;            if len(missing_subjects) &lt;= 10:\n&quot;,&#10;    &quot;                print(f\&quot;Missing subject IDs: {list(missing_subjects)}\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Helper functions defined\&quot;)\n&quot;,&#10;    &quot;print(\&quot;Use these for data quality checks and result saving\&quot;)&quot;&#10;   ]&#10;  }&#10; ],&#10; &quot;metadata&quot;: {&#10;  &quot;kernelspec&quot;: {&#10;   &quot;display_name&quot;: &quot;Python 3&quot;,&#10;   &quot;language&quot;: &quot;python&quot;,&#10;   &quot;name&quot;: &quot;python3&quot;&#10;  },&#10;  &quot;language_info&quot;: {&#10;   &quot;codemirror_mode&quot;: {&#10;    &quot;name&quot;: &quot;ipython&quot;,&#10;    &quot;version&quot;: 3&#10;   },&#10;   &quot;file_extension&quot;: &quot;.py&quot;,&#10;   &quot;mimetype&quot;: &quot;text/x-python&quot;,&#10;   &quot;name&quot;: &quot;python&quot;,&#10;   &quot;nbconvert_exporter&quot;: &quot;python&quot;,&#10;   &quot;pygments_lexer&quot;: &quot;ipython3&quot;,&#10;   &quot;version&quot;: &quot;3.8.5&quot;&#10;  }&#10; },&#10; &quot;nbformat&quot;: 4,&#10; &quot;nbformat_minor&quot;: 4&#10;}&#10;&#10;" />
              <option name="updatedContent" value="{&#10; &quot;cells&quot;: [&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;# MLHC Final Project: ICU Outcome Prediction\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;## Objective\n&quot;,&#10;    &quot;Build prediction models for three ICU outcomes using MIMIC-III data:\n&quot;,&#10;    &quot;1. **Mortality** - Death during hospitalization or within 30 days after discharge\n&quot;,&#10;    &quot;2. **Prolonged Stay** - Length of stay &gt; 7 days\n&quot;,&#10;    &quot;3. **Hospital Readmission** - Readmission within 30 days after discharge\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Data Window**: First 48 hours of first hospital admission (with 6-hour gap)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Cohort**: Patients from initial_cohort.csv only&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 1. Setup and Imports&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Core libraries\n&quot;,&#10;    &quot;import pandas as pd\n&quot;,&#10;    &quot;import numpy as np\n&quot;,&#10;    &quot;import matplotlib.pyplot as plt\n&quot;,&#10;    &quot;import seaborn as sns\n&quot;,&#10;    &quot;from datetime import datetime, timedelta\n&quot;,&#10;    &quot;import warnings\n&quot;,&#10;    &quot;warnings.filterwarnings('ignore')\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Machine learning libraries\n&quot;,&#10;    &quot;from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n&quot;,&#10;    &quot;from sklearn.preprocessing import StandardScaler, LabelEncoder\n&quot;,&#10;    &quot;from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n&quot;,&#10;    &quot;from sklearn.linear_model import LogisticRegression\n&quot;,&#10;    &quot;from sklearn.metrics import (\n&quot;,&#10;    &quot;    accuracy_score, precision_score, recall_score, f1_score, \n&quot;,&#10;    &quot;    roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix,\n&quot;,&#10;    &quot;    classification_report\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;from sklearn.impute import SimpleImputer\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Set display options\n&quot;,&#10;    &quot;pd.set_option('display.max_columns', None)\n&quot;,&#10;    &quot;pd.set_option('display.max_rows', 100)\n&quot;,&#10;    &quot;plt.style.use('seaborn-v0_8')\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Random seed for reproducibility\n&quot;,&#10;    &quot;RANDOM_STATE = 42\n&quot;,&#10;    &quot;np.random.seed(RANDOM_STATE)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 2. Configuration and Constants&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# File paths\n&quot;,&#10;    &quot;DATA_PATH = \&quot;./data/\&quot;\n&quot;,&#10;    &quot;COHORT_FILE = DATA_PATH + \&quot;initial_cohort.csv\&quot;\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Time windows (in hours)\n&quot;,&#10;    &quot;PREDICTION_WINDOW = 48  # First 48 hours\n&quot;,&#10;    &quot;GAP_HOURS = 6  # 6-hour gap before prediction\n&quot;,&#10;    &quot;EFFECTIVE_WINDOW = PREDICTION_WINDOW - GAP_HOURS  # 42 hours of actual data\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Outcome definitions\n&quot;,&#10;    &quot;PROLONGED_STAY_THRESHOLD = 7  # days\n&quot;,&#10;    &quot;READMISSION_WINDOW = 30  # days\n&quot;,&#10;    &quot;MORTALITY_WINDOW = 30  # days post-discharge\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Model parameters\n&quot;,&#10;    &quot;TEST_SIZE = 0.2\n&quot;,&#10;    &quot;VALIDATION_SIZE = 0.2\n&quot;,&#10;    &quot;CV_FOLDS = 5\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;Configuration loaded:\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;- Effective data window: {EFFECTIVE_WINDOW} hours\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;- Prolonged stay threshold: {PROLONGED_STAY_THRESHOLD} days\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;- Readmission window: {READMISSION_WINDOW} days\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;- Mortality window: {MORTALITY_WINDOW} days post-discharge\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 3. Data Loading and Initial Exploration&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def load_initial_cohort(filepath=COHORT_FILE):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Load the initial cohort of patients.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    try:\n&quot;,&#10;    &quot;        cohort = pd.read_csv(filepath)\n&quot;,&#10;    &quot;        print(f\&quot;Successfully loaded {len(cohort)} patients from initial cohort\&quot;)\n&quot;,&#10;    &quot;        return cohort\n&quot;,&#10;    &quot;    except FileNotFoundError:\n&quot;,&#10;    &quot;        print(f\&quot;Error: File {filepath} not found\&quot;)\n&quot;,&#10;    &quot;        return None\n&quot;,&#10;    &quot;    except Exception as e:\n&quot;,&#10;    &quot;        print(f\&quot;Error loading cohort: {e}\&quot;)\n&quot;,&#10;    &quot;        return None\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Load initial cohort\n&quot;,&#10;    &quot;cohort_df = load_initial_cohort()\n&quot;,&#10;    &quot;if cohort_df is not None:\n&quot;,&#10;    &quot;    print(f\&quot;\\nCohort shape: {cohort_df.shape}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Columns: {list(cohort_df.columns)}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;\\nFirst 10 subject IDs:\&quot;)\n&quot;,&#10;    &quot;    print(cohort_df.head(10))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Get list of subject IDs for filtering\n&quot;,&#10;    &quot;    subject_ids = cohort_df['subject_id'].tolist()\n&quot;,&#10;    &quot;    print(f\&quot;\\nTotal subjects in cohort: {len(subject_ids)}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 4. Example Data Analysis&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Load the test example file to understand data structure\n&quot;,&#10;    &quot;try:\n&quot;,&#10;    &quot;    test_data = pd.read_csv('./data/test_example.csv')\n&quot;,&#10;    &quot;    print(\&quot;Test example data loaded successfully\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Shape: {test_data.shape}\&quot;)\n&quot;,&#10;    &quot;    print(\&quot;\\nColumns:\&quot;)\n&quot;,&#10;    &quot;    print(test_data.columns.tolist())\n&quot;,&#10;    &quot;    print(\&quot;\\nFirst few rows:\&quot;)\n&quot;,&#10;    &quot;    print(test_data.head())\n&quot;,&#10;    &quot;    print(\&quot;\\nData types:\&quot;)\n&quot;,&#10;    &quot;    print(test_data.dtypes)\n&quot;,&#10;    &quot;except FileNotFoundError:\n&quot;,&#10;    &quot;    print(\&quot;test_example.csv not found in data directory\&quot;)\n&quot;,&#10;    &quot;except Exception as e:\n&quot;,&#10;    &quot;    print(f\&quot;Error loading test example: {e}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 5. Data Processing Pipeline\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Note**: Adapt these functions based on your actual MIMIC-III data structure&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def filter_to_cohort(df, cohort_subject_ids, subject_col='subject_id'):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Filter dataframe to only include patients in our cohort.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    if subject_col not in df.columns:\n&quot;,&#10;    &quot;        print(f\&quot;Warning: {subject_col} not found in dataframe columns\&quot;)\n&quot;,&#10;    &quot;        return df\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    initial_count = len(df)\n&quot;,&#10;    &quot;    filtered_df = df[df[subject_col].isin(cohort_subject_ids)]\n&quot;,&#10;    &quot;    final_count = len(filtered_df)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;Filtered from {initial_count} to {final_count} rows\&quot;)\n&quot;,&#10;    &quot;    return filtered_df\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def extract_time_window_data(df, first_admissions, time_col, window_hours=EFFECTIVE_WINDOW):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Extract data within the specified time window from first admission.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    # Merge with first admission times\n&quot;,&#10;    &quot;    df_with_admit = df.merge(first_admissions, on='subject_id', how='inner')\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Convert time column to datetime\n&quot;,&#10;    &quot;    df_with_admit[time_col] = pd.to_datetime(df_with_admit[time_col])\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Calculate time from admission\n&quot;,&#10;    &quot;    df_with_admit['hours_from_admit'] = (\n&quot;,&#10;    &quot;        df_with_admit[time_col] - df_with_admit['first_admittime']\n&quot;,&#10;    &quot;    ).dt.total_seconds() / 3600\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Filter to time window (0 to window_hours)\n&quot;,&#10;    &quot;    window_data = df_with_admit[\n&quot;,&#10;    &quot;        (df_with_admit['hours_from_admit'] &gt;= 0) &amp; \n&quot;,&#10;    &quot;        (df_with_admit['hours_from_admit'] &lt;= window_hours)\n&quot;,&#10;    &quot;    ]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;Extracted {len(window_data)} records within {window_hours}-hour window\&quot;)\n&quot;,&#10;    &quot;    return window_data\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Data processing functions defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 6. Feature Engineering&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def create_basic_features(data_df, subject_ids):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Create basic features from available data.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Filter to cohort patients\n&quot;,&#10;    &quot;    cohort_data = filter_to_cohort(data_df, subject_ids)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Group by subject_id and create aggregated features\n&quot;,&#10;    &quot;    features = []\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Get numeric columns for aggregation\n&quot;,&#10;    &quot;    numeric_cols = cohort_data.select_dtypes(include=[np.number]).columns\n&quot;,&#10;    &quot;    numeric_cols = [col for col in numeric_cols if col != 'subject_id']\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    if len(numeric_cols) &gt; 0:\n&quot;,&#10;    &quot;        # Calculate statistics for numeric columns\n&quot;,&#10;    &quot;        for col in numeric_cols:\n&quot;,&#10;    &quot;            col_stats = cohort_data.groupby('subject_id')[col].agg([\n&quot;,&#10;    &quot;                'mean', 'std', 'min', 'max', 'count'\n&quot;,&#10;    &quot;            ]).reset_index()\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Rename columns\n&quot;,&#10;    &quot;            col_stats.columns = ['subject_id'] + [\n&quot;,&#10;    &quot;                f'{col}_{stat}' for stat in ['mean', 'std', 'min', 'max', 'count']\n&quot;,&#10;    &quot;            ]\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            features.append(col_stats)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Merge all features\n&quot;,&#10;    &quot;    if features:\n&quot;,&#10;    &quot;        final_features = features[0]\n&quot;,&#10;    &quot;        for feat in features[1:]:\n&quot;,&#10;    &quot;            final_features = final_features.merge(feat, on='subject_id', how='outer')\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        print(f\&quot;Created {len(final_features.columns)-1} features for {len(final_features)} patients\&quot;)\n&quot;,&#10;    &quot;        return final_features\n&quot;,&#10;    &quot;    else:\n&quot;,&#10;    &quot;        print(\&quot;No numeric columns found for feature creation\&quot;)\n&quot;,&#10;    &quot;        return pd.DataFrame({'subject_id': subject_ids})\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Feature engineering functions defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 7. Target Variable Creation&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def create_example_targets(subject_ids):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Create example target variables for demonstration.\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    In a real implementation, these would be derived from:\n&quot;,&#10;    &quot;    - ADMISSIONS table for length of stay and readmissions\n&quot;,&#10;    &quot;    - PATIENTS table for mortality dates\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Create random targets for demonstration\n&quot;,&#10;    &quot;    np.random.seed(RANDOM_STATE)\n&quot;,&#10;    &quot;    n_patients = len(subject_ids)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    targets = pd.DataFrame({\n&quot;,&#10;    &quot;        'subject_id': subject_ids,\n&quot;,&#10;    &quot;        'mortality': np.random.binomial(1, 0.15, n_patients),  # 15% mortality rate\n&quot;,&#10;    &quot;        'prolonged_stay': np.random.binomial(1, 0.25, n_patients),  # 25% prolonged stay\n&quot;,&#10;    &quot;        'readmission': np.random.binomial(1, 0.20, n_patients)  # 20% readmission rate\n&quot;,&#10;    &quot;    })\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(\&quot;Example Target Variable Summary:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Mortality rate: {targets['mortality'].mean():.2%}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Prolonged stay rate: {targets['prolonged_stay'].mean():.2%}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Readmission rate: {targets['readmission'].mean():.2%}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return targets\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Target variable creation function defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 8. Model Training and Evaluation&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def prepare_data_for_modeling(features_df, targets_df):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Prepare final dataset for modeling.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Merge features with targets\n&quot;,&#10;    &quot;    final_data = features_df.merge(targets_df, on='subject_id', how='inner')\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Separate features and targets\n&quot;,&#10;    &quot;    feature_cols = [col for col in final_data.columns \n&quot;,&#10;    &quot;                   if col not in ['subject_id', 'mortality', 'prolonged_stay', 'readmission']]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    X = final_data[feature_cols]\n&quot;,&#10;    &quot;    y_mortality = final_data['mortality']\n&quot;,&#10;    &quot;    y_prolonged = final_data['prolonged_stay']\n&quot;,&#10;    &quot;    y_readmission = final_data['readmission']\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;Final dataset shape: {X.shape}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Features: {len(feature_cols)}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Samples: {len(X)}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return X, y_mortality, y_prolonged, y_readmission, feature_cols\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def train_and_evaluate_model(X, y, target_name, model_name='RandomForest'):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Train and evaluate a model for a specific target.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Split data\n&quot;,&#10;    &quot;    X_train, X_test, y_train, y_test = train_test_split(\n&quot;,&#10;    &quot;        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n&quot;,&#10;    &quot;    )\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Handle missing values\n&quot;,&#10;    &quot;    imputer = SimpleImputer(strategy='median')\n&quot;,&#10;    &quot;    X_train_imputed = imputer.fit_transform(X_train)\n&quot;,&#10;    &quot;    X_test_imputed = imputer.transform(X_test)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Scale features\n&quot;,&#10;    &quot;    scaler = StandardScaler()\n&quot;,&#10;    &quot;    X_train_scaled = scaler.fit_transform(X_train_imputed)\n&quot;,&#10;    &quot;    X_test_scaled = scaler.transform(X_test_imputed)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Train model\n&quot;,&#10;    &quot;    if model_name == 'RandomForest':\n&quot;,&#10;    &quot;        model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n&quot;,&#10;    &quot;    elif model_name == 'LogisticRegression':\n&quot;,&#10;    &quot;        model = LogisticRegression(random_state=RANDOM_STATE)\n&quot;,&#10;    &quot;    else:\n&quot;,&#10;    &quot;        model = GradientBoostingClassifier(random_state=RANDOM_STATE)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    model.fit(X_train_scaled, y_train)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Evaluate\n&quot;,&#10;    &quot;    y_pred = model.predict(X_test_scaled)\n&quot;,&#10;    &quot;    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Calculate metrics\n&quot;,&#10;    &quot;    accuracy = accuracy_score(y_test, y_pred)\n&quot;,&#10;    &quot;    precision = precision_score(y_test, y_pred)\n&quot;,&#10;    &quot;    recall = recall_score(y_test, y_pred)\n&quot;,&#10;    &quot;    f1 = f1_score(y_test, y_pred)\n&quot;,&#10;    &quot;    auc = roc_auc_score(y_test, y_pred_proba)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n{model_name} - {target_name}:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  Accuracy: {accuracy:.3f}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  Precision: {precision:.3f}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  Recall: {recall:.3f}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  F1-Score: {f1:.3f}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  AUC-ROC: {auc:.3f}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return {\n&quot;,&#10;    &quot;        'model': model_name,\n&quot;,&#10;    &quot;        'target': target_name,\n&quot;,&#10;    &quot;        'accuracy': accuracy,\n&quot;,&#10;    &quot;        'precision': precision,\n&quot;,&#10;    &quot;        'recall': recall,\n&quot;,&#10;    &quot;        'f1': f1,\n&quot;,&#10;    &quot;        'auc': auc\n&quot;,&#10;    &quot;    }\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Model training and evaluation functions defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 9. Main Pipeline Execution&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Execute the main pipeline\n&quot;,&#10;    &quot;if cohort_df is not None and len(subject_ids) &gt; 0:\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Try to use test example data if available\n&quot;,&#10;    &quot;    try:\n&quot;,&#10;    &quot;        test_data = pd.read_csv('./data/test_example.csv')\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        # Create features from available data\n&quot;,&#10;    &quot;        print(\&quot;Creating features from test data...\&quot;)\n&quot;,&#10;    &quot;        features = create_basic_features(test_data, subject_ids)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        # Create example targets\n&quot;,&#10;    &quot;        print(\&quot;\\nCreating example target variables...\&quot;)\n&quot;,&#10;    &quot;        targets = create_example_targets(subject_ids)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        # Prepare data for modeling\n&quot;,&#10;    &quot;        print(\&quot;\\nPreparing data for modeling...\&quot;)\n&quot;,&#10;    &quot;        X, y_mortality, y_prolonged, y_readmission, feature_cols = prepare_data_for_modeling(\n&quot;,&#10;    &quot;            features, targets\n&quot;,&#10;    &quot;        )\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        if X.shape[1] &gt; 0:  # If we have features\n&quot;,&#10;    &quot;            # Train models for each target\n&quot;,&#10;    &quot;            results = []\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            targets_dict = {\n&quot;,&#10;    &quot;                'Mortality': y_mortality,\n&quot;,&#10;    &quot;                'Prolonged Stay': y_prolonged,\n&quot;,&#10;    &quot;                'Readmission': y_readmission\n&quot;,&#10;    &quot;            }\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            for target_name, y_target in targets_dict.items():\n&quot;,&#10;    &quot;                result = train_and_evaluate_model(X, y_target, target_name)\n&quot;,&#10;    &quot;                results.append(result)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Display results summary\n&quot;,&#10;    &quot;            results_df = pd.DataFrame(results)\n&quot;,&#10;    &quot;            print(\&quot;\\n\&quot; + \&quot;=\&quot;*50)\n&quot;,&#10;    &quot;            print(\&quot;RESULTS SUMMARY\&quot;)\n&quot;,&#10;    &quot;            print(\&quot;=\&quot;*50)\n&quot;,&#10;    &quot;            print(results_df.to_string(index=False))\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;        else:\n&quot;,&#10;    &quot;            print(\&quot;No features available for modeling\&quot;)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;    except FileNotFoundError:\n&quot;,&#10;    &quot;        print(\&quot;test_example.csv not found. Please add your MIMIC-III data files.\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;\\nTo complete this project, you'll need to:\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;1. Add MIMIC-III data files to the data/ directory\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;2. Modify the data loading functions to match your data structure\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;3. Implement proper feature engineering based on MIMIC-III tables\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;4. Create real target variables from admission and patient data\&quot;)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;else:\n&quot;,&#10;    &quot;    print(\&quot;No cohort data loaded. Please check initial_cohort.csv file.\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 10. Next Steps and TODO\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;### To Complete Your MLHC Project:\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;1. **Add MIMIC-III Data**:\n&quot;,&#10;    &quot;   - Place MIMIC-III CSV files in the `data/` directory\n&quot;,&#10;    &quot;   - Key tables needed: ADMISSIONS, PATIENTS, CHARTEVENTS, LABEVENTS\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;2. **Implement Real Data Loading**:\n&quot;,&#10;    &quot;   - Modify functions to load actual MIMIC-III tables\n&quot;,&#10;    &quot;   - Filter to cohort patients\n&quot;,&#10;    &quot;   - Extract first 48-hour windows\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;3. **Enhanced Feature Engineering**:\n&quot;,&#10;    &quot;   - Vital signs (heart rate, blood pressure, temperature)\n&quot;,&#10;    &quot;   - Laboratory values (hemoglobin, creatinine, electrolytes)\n&quot;,&#10;    &quot;   - Demographics (age, gender)\n&quot;,&#10;    &quot;   - Medications and procedures\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;4. **Real Target Variables**:\n&quot;,&#10;    &quot;   - Calculate actual mortality from PATIENTS.DOD\n&quot;,&#10;    &quot;   - Compute length of stay from ADMISSIONS\n&quot;,&#10;    &quot;   - Identify readmissions within 30 days\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;5. **Model Improvements**:\n&quot;,&#10;    &quot;   - Hyperparameter tuning\n&quot;,&#10;    &quot;   - Cross-validation\n&quot;,&#10;    &quot;   - Advanced algorithms (XGBoost, Neural Networks)\n&quot;,&#10;    &quot;   - Feature selection and importance analysis\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;6. **Evaluation and Reporting**:\n&quot;,&#10;    &quot;   - Clinical interpretation of results\n&quot;,&#10;    &quot;   - Feature importance analysis\n&quot;,&#10;    &quot;   - Model comparison and validation\n&quot;,&#10;    &quot;   - Documentation for your paper\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;### Current Status:\n&quot;,&#10;    &quot; Project structure and pipeline framework complete  \n&quot;,&#10;    &quot; Basic modeling and evaluation functions ready  \n&quot;,&#10;    &quot; Awaiting MIMIC-III data integration  \n&quot;,&#10;    &quot; Need real feature engineering implementation  \n&quot;,&#10;    &quot; Need actual target variable calculation  &quot;&#10;   ]&#10;  }&#10; ],&#10; &quot;metadata&quot;: {&#10;  &quot;kernelspec&quot;: {&#10;   &quot;display_name&quot;: &quot;Python 3&quot;,&#10;   &quot;language&quot;: &quot;python&quot;,&#10;   &quot;name&quot;: &quot;python3&quot;&#10;  },&#10;  &quot;language_info&quot;: {&#10;   &quot;codemirror_mode&quot;: {&#10;    &quot;name&quot;: &quot;ipython&quot;,&#10;    &quot;version&quot;: 3&#10;   },&#10;   &quot;file_extension&quot;: &quot;.py&quot;,&#10;   &quot;mimetype&quot;: &quot;text/x-python&quot;,&#10;   &quot;name&quot;: &quot;python&quot;,&#10;   &quot;nbconvert_exporter&quot;: &quot;python&quot;,&#10;   &quot;pygments_lexer&quot;: &quot;ipython3&quot;,&#10;   &quot;version&quot;: &quot;3.8.5&quot;&#10;  }&#10; },&#10; &quot;nbformat&quot;: 4,&#10; &quot;nbformat_minor&quot;: 4&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/project/mlhc_final_project_clean.ipynb">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/project/mlhc_final_project_clean.ipynb" />
              <option name="originalContent" value="#%%&#10;" />
              <option name="updatedContent" value="{&#10; &quot;cells&quot;: [&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;# MLHC Final Project: ICU Outcome Prediction\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;## Objective\n&quot;,&#10;    &quot;Build prediction models for three ICU outcomes using MIMIC-III data:\n&quot;,&#10;    &quot;1. **Mortality** - Death during hospitalization or within 30 days after discharge\n&quot;,&#10;    &quot;2. **Prolonged Stay** - Length of stay &gt; 7 days\n&quot;,&#10;    &quot;3. **Hospital Readmission** - Readmission within 30 days after discharge\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Data Window**: First 48 hours of first hospital admission (with 6-hour gap)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Cohort**: Patients from initial_cohort.csv only&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 1. Setup and Imports&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Core libraries\n&quot;,&#10;    &quot;import pandas as pd\n&quot;,&#10;    &quot;import numpy as np\n&quot;,&#10;    &quot;import matplotlib.pyplot as plt\n&quot;,&#10;    &quot;import seaborn as sns\n&quot;,&#10;    &quot;from datetime import datetime, timedelta\n&quot;,&#10;    &quot;import warnings\n&quot;,&#10;    &quot;warnings.filterwarnings('ignore')\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Machine learning libraries\n&quot;,&#10;    &quot;from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n&quot;,&#10;    &quot;from sklearn.preprocessing import StandardScaler, LabelEncoder\n&quot;,&#10;    &quot;from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n&quot;,&#10;    &quot;from sklearn.linear_model import LogisticRegression\n&quot;,&#10;    &quot;from sklearn.metrics import (\n&quot;,&#10;    &quot;    accuracy_score, precision_score, recall_score, f1_score, \n&quot;,&#10;    &quot;    roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix,\n&quot;,&#10;    &quot;    classification_report\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;from sklearn.impute import SimpleImputer\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Set display options\n&quot;,&#10;    &quot;pd.set_option('display.max_columns', None)\n&quot;,&#10;    &quot;pd.set_option('display.max_rows', 100)\n&quot;,&#10;    &quot;plt.style.use('default')\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Random seed for reproducibility\n&quot;,&#10;    &quot;RANDOM_STATE = 42\n&quot;,&#10;    &quot;np.random.seed(RANDOM_STATE)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 2. Configuration and Constants&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# File paths\n&quot;,&#10;    &quot;DATA_PATH = \&quot;./data/\&quot;\n&quot;,&#10;    &quot;COHORT_FILE = DATA_PATH + \&quot;initial_cohort.csv\&quot;\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Time windows (in hours)\n&quot;,&#10;    &quot;PREDICTION_WINDOW = 48  # First 48 hours\n&quot;,&#10;    &quot;GAP_HOURS = 6  # 6-hour gap before prediction\n&quot;,&#10;    &quot;EFFECTIVE_WINDOW = PREDICTION_WINDOW - GAP_HOURS  # 42 hours of actual data\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Outcome definitions\n&quot;,&#10;    &quot;PROLONGED_STAY_THRESHOLD = 7  # days\n&quot;,&#10;    &quot;READMISSION_WINDOW = 30  # days\n&quot;,&#10;    &quot;MORTALITY_WINDOW = 30  # days post-discharge\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Model parameters\n&quot;,&#10;    &quot;TEST_SIZE = 0.2\n&quot;,&#10;    &quot;VALIDATION_SIZE = 0.2\n&quot;,&#10;    &quot;CV_FOLDS = 5\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;Configuration loaded:\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;- Effective data window: {EFFECTIVE_WINDOW} hours\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;- Prolonged stay threshold: {PROLONGED_STAY_THRESHOLD} days\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;- Readmission window: {READMISSION_WINDOW} days\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;- Mortality window: {MORTALITY_WINDOW} days post-discharge\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 3. Data Loading and Initial Exploration&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def load_initial_cohort(filepath=COHORT_FILE):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Load the initial cohort of patients.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    try:\n&quot;,&#10;    &quot;        cohort = pd.read_csv(filepath)\n&quot;,&#10;    &quot;        print(f\&quot;Successfully loaded {len(cohort)} patients from initial cohort\&quot;)\n&quot;,&#10;    &quot;        return cohort\n&quot;,&#10;    &quot;    except FileNotFoundError:\n&quot;,&#10;    &quot;        print(f\&quot;Error: File {filepath} not found\&quot;)\n&quot;,&#10;    &quot;        return None\n&quot;,&#10;    &quot;    except Exception as e:\n&quot;,&#10;    &quot;        print(f\&quot;Error loading cohort: {e}\&quot;)\n&quot;,&#10;    &quot;        return None\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Load initial cohort\n&quot;,&#10;    &quot;cohort_df = load_initial_cohort()\n&quot;,&#10;    &quot;if cohort_df is not None:\n&quot;,&#10;    &quot;    print(f\&quot;\\nCohort shape: {cohort_df.shape}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Columns: {list(cohort_df.columns)}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;\\nFirst 10 subject IDs:\&quot;)\n&quot;,&#10;    &quot;    print(cohort_df.head(10))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Get list of subject IDs for filtering\n&quot;,&#10;    &quot;    subject_ids = cohort_df['subject_id'].tolist()\n&quot;,&#10;    &quot;    print(f\&quot;\\nTotal subjects in cohort: {len(subject_ids)}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 4. Example Data Analysis&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Load the test example file to understand data structure\n&quot;,&#10;    &quot;try:\n&quot;,&#10;    &quot;    test_data = pd.read_csv('./data/test_example.csv')\n&quot;,&#10;    &quot;    print(\&quot;Test example data loaded successfully\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Shape: {test_data.shape}\&quot;)\n&quot;,&#10;    &quot;    print(\&quot;\\nColumns:\&quot;)\n&quot;,&#10;    &quot;    print(test_data.columns.tolist())\n&quot;,&#10;    &quot;    print(\&quot;\\nFirst few rows:\&quot;)\n&quot;,&#10;    &quot;    print(test_data.head())\n&quot;,&#10;    &quot;    print(\&quot;\\nData types:\&quot;)\n&quot;,&#10;    &quot;    print(test_data.dtypes)\n&quot;,&#10;    &quot;except FileNotFoundError:\n&quot;,&#10;    &quot;    print(\&quot;test_example.csv not found in data directory\&quot;)\n&quot;,&#10;    &quot;except Exception as e:\n&quot;,&#10;    &quot;    print(f\&quot;Error loading test example: {e}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 5. Data Processing Pipeline\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Note**: Adapt these functions based on your actual MIMIC-III data structure&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def filter_to_cohort(df, cohort_subject_ids, subject_col='subject_id'):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Filter dataframe to only include patients in our cohort.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    if subject_col not in df.columns:\n&quot;,&#10;    &quot;        print(f\&quot;Warning: {subject_col} not found in dataframe columns\&quot;)\n&quot;,&#10;    &quot;        return df\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    initial_count = len(df)\n&quot;,&#10;    &quot;    filtered_df = df[df[subject_col].isin(cohort_subject_ids)]\n&quot;,&#10;    &quot;    final_count = len(filtered_df)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;Filtered from {initial_count} to {final_count} rows\&quot;)\n&quot;,&#10;    &quot;    return filtered_df\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def extract_time_window_data(df, first_admissions, time_col, window_hours=EFFECTIVE_WINDOW):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Extract data within the specified time window from first admission.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    # Merge with first admission times\n&quot;,&#10;    &quot;    df_with_admit = df.merge(first_admissions, on='subject_id', how='inner')\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Convert time column to datetime\n&quot;,&#10;    &quot;    df_with_admit[time_col] = pd.to_datetime(df_with_admit[time_col])\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Calculate time from admission\n&quot;,&#10;    &quot;    df_with_admit['hours_from_admit'] = (\n&quot;,&#10;    &quot;        df_with_admit[time_col] - df_with_admit['first_admittime']\n&quot;,&#10;    &quot;    ).dt.total_seconds() / 3600\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Filter to time window (0 to window_hours)\n&quot;,&#10;    &quot;    window_data = df_with_admit[\n&quot;,&#10;    &quot;        (df_with_admit['hours_from_admit'] &gt;= 0) &amp; \n&quot;,&#10;    &quot;        (df_with_admit['hours_from_admit'] &lt;= window_hours)\n&quot;,&#10;    &quot;    ]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;Extracted {len(window_data)} records within {window_hours}-hour window\&quot;)\n&quot;,&#10;    &quot;    return window_data\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Data processing functions defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 6. Feature Engineering&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def create_basic_features(data_df, subject_ids):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Create basic features from available data.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Filter to cohort patients\n&quot;,&#10;    &quot;    cohort_data = filter_to_cohort(data_df, subject_ids)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Group by subject_id and create aggregated features\n&quot;,&#10;    &quot;    features = []\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Get numeric columns for aggregation\n&quot;,&#10;    &quot;    numeric_cols = cohort_data.select_dtypes(include=[np.number]).columns\n&quot;,&#10;    &quot;    numeric_cols = [col for col in numeric_cols if col != 'subject_id']\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    if len(numeric_cols) &gt; 0:\n&quot;,&#10;    &quot;        # Calculate statistics for numeric columns\n&quot;,&#10;    &quot;        for col in numeric_cols:\n&quot;,&#10;    &quot;            col_stats = cohort_data.groupby('subject_id')[col].agg([\n&quot;,&#10;    &quot;                'mean', 'std', 'min', 'max', 'count'\n&quot;,&#10;    &quot;            ]).reset_index()\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Rename columns\n&quot;,&#10;    &quot;            col_stats.columns = ['subject_id'] + [\n&quot;,&#10;    &quot;                f'{col}_{stat}' for stat in ['mean', 'std', 'min', 'max', 'count']\n&quot;,&#10;    &quot;            ]\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            features.append(col_stats)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Merge all features\n&quot;,&#10;    &quot;    if features:\n&quot;,&#10;    &quot;        final_features = features[0]\n&quot;,&#10;    &quot;        for feat in features[1:]:\n&quot;,&#10;    &quot;            final_features = final_features.merge(feat, on='subject_id', how='outer')\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        print(f\&quot;Created {len(final_features.columns)-1} features for {len(final_features)} patients\&quot;)\n&quot;,&#10;    &quot;        return final_features\n&quot;,&#10;    &quot;    else:\n&quot;,&#10;    &quot;        print(\&quot;No numeric columns found for feature creation\&quot;)\n&quot;,&#10;    &quot;        return pd.DataFrame({'subject_id': subject_ids})\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Feature engineering functions defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 7. Target Variable Creation&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def create_example_targets(subject_ids):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Create example target variables for demonstration.\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    In a real implementation, these would be derived from:\n&quot;,&#10;    &quot;    - ADMISSIONS table for length of stay and readmissions\n&quot;,&#10;    &quot;    - PATIENTS table for mortality dates\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Create random targets for demonstration\n&quot;,&#10;    &quot;    np.random.seed(RANDOM_STATE)\n&quot;,&#10;    &quot;    n_patients = len(subject_ids)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    targets = pd.DataFrame({\n&quot;,&#10;    &quot;        'subject_id': subject_ids,\n&quot;,&#10;    &quot;        'mortality': np.random.binomial(1, 0.15, n_patients),  # 15% mortality rate\n&quot;,&#10;    &quot;        'prolonged_stay': np.random.binomial(1, 0.25, n_patients),  # 25% prolonged stay\n&quot;,&#10;    &quot;        'readmission': np.random.binomial(1, 0.20, n_patients)  # 20% readmission rate\n&quot;,&#10;    &quot;    })\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(\&quot;Example Target Variable Summary:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Mortality rate: {targets['mortality'].mean():.2%}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Prolonged stay rate: {targets['prolonged_stay'].mean():.2%}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Readmission rate: {targets['readmission'].mean():.2%}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return targets\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Target variable creation function defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 8. Model Training and Evaluation&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def prepare_data_for_modeling(features_df, targets_df):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Prepare final dataset for modeling.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Merge features with targets\n&quot;,&#10;    &quot;    final_data = features_df.merge(targets_df, on='subject_id', how='inner')\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Separate features and targets\n&quot;,&#10;    &quot;    feature_cols = [col for col in final_data.columns \n&quot;,&#10;    &quot;                   if col not in ['subject_id', 'mortality', 'prolonged_stay', 'readmission']]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    X = final_data[feature_cols]\n&quot;,&#10;    &quot;    y_mortality = final_data['mortality']\n&quot;,&#10;    &quot;    y_prolonged = final_data['prolonged_stay']\n&quot;,&#10;    &quot;    y_readmission = final_data['readmission']\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;Final dataset shape: {X.shape}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Features: {len(feature_cols)}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;Samples: {len(X)}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return X, y_mortality, y_prolonged, y_readmission, feature_cols\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def train_and_evaluate_model(X, y, target_name, model_name='RandomForest'):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Train and evaluate a model for a specific target.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Split data\n&quot;,&#10;    &quot;    X_train, X_test, y_train, y_test = train_test_split(\n&quot;,&#10;    &quot;        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n&quot;,&#10;    &quot;    )\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Handle missing values\n&quot;,&#10;    &quot;    imputer = SimpleImputer(strategy='median')\n&quot;,&#10;    &quot;    X_train_imputed = imputer.fit_transform(X_train)\n&quot;,&#10;    &quot;    X_test_imputed = imputer.transform(X_test)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Scale features\n&quot;,&#10;    &quot;    scaler = StandardScaler()\n&quot;,&#10;    &quot;    X_train_scaled = scaler.fit_transform(X_train_imputed)\n&quot;,&#10;    &quot;    X_test_scaled = scaler.transform(X_test_imputed)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Train model\n&quot;,&#10;    &quot;    if model_name == 'RandomForest':\n&quot;,&#10;    &quot;        model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n&quot;,&#10;    &quot;    elif model_name == 'LogisticRegression':\n&quot;,&#10;    &quot;        model = LogisticRegression(random_state=RANDOM_STATE)\n&quot;,&#10;    &quot;    else:\n&quot;,&#10;    &quot;        model = GradientBoostingClassifier(random_state=RANDOM_STATE)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    model.fit(X_train_scaled, y_train)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Evaluate\n&quot;,&#10;    &quot;    y_pred = model.predict(X_test_scaled)\n&quot;,&#10;    &quot;    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Calculate metrics\n&quot;,&#10;    &quot;    accuracy = accuracy_score(y_test, y_pred)\n&quot;,&#10;    &quot;    precision = precision_score(y_test, y_pred)\n&quot;,&#10;    &quot;    recall = recall_score(y_test, y_pred)\n&quot;,&#10;    &quot;    f1 = f1_score(y_test, y_pred)\n&quot;,&#10;    &quot;    auc = roc_auc_score(y_test, y_pred_proba)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n{model_name} - {target_name}:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  Accuracy: {accuracy:.3f}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  Precision: {precision:.3f}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  Recall: {recall:.3f}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  F1-Score: {f1:.3f}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;  AUC-ROC: {auc:.3f}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return {\n&quot;,&#10;    &quot;        'model': model_name,\n&quot;,&#10;    &quot;        'target': target_name,\n&quot;,&#10;    &quot;        'accuracy': accuracy,\n&quot;,&#10;    &quot;        'precision': precision,\n&quot;,&#10;    &quot;        'recall': recall,\n&quot;,&#10;    &quot;        'f1': f1,\n&quot;,&#10;    &quot;        'auc': auc\n&quot;,&#10;    &quot;    }\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Model training and evaluation functions defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 9. Main Pipeline Execution&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Execute the main pipeline\n&quot;,&#10;    &quot;if 'cohort_df' in locals() and cohort_df is not None and len(subject_ids) &gt; 0:\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Try to use test example data if available\n&quot;,&#10;    &quot;    try:\n&quot;,&#10;    &quot;        test_data = pd.read_csv('./data/test_example.csv')\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        # Create features from available data\n&quot;,&#10;    &quot;        print(\&quot;Creating features from test data...\&quot;)\n&quot;,&#10;    &quot;        features = create_basic_features(test_data, subject_ids)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        # Create example targets\n&quot;,&#10;    &quot;        print(\&quot;\\nCreating example target variables...\&quot;)\n&quot;,&#10;    &quot;        targets = create_example_targets(subject_ids)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        # Prepare data for modeling\n&quot;,&#10;    &quot;        print(\&quot;\\nPreparing data for modeling...\&quot;)\n&quot;,&#10;    &quot;        X, y_mortality, y_prolonged, y_readmission, feature_cols = prepare_data_for_modeling(\n&quot;,&#10;    &quot;            features, targets\n&quot;,&#10;    &quot;        )\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        if X.shape[1] &gt; 0:  # If we have features\n&quot;,&#10;    &quot;            # Train models for each target\n&quot;,&#10;    &quot;            results = []\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            targets_dict = {\n&quot;,&#10;    &quot;                'Mortality': y_mortality,\n&quot;,&#10;    &quot;                'Prolonged Stay': y_prolonged,\n&quot;,&#10;    &quot;                'Readmission': y_readmission\n&quot;,&#10;    &quot;            }\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            for target_name, y_target in targets_dict.items():\n&quot;,&#10;    &quot;                result = train_and_evaluate_model(X, y_target, target_name)\n&quot;,&#10;    &quot;                results.append(result)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Display results summary\n&quot;,&#10;    &quot;            results_df = pd.DataFrame(results)\n&quot;,&#10;    &quot;            print(\&quot;\\n\&quot; + \&quot;=\&quot;*50)\n&quot;,&#10;    &quot;            print(\&quot;RESULTS SUMMARY\&quot;)\n&quot;,&#10;    &quot;            print(\&quot;=\&quot;*50)\n&quot;,&#10;    &quot;            print(results_df.to_string(index=False))\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;        else:\n&quot;,&#10;    &quot;            print(\&quot;No features available for modeling\&quot;)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;    except FileNotFoundError:\n&quot;,&#10;    &quot;        print(\&quot;test_example.csv not found. Please add your MIMIC-III data files.\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;\\nTo complete this project, you'll need to:\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;1. Add MIMIC-III data files to the data/ directory\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;2. Modify the data loading functions to match your data structure\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;3. Implement proper feature engineering based on MIMIC-III tables\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;4. Create real target variables from admission and patient data\&quot;)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;else:\n&quot;,&#10;    &quot;    print(\&quot;No cohort data loaded. Please run the data loading cell first.\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 10. MIMIC-III Data Integration (TODO)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;When you have access to MIMIC-III data, implement these functions:&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def load_mimic_tables():\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Load MIMIC-III tables - implement based on your data source.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Example implementation for CSV files:\n&quot;,&#10;    &quot;    # tables = {\n&quot;,&#10;    &quot;    #     'admissions': pd.read_csv(DATA_PATH + 'ADMISSIONS.csv'),\n&quot;,&#10;    &quot;    #     'patients': pd.read_csv(DATA_PATH + 'PATIENTS.csv'),\n&quot;,&#10;    &quot;    #     'icustays': pd.read_csv(DATA_PATH + 'ICUSTAYS.csv'),\n&quot;,&#10;    &quot;    #     'chartevents': pd.read_csv(DATA_PATH + 'CHARTEVENTS.csv'),\n&quot;,&#10;    &quot;    #     'labevents': pd.read_csv(DATA_PATH + 'LABEVENTS.csv')\n&quot;,&#10;    &quot;    # }\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(\&quot;TODO: Implement MIMIC-III data loading\&quot;)\n&quot;,&#10;    &quot;    return None\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def create_real_targets(admissions_df, patients_df, subject_ids):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Create real target variables from MIMIC-III data.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # TODO: Implement actual target creation\n&quot;,&#10;    &quot;    # 1. Mortality: Use patients.DOD and admissions dates\n&quot;,&#10;    &quot;    # 2. Prolonged stay: Calculate LOS from admissions\n&quot;,&#10;    &quot;    # 3. Readmission: Find subsequent admissions within 30 days\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(\&quot;TODO: Implement real target variable creation\&quot;)\n&quot;,&#10;    &quot;    return None\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def create_clinical_features(chartevents_df, labevents_df, subject_ids):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Create clinical features from MIMIC-III data.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # TODO: Implement clinical feature engineering\n&quot;,&#10;    &quot;    # 1. Vital signs from chartevents\n&quot;,&#10;    &quot;    # 2. Lab values from labevents\n&quot;,&#10;    &quot;    # 3. Time-based aggregations within 42-hour window\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(\&quot;TODO: Implement clinical feature engineering\&quot;)\n&quot;,&#10;    &quot;    return None\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;MIMIC-III integration templates defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 11. Advanced Modeling (Optional)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def plot_roc_curves(models_dict, X_test, y_test, target_name):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Plot ROC curves for multiple models.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    plt.figure(figsize=(10, 6))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    for model_name, model in models_dict.items():\n&quot;,&#10;    &quot;        y_pred_proba = model.predict_proba(X_test)[:, 1]\n&quot;,&#10;    &quot;        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n&quot;,&#10;    &quot;        auc = roc_auc_score(y_test, y_pred_proba)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n&quot;,&#10;    &quot;    plt.xlabel('False Positive Rate')\n&quot;,&#10;    &quot;    plt.ylabel('True Positive Rate')\n&quot;,&#10;    &quot;    plt.title(f'ROC Curves - {target_name}')\n&quot;,&#10;    &quot;    plt.legend()\n&quot;,&#10;    &quot;    plt.grid(True)\n&quot;,&#10;    &quot;    plt.show()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def perform_cross_validation(X, y, target_name):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Perform cross-validation for model comparison.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    models = {\n&quot;,&#10;    &quot;        'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE),\n&quot;,&#10;    &quot;        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n&quot;,&#10;    &quot;        'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE)\n&quot;,&#10;    &quot;    }\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    cv_results = []\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    for model_name, model in models.items():\n&quot;,&#10;    &quot;        scores = cross_val_score(model, X, y, cv=CV_FOLDS, scoring='roc_auc')\n&quot;,&#10;    &quot;        cv_results.append({\n&quot;,&#10;    &quot;            'Model': model_name,\n&quot;,&#10;    &quot;            'Target': target_name,\n&quot;,&#10;    &quot;            'Mean AUC': scores.mean(),\n&quot;,&#10;    &quot;            'Std AUC': scores.std()\n&quot;,&#10;    &quot;        })\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        print(f\&quot;{model_name} - {target_name}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return cv_results\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Advanced modeling functions defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 12. Results Export and Documentation&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def save_results(results_df, filename='model_results.csv'):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Save results to CSV file.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n&quot;,&#10;    &quot;    filepath = f'./results_{timestamp}_{filename}'\n&quot;,&#10;    &quot;    results_df.to_csv(filepath, index=False)\n&quot;,&#10;    &quot;    print(f\&quot;Results saved to {filepath}\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;def generate_summary_report():\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Generate a summary report for the paper.\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    if 'results_df' in locals():\n&quot;,&#10;    &quot;        print(\&quot;\\n\&quot; + \&quot;=\&quot;*60)\n&quot;,&#10;    &quot;        print(\&quot;PROJECT SUMMARY REPORT\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;=\&quot;*60)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        print(f\&quot;\\nDataset Information:\&quot;)\n&quot;,&#10;    &quot;        print(f\&quot;- Cohort size: {len(subject_ids)} patients\&quot;)\n&quot;,&#10;    &quot;        print(f\&quot;- Prediction window: {EFFECTIVE_WINDOW} hours\&quot;)\n&quot;,&#10;    &quot;        print(f\&quot;- Gap before prediction: {GAP_HOURS} hours\&quot;)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        print(f\&quot;\\nModel Performance Summary:\&quot;)\n&quot;,&#10;    &quot;        for target in ['Mortality', 'Prolonged Stay', 'Readmission']:\n&quot;,&#10;    &quot;            target_results = results_df[results_df['target'] == target]\n&quot;,&#10;    &quot;            if len(target_results) &gt; 0:\n&quot;,&#10;    &quot;                best_model = target_results.loc[target_results['auc'].idxmax()]\n&quot;,&#10;    &quot;                print(f\&quot;\\n{target}:\&quot;)\n&quot;,&#10;    &quot;                print(f\&quot;  Best Model: {best_model['model']}\&quot;)\n&quot;,&#10;    &quot;                print(f\&quot;  AUC-ROC: {best_model['auc']:.3f}\&quot;)\n&quot;,&#10;    &quot;                print(f\&quot;  Precision: {best_model['precision']:.3f}\&quot;)\n&quot;,&#10;    &quot;                print(f\&quot;  Recall: {best_model['recall']:.3f}\&quot;)\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        print(f\&quot;\\nNext Steps for Paper:\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;1. Implement real MIMIC-III data loading\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;2. Create clinical features from vital signs and labs\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;3. Validate results with cross-validation\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;4. Compare with clinical baselines\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;5. Interpret feature importance\&quot;)\n&quot;,&#10;    &quot;        print(\&quot;6. Discuss clinical implications\&quot;)\n&quot;,&#10;    &quot;    else:\n&quot;,&#10;    &quot;        print(\&quot;No results available. Run the pipeline first.\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;Results export functions defined\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## 13. Next Steps and TODO Summary\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;###  Completed:\n&quot;,&#10;    &quot;- Project structure and pipeline framework\n&quot;,&#10;    &quot;- Basic modeling and evaluation functions\n&quot;,&#10;    &quot;- Example pipeline with dummy data\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;###  In Progress:\n&quot;,&#10;    &quot;- Cohort data loading (initial_cohort.csv loaded)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;###  TODO:\n&quot;,&#10;    &quot;1. **Add MIMIC-III Data Files**:\n&quot;,&#10;    &quot;   - ADMISSIONS.csv\n&quot;,&#10;    &quot;   - PATIENTS.csv\n&quot;,&#10;    &quot;   - CHARTEVENTS.csv\n&quot;,&#10;    &quot;   - LABEVENTS.csv\n&quot;,&#10;    &quot;   - ICUSTAYS.csv (optional)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;2. **Implement Real Data Pipeline**:\n&quot;,&#10;    &quot;   - Load MIMIC-III tables\n&quot;,&#10;    &quot;   - Filter to cohort patients\n&quot;,&#10;    &quot;   - Extract 48-hour time windows\n&quot;,&#10;    &quot;   - Create clinical features\n&quot;,&#10;    &quot;   - Generate real target variables\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;3. **Enhanced Features**:\n&quot;,&#10;    &quot;   - Vital signs (HR, BP, temp, SpO2, RR)\n&quot;,&#10;    &quot;   - Lab values (CBC, chemistry, ABG)\n&quot;,&#10;    &quot;   - Demographics (age, gender)\n&quot;,&#10;    &quot;   - Medications (optional)\n&quot;,&#10;    &quot;   - Procedures (optional)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;4. **Model Improvements**:\n&quot;,&#10;    &quot;   - Hyperparameter tuning\n&quot;,&#10;    &quot;   - Advanced algorithms (XGBoost)\n&quot;,&#10;    &quot;   - Ensemble methods\n&quot;,&#10;    &quot;   - Feature selection\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;5. **Validation and Reporting**:\n&quot;,&#10;    &quot;   - Cross-validation\n&quot;,&#10;    &quot;   - Feature importance analysis\n&quot;,&#10;    &quot;   - Clinical interpretation\n&quot;,&#10;    &quot;   - Paper documentation\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;### Current Status:\n&quot;,&#10;    &quot;Ready to run with example data. Replace TODO sections with real MIMIC-III implementation.&quot;&#10;   ]&#10;  }&#10; ],&#10; &quot;metadata&quot;: {&#10;  &quot;kernelspec&quot;: {&#10;   &quot;display_name&quot;: &quot;Python 3&quot;,&#10;   &quot;language&quot;: &quot;python&quot;,&#10;   &quot;name&quot;: &quot;python3&quot;&#10;  },&#10;  &quot;language_info&quot;: {&#10;   &quot;codemirror_mode&quot;: {&#10;    &quot;name&quot;: &quot;ipython&quot;,&#10;    &quot;version&quot;: 3&#10;   },&#10;   &quot;file_extension&quot;: &quot;.py&quot;,&#10;   &quot;mimetype&quot;: &quot;text/x-python&quot;,&#10;   &quot;name&quot;: &quot;python&quot;,&#10;   &quot;nbconvert_exporter&quot;: &quot;python&quot;,&#10;   &quot;pygments_lexer&quot;: &quot;ipython3&quot;,&#10;   &quot;version&quot;: &quot;3.8.5&quot;&#10;  }&#10; },&#10; &quot;nbformat&quot;: 4,&#10; &quot;nbformat_minor&quot;: 4&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>