{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791201e3",
   "metadata": {},
   "source": [
    "End-to-end (current focus: Readmission with XGBoost + Optuna). Structure prepared for mortality & prolonged LOS later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment & core imports\n",
    "import os, sys, json, random, platform, importlib, datetime\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "PROJECT_ROOT = (Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd())\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\n",
    "RUNS_ROOT = PROJECT_ROOT / 'runs'\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data dir exists: {(DATA_DIR).exists()}\")\n",
    "VERSIONS = {'python': sys.version.split()[0], 'platform': platform.platform()}\n",
    "for pkg in ['xgboost','optuna','shap','sklearn','pandas','numpy']:\n",
    "    try:\n",
    "        m = importlib.import_module(pkg if pkg != 'sklearn' else 'sklearn')\n",
    "        VERSIONS[pkg] = getattr(m,'__version__','?')\n",
    "    except Exception as e:\n",
    "        VERSIONS[pkg] = f'NA({e})'\n",
    "print('Versions:', json.dumps(VERSIONS, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560f373",
   "metadata": {},
   "source": [
    "### Labels\n",
    "Load readmission labels (or synthesize) and report prevalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c3dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or generate labels (readmission focus)\n",
    "import pandas as pd, random\n",
    "LABELS_PATH = None\n",
    "LABEL_CANDIDATES = [DATA_DIR / 'labels.csv', PROJECT_ROOT / 'labels.csv']\n",
    "for cand in LABEL_CANDIDATES:\n",
    "    if cand.exists():\n",
    "        LABELS_PATH = cand\n",
    "        break\n",
    "labels_df = None\n",
    "if LABELS_PATH is not None:\n",
    "    labels_df = pd.read_csv(LABELS_PATH)\n",
    "else:\n",
    "    cohort_path = DATA_DIR / 'initial_cohort.csv'\n",
    "    if not cohort_path.exists():\n",
    "        raise FileNotFoundError('initial_cohort.csv missing; cannot synthesize labels')\n",
    "    subj = pd.read_csv(cohort_path)\n",
    "    random.seed(SEED)\n",
    "    synth = pd.Series([1 if random.random() < 0.043 else 0 for _ in range(len(subj))])\n",
    "    labels_df = pd.DataFrame({'subject_id': subj['subject_id'],'hadm_id': -1,'readmission_label': synth.values})\n",
    "    LABELS_PATH = '<synthetic>'\n",
    "# Normalize column name\n",
    "if 'readmission_label' not in labels_df.columns:\n",
    "    lower_map = {c.lower(): c for c in labels_df.columns}\n",
    "    for alias in ['readmission_label','readmission','readmit','readmit_30d','readmission_30d']:\n",
    "        if alias in lower_map:\n",
    "            if lower_map[alias] != 'readmission_label':\n",
    "                labels_df.rename(columns={lower_map[alias]:'readmission_label'}, inplace=True)\n",
    "            break\n",
    "if 'readmission_label' not in labels_df.columns:\n",
    "    raise ValueError('Could not identify readmission label column')\n",
    "labels_df = labels_df.drop_duplicates('subject_id')\n",
    "labels_df['readmission_label'] = labels_df['readmission_label'].astype(int)\n",
    "assert labels_df['subject_id'].isna().sum()==0\n",
    "prev = labels_df['readmission_label'].mean()\n",
    "print(f\"Labels source: {LABELS_PATH} | shape={labels_df.shape} | prevalence={prev:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4f32a",
   "metadata": {},
   "source": [
    "### Features\n",
    "Load (or regenerate) prepared feature matrix aligned to subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature matrix (regenerate if tiny/corrupt)\n",
    "import pandas as pd, json, hashlib\n",
    "feature_path = ARTIFACTS_DIR / 'features_full.parquet'\n",
    "regenerated = False\n",
    "if feature_path.exists() and feature_path.stat().st_size < 1000:\n",
    "    print('Corrupted feature parquet detected; attempting regeneration.')\n",
    "    cache_dir = DATA_DIR / 'extracted_cache'\n",
    "    try:\n",
    "        from src.features import build_features, build_feature_provenance  # type: ignore\n",
    "        def load_opt(name):\n",
    "            p = cache_dir / name\n",
    "            return pd.read_parquet(p) if p.exists() else None\n",
    "        demo = load_opt('demographics.parquet')\n",
    "        first_adm = load_opt('first_admissions.parquet')\n",
    "        vitals = load_opt('vitals_48h.parquet')\n",
    "        labs = load_opt('labs_48h.parquet')\n",
    "        rx = load_opt('prescriptions_48h.parquet')\n",
    "        proc = load_opt('procedures_48h.parquet')\n",
    "        feats = build_features(first_adm, demo, vitals, labs, rx, proc)\n",
    "        feats = feats.reindex(labels_df['subject_id']).fillna(0.0)\n",
    "        feats.to_parquet(feature_path)\n",
    "        prov = build_feature_provenance(feats)\n",
    "        (ARTIFACTS_DIR / 'feature_provenance.json').write_text(json.dumps(prov, indent=2))\n",
    "        (ARTIFACTS_DIR / 'feature_columns.json').write_text(json.dumps(list(feats.columns)))\n",
    "        regenerated = True\n",
    "        print('Regenerated features:', feats.shape)\n",
    "    except Exception as e:\n",
    "        print('Feature regeneration failed:', e)\n",
    "if not feature_path.exists():\n",
    "    raise FileNotFoundError(f'Missing {feature_path}; ensure extraction step executed.')\n",
    "feature_df = pd.read_parquet(feature_path)\n",
    "if 'subject_id' in feature_df.columns:\n",
    "    feature_df = feature_df.set_index('subject_id')\n",
    "feature_df = feature_df.reindex(labels_df['subject_id']).fillna(0.0)\n",
    "print('Features loaded shape:', feature_df.shape, '| regenerated' if regenerated else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa08550",
   "metadata": {},
   "source": [
    "### Train/Validation/Test Split\n",
    "Create 60/20/20 stratified split and compute imbalance weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/valid/test split (60/20/20) + class weight factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "readmit_y = labels_df['readmission_label'].astype(int).to_numpy()\n",
    "subject_index = feature_df.index.to_numpy()\n",
    "X = feature_df.values\n",
    "X_tr, X_temp, y_tr, y_temp, sid_tr, sid_temp = train_test_split(\n",
    "    X, readmit_y, subject_index, test_size=0.4, stratify=readmit_y, random_state=SEED)\n",
    "X_val, X_te, y_val, y_te, sid_val, sid_te = train_test_split(\n",
    "    X_temp, y_temp, sid_temp, test_size=0.5, stratify=y_temp, random_state=SEED)\n",
    "pos_rate = y_tr.mean(); scale_pos_weight = (1-pos_rate)/max(pos_rate,1e-6)\n",
    "print(f'Split -> train {X_tr.shape} valid {X_val.shape} test {X_te.shape} | pos_rate_train={pos_rate:.4f} | spw≈{scale_pos_weight:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281cc739",
   "metadata": {},
   "source": [
    "### Metrics Helpers\n",
    "Utility functions to compute threshold-dependent metrics and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb41202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "C_FP = 1.0; C_FN = 5.0\n",
    "beta = 2.0\n",
    "\n",
    "def metrics_at(proba, y, thr):\n",
    "    pred = (proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    cost = C_FP*fp + C_FN*fn\n",
    "    f1 = f1_score(y, pred)\n",
    "    prec = tp/(tp+fp+1e-9); rec = tp/(tp+fn+1e-9)\n",
    "    fbeta = (1+beta**2)*prec*rec/(beta**2*prec+rec+1e-9)\n",
    "    return dict(f1=f1, precision=prec, recall=rec, cost=cost, fbeta=fbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c159178",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "Train a simple class-weighted logistic regression for reference AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b62e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "baseline_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy='median')),\n",
    "    (\"sc\", StandardScaler(with_mean=False)),\n",
    "    (\"lr\", LogisticRegression(max_iter=500, class_weight='balanced', solver='liblinear'))\n",
    "])\n",
    "baseline_pipe.fit(X_tr, y_tr)\n",
    "baseline_val_proba = baseline_pipe.predict_proba(X_val)[:,1]\n",
    "baseline_auc = roc_auc_score(y_val, baseline_val_proba)\n",
    "print('Baseline Logistic Validation AUC:', round(baseline_auc,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c9077e",
   "metadata": {},
   "source": [
    "### XGBoost Data Structures\n",
    "Create DMatrix objects for training, validation, and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea999950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost test DMatrix (train/val splits handled by CV inside objective)\n",
    "import xgboost as xgb\n",
    "D_te = xgb.DMatrix(X_te, label=y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a6fec7",
   "metadata": {},
   "source": [
    "### Optuna Study Setup\n",
    "Initialize study for AUC maximization with TPE + median pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna, xgboost as xgb\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=SEED), pruner=MedianPruner())\n",
    "print('Study created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f79cc9b",
   "metadata": {},
   "source": [
    "### Objective Definition\n",
    "Define Optuna objective: 5-fold stratified CV with early stopping (mean validation AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna objective: 5-fold stratified CV AUC with early stopping\n",
    "import numpy as np, xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "EARLY_STOP = 50\n",
    "MAX_ROUNDS = 1200\n",
    "N_FOLDS = 5\n",
    "def objective(trial: optuna.Trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'tree_method': 'hist',\n",
    "        'eta': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1.0, 10.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 5.0, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "    }\n",
    "    rounds = trial.suggest_int('n_estimators', 300, MAX_ROUNDS)\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    fold_aucs = []\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_tr, y_tr), 1):\n",
    "        Xtr_f, Xva_f = X_tr[tr_idx], X_tr[va_idx]\n",
    "        ytr_f, yva_f = y_tr[tr_idx], y_tr[va_idx]\n",
    "        Dtr = xgb.DMatrix(Xtr_f, label=ytr_f)\n",
    "        Dva = xgb.DMatrix(Xva_f, label=yva_f)\n",
    "        booster = xgb.train(params, Dtr, num_boost_round=rounds,\n",
    "                            evals=[(Dva,'valid')],\n",
    "                            early_stopping_rounds=EARLY_STOP, verbose_eval=False)\n",
    "        fold_aucs.append(booster.best_score)\n",
    "    mean_auc = float(np.mean(fold_aucs))\n",
    "    trial.set_user_attr('fold_aucs', fold_aucs)\n",
    "    trial.set_user_attr('cv_mean_auc', mean_auc)\n",
    "    return mean_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae8d32",
   "metadata": {},
   "source": [
    "### Run Hyperparameter Search\n",
    "Execute trials optimizing mean 5-fold CV AUC (early stopping each fold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d964627",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 15  # adjust upward for thorough search\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "print('Best AUC:', study.best_value)\n",
    "print('Best Params:', study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5bedf",
   "metadata": {},
   "source": [
    "### Inspect Trials\n",
    "Overview of trials and (optional) optimization history plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16676a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df = study.trials_dataframe()\n",
    "print('Trials:', trials_df.shape)\n",
    "try:\n",
    "    optuna.visualization.plot_optimization_history(study)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b693a61",
   "metadata": {},
   "source": [
    "### Final Model Training\n",
    "Train final booster on combined train+validation using best params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7400eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb, numpy as np\n",
    "X_tr_full = np.vstack([X_tr, X_val])\n",
    "y_tr_full = np.concatenate([y_tr, y_val])\n",
    "D_full = xgb.DMatrix(X_tr_full, label=y_tr_full)\n",
    "params = study.best_params.copy()\n",
    "# Map naming differences\n",
    "params_fixed = {\n",
    "    'objective':'binary:logistic','eval_metric':'auc','tree_method':'hist','eta':params['learning_rate'],\n",
    "    'max_depth':params['max_depth'],'min_child_weight':params['min_child_weight'],'subsample':params['subsample'],\n",
    "    'colsample_bytree':params['colsample_bytree'],'lambda':params['lambda'],'alpha':params['alpha'],\n",
    "    'gamma':params['gamma'],'scale_pos_weight':scale_pos_weight,\n",
    "}\n",
    "final_rounds = study.best_params['n_estimators']\n",
    "final_booster = xgb.train(params_fixed, D_full, num_boost_round=final_rounds, evals=[(D_full,'train')], verbose_eval=False)\n",
    "print('Final booster trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432add8f",
   "metadata": {},
   "source": [
    "### Calibration & Threshold\n",
    "Fit isotonic on validation; pick F1-optimal threshold on calibrated validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ab5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual isotonic calibration (avoids CalibratedClassifierCV API issues)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import numpy as np\n",
    "\n",
    "base_params = {k:params_fixed[k] for k in ['max_depth','subsample','colsample_bytree']}\n",
    "base_model = XGBClassifier(**base_params,\n",
    "                           learning_rate=params['learning_rate'],\n",
    "                           n_estimators=final_rounds,\n",
    "                           min_child_weight=params['min_child_weight'],\n",
    "                           reg_lambda=params['lambda'],\n",
    "                           reg_alpha=params['alpha'],\n",
    "                           gamma=params['gamma'],\n",
    "                           objective='binary:logistic',\n",
    "                           tree_method='hist',\n",
    "                           scale_pos_weight=scale_pos_weight,\n",
    "                           eval_metric='logloss',\n",
    "                           verbosity=0)\n",
    "# Fit on training only (validation reserved for calibration mapping)\n",
    "base_model.fit(X_tr, y_tr)\n",
    "val_proba_raw = base_model.predict_proba(X_val)[:,1]\n",
    "iso = IsotonicRegression(out_of_bounds='clip')\n",
    "iso.fit(val_proba_raw, y_val)\n",
    "print('Isotonic calibration fitted on validation set.')\n",
    "\n",
    "def predict_calibrated(X):\n",
    "    return iso.transform(base_model.predict_proba(X)[:,1])\n",
    "\n",
    "# Derive operating threshold on calibrated validation probabilities\n",
    "val_cal = predict_calibrated(X_val)\n",
    "ths = np.linspace(0.01,0.9,300)\n",
    "threshold_info = None\n",
    "for t in ths:\n",
    "    m = metrics_at(val_cal, y_val, t)\n",
    "    if (threshold_info is None) or (m['f1'] > threshold_info['f1']):\n",
    "        threshold_info = {**m, 'threshold': float(t)}\n",
    "print('Selected threshold (calibrated validation):', threshold_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844e92f",
   "metadata": {},
   "source": [
    "### Test Evaluation\n",
    "Apply calibrated model + selected threshold; report core metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate calibrated model on test set\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import json\n",
    "cal_proba_test = predict_calibrated(X_te)\n",
    "auc = roc_auc_score(y_te, cal_proba_test)\n",
    "pr = average_precision_score(y_te, cal_proba_test)\n",
    "brier = brier_score_loss(y_te, cal_proba_test)\n",
    "thr = threshold_info['threshold']\n",
    "th_metrics = metrics_at(cal_proba_test, y_te, thr)\n",
    "report = {\n",
    "    'auc': float(auc),\n",
    "    'pr_auc': float(pr),\n",
    "    'brier': float(brier),\n",
    "    'threshold': float(thr),\n",
    "    'f1_at_threshold': float(th_metrics['f1']),\n",
    "    'precision_at_threshold': float(th_metrics['precision']),\n",
    "    'recall_at_threshold': float(th_metrics['recall']),\n",
    "    'cost_at_threshold': float(th_metrics['cost']),\n",
    "}\n",
    "print(json.dumps(report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03cc3e",
   "metadata": {},
   "source": [
    "### SHAP Summary\n",
    "Compute SHAP values on a sample for global importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76582e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP global explanation (sample subset)\n",
    "import numpy as np\n",
    "try:\n",
    "    import shap\n",
    "    X_tr_full = np.vstack([X_tr, X_val])\n",
    "    sample_idx = np.random.choice(X_tr_full.shape[0], size=min(400, X_tr_full.shape[0]), replace=False)\n",
    "    X_sample = X_tr_full[sample_idx]\n",
    "    import xgboost as xgb\n",
    "    explainer = shap.TreeExplainer(final_booster)\n",
    "    shap_val = explainer.shap_values(X_sample)\n",
    "    mean_abs = np.abs(shap_val).mean(axis=0)\n",
    "    top_order = np.argsort(-mean_abs)[:20]\n",
    "    print('Top SHAP feature indices (first 10 of 20):', top_order[:10])\n",
    "except Exception as e:\n",
    "    print('SHAP skipped:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36d889",
   "metadata": {},
   "source": [
    "### Bootstrap AUC CI\n",
    "Estimate uncertainty of test ROC AUC via stratified bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eaa6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap 95% CI for test AUC\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "R = 1000\n",
    "rng = np.random.default_rng(42)\n",
    "auc_samples = []\n",
    "for _ in range(R):\n",
    "    idx_pos = np.where(y_te==1)[0]\n",
    "    idx_neg = np.where(y_te==0)[0]\n",
    "    b_pos = rng.choice(idx_pos, size=len(idx_pos), replace=True)\n",
    "    b_neg = rng.choice(idx_neg, size=len(idx_neg), replace=True)\n",
    "    b_idx = np.concatenate([b_pos, b_neg])\n",
    "    auc_samples.append(roc_auc_score(y_te[b_idx], cal_proba_test[b_idx]))\n",
    "auc_samples = np.array(auc_samples)\n",
    "ci_low, ci_high = np.percentile(auc_samples, [2.5,97.5])\n",
    "print(f'AUC bootstrap mean={auc_samples.mean():.4f} 95% CI=({ci_low:.4f},{ci_high:.4f}) n={R}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90e623",
   "metadata": {},
   "source": [
    "### 5-Fold CV Comparison\n",
    "Compare XGB vs logistic AUC on combined train+validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold CV AUC comparison: XGB vs Logistic baseline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "X_cv = np.vstack([X_tr, X_val])\n",
    "y_cv = np.concatenate([y_tr, y_val])\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "auc_xgb = []; auc_lr = []\n",
    "params_cv = params_fixed.copy(); params_cv['eval_metric'] = 'auc'\n",
    "for fold,(tr_idx, va_idx) in enumerate(skf.split(X_cv, y_cv), 1):\n",
    "    Xtr, Xva = X_cv[tr_idx], X_cv[va_idx]; ytr, yva = y_cv[tr_idx], y_cv[va_idx]\n",
    "    lr_pipe = Pipeline([('imp', SimpleImputer(strategy='median')),('sc', StandardScaler(with_mean=False)),('lr', LogisticRegression(max_iter=500, class_weight='balanced', solver='liblinear'))])\n",
    "    lr_pipe.fit(Xtr, ytr)\n",
    "    auc_lr.append(roc_auc_score(yva, lr_pipe.predict_proba(Xva)[:,1]))\n",
    "    Dtr = xgb.DMatrix(Xtr, label=ytr); Dva = xgb.DMatrix(Xva, label=yva)\n",
    "    booster = xgb.train(params_cv, Dtr, num_boost_round=params_cv.get('n_estimators', final_rounds), evals=[(Dva,'valid')], verbose_eval=False)\n",
    "    auc_xgb.append(roc_auc_score(yva, booster.predict(Dva)))\n",
    "print('CV AUC Logistic: mean', f'{np.mean(auc_lr):.4f}', '±', f'{np.std(auc_lr):.4f}')\n",
    "print('CV AUC XGB     : mean', f'{np.mean(auc_xgb):.4f}', '±', f'{np.std(auc_xgb):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b07989",
   "metadata": {},
   "source": [
    "### Persist Artifacts\n",
    "Save model, calibration objects, metrics, threshold, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8520590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist artifacts (model + calibration + metadata)\n",
    "import json, joblib, hashlib, time, subprocess\n",
    "OUT_DIR = ARTIFACTS_DIR\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "final_booster.save_model(str(OUT_DIR / 'model_readmission.json'))\n",
    "joblib.dump(iso, OUT_DIR / 'isotonic.joblib')\n",
    "joblib.dump(base_model, OUT_DIR / 'base_model.joblib')\n",
    "with open(OUT_DIR / 'best_params.json','w',encoding='utf-8') as f: json.dump(study.best_params, f, indent=2)\n",
    "with open(OUT_DIR / 'metrics.json','w',encoding='utf-8') as f: json.dump(report, f, indent=2)\n",
    "with open(OUT_DIR / 'threshold.txt','w') as f: f.write(str(report['threshold']))\n",
    "try:\n",
    "    git_commit = subprocess.check_output(['git','rev-parse','HEAD'], text=True).strip()\n",
    "except Exception:\n",
    "    git_commit = 'UNKNOWN'\n",
    "feat_cols = list(feature_df.columns)\n",
    "feat_sig = hashlib.sha256(('|'.join(feat_cols)).encode()).hexdigest()[:16]\n",
    "meta = {\n",
    "    'saved_utc': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n",
    "    'git_commit': git_commit,\n",
    "    'n_features': len(feat_cols),\n",
    "    'feature_sig_sha256_16': feat_sig,\n",
    "    'prevalence_train': float(y_tr.mean()),\n",
    "    'prevalence_valid': float(y_val.mean()),\n",
    "    'prevalence_test': float(y_te.mean()),\n",
    "    'optuna_best_value': float(study.best_value),\n",
    "    'threshold_info': threshold_info,\n",
    "    'calibration': 'isotonic_on_validation',\n",
    "}\n",
    "with open(OUT_DIR / 'run_metadata.json','w',encoding='utf-8') as f: json.dump(meta, f, indent=2)\n",
    "print('Artifacts saved ->', OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf753cc",
   "metadata": {},
   "source": [
    "### Experiment Registry\n",
    "Append current run metrics to CSV registry for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append metrics to experiment registry\n",
    "import csv, time\n",
    "REG_PATH = PROJECT_ROOT / 'experiment_registry.csv'\n",
    "row = {'ts': time.time(), **report}\n",
    "write_header = not REG_PATH.exists()\n",
    "with open(REG_PATH,'a',newline='') as f:\n",
    "    w = csv.DictWriter(f, fieldnames=row.keys())\n",
    "    if write_header: w.writeheader()\n",
    "    w.writerow(row)\n",
    "print('Logged metrics to', REG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa550ed7",
   "metadata": {},
   "source": [
    "### Single Prediction Demo\n",
    "Show calibrated probability for one test instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a863509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single example calibrated probability demo\n",
    "raw_proba = final_booster.predict(D_te)[0]\n",
    "calib_proba = iso.transform([raw_proba])[0]\n",
    "print('Single test example calibrated probability:', float(calib_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fdfeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sample_idx = np.random.choice(X_tr_full.shape[0], size=min(400, X_tr_full.shape[0]), replace=False)\n",
    "X_sample = X_tr_full[sample_idx]\n",
    "try:\n",
    "    import shap\n",
    "    explainer = shap.TreeExplainer(final_booster)\n",
    "    shap_val = explainer.shap_values(X_sample)\n",
    "    mean_abs = np.abs(shap_val).mean(axis=0)\n",
    "    top_order = np.argsort(-mean_abs)[:20]\n",
    "    print('Top 20 SHAP feature indices:', top_order[:10], '...')\n",
    "except Exception as e:\n",
    "    print('SHAP skipped:', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
