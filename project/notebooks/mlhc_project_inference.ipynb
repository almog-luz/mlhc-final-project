{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5713f6552d09a35e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# MLHC: Lean Inference\n",
    "\n",
    "Use saved models to produce predictions from 0â€“48h features; optionally compute metrics if labels are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the repo root and project/ are on sys.path for 'project' imports\n",
    "import sys, os, pathlib\n",
    "candidates = []\n",
    "cwd = pathlib.Path().resolve()\n",
    "candidates.append(cwd)\n",
    "candidates.append(cwd.parent)\n",
    "candidates.append(cwd / 'project')\n",
    "candidates.append(cwd.parent / 'project')\n",
    "for p in candidates:\n",
    "    p = str(p)\n",
    "    if os.path.isdir(p) and p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "try:\n",
    "    import project  # noqa: F401\n",
    "    print('Import check OK. sys.path[0]=', sys.path[0])\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'Failed to import project package. sys.path= {sys.path}') from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GCP project and input paths\n",
    "GCP_PROJECT_ID = 'YOUR_GCP_PROJECT'  # <-- set this\n",
    "SUBJECTS_CSV = os.path.join('project', 'data', 'test_example.csv')  # CSV with subject_id column\n",
    "LABELS_CSV = None  # optional: set to labels CSV with subject_id and any of *_label columns\n",
    "GCP_PROJECT_ID, SUBJECTS_CSV, LABELS_CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "from google.cloud import bigquery\n",
    "from project.unseen_data_evaluation import run_pipeline_on_unseen_data\n",
    "import pandas as pd\n",
    "\n",
    "bq_client = bigquery.Client(project=GCP_PROJECT_ID)\n",
    "subjects_df = pd.read_csv(SUBJECTS_CSV)\n",
    "if 'subject_id' not in subjects_df.columns:\n",
    "    raise ValueError('SUBJECTS_CSV must contain subject_id column')\n",
    "subject_ids = subjects_df['subject_id'].dropna().astype(int).tolist()\n",
    "preds = run_pipeline_on_unseen_data(subject_ids, bq_client)\n",
    "print('Predictions shape:', preds.shape)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46006d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: compute metrics if LABELS_CSV provided\n",
    "if LABELS_CSV:\n",
    "    from project.metrics_utils import compute_binary_metrics, metrics_to_dict\n",
    "    import json\n",
    "    labels_df = pd.read_csv(LABELS_CSV)\n",
    "    if 'subject_id' not in labels_df.columns:\n",
    "        raise ValueError('Labels CSV must contain subject_id')\n",
    "    # Normalize possible variant label column names\n",
    "    rename_map = {\n",
    "        'prolonged_los': 'prolonged_los_label',\n",
    "        'prolonged_los_label': 'prolonged_los_label',\n",
    "        'prolonged_los>7d': 'prolonged_los_label',\n",
    "        'mortality': 'mortality_label',\n",
    "        'readmission': 'readmission_label',\n",
    "    }\n",
    "    cols_lower = {c: c for c in labels_df.columns}\n",
    "    for c in list(labels_df.columns):\n",
    "        lc = c.lower()\n",
    "        if lc in rename_map:\n",
    "            cols_lower[c] = rename_map[lc]\n",
    "        elif lc.endswith('_label'):\n",
    "            cols_lower[c] = lc\n",
    "    labels_df = labels_df.rename(columns=cols_lower)\n",
    "    merged = preds.merge(labels_df, on='subject_id', how='inner')\n",
    "    results = {}\n",
    "    for y_col, p_col, name in [\n",
    "        ('mortality_label', 'mortality_proba', 'mortality'),\n",
    "        ('prolonged_los_label', 'prolonged_LOS_proba', 'prolonged_los'),\n",
    "        ('readmission_label', 'readmission_proba', 'readmission'),\n",
    "    ]:\n",
    "        if y_col in merged.columns and p_col in merged.columns:\n",
    "            y = merged[y_col].dropna().astype(int).values\n",
    "            p = merged.loc[merged[y_col].dropna().index, p_col].astype(float).values\n",
    "            if y.size:\n",
    "                m = compute_binary_metrics(y, p, threshold_objective='f1')\n",
    "                results[name] = metrics_to_dict(m)\n",
    "    print(json.dumps(results, indent=2))\n",
    "else:\n",
    "    print('Set LABELS_CSV to compute metrics.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c21f01ffcb45cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
