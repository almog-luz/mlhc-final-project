{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:15:47.278276Z",
     "start_time": "2025-09-05T06:15:43.377962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Connect to Google BigQuery ---\n",
    "from google.cloud import bigquery\n",
    "# If you have set up authentication (e.g., GOOGLE_APPLICATION_CREDENTIALS env variable), this will work.\n",
    "# Otherwise, you may need to authenticate manually or use a service account key.\n",
    "# Replace 'your-gcp-project-id' with your actual GCP project ID.\n",
    "bq_client = bigquery.Client(project='ml-for-healthcare-2025')\n",
    "print('Connected to BigQuery!')\n"
   ],
   "id": "13d246a2b2a0ad5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to BigQuery!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:15:47.315713Z",
     "start_time": "2025-09-05T06:15:47.295681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "\n",
    "# Load the initial cohort subject IDs\n",
    "cohort_path = 'data/initial_cohort.csv'  # relative to the project folder\n",
    "if not os.path.exists(cohort_path):\n",
    "    # fallback for alternate execution contexts\n",
    "    alt_path = '../project/data/initial_cohort.csv'\n",
    "    if os.path.exists(alt_path):\n",
    "        cohort_path = alt_path\n",
    "\n",
    "cohort_df = pd.read_csv(cohort_path)\n",
    "if 'subject_id' not in cohort_df.columns:\n",
    "    raise ValueError(\"initial_cohort.csv must contain a 'subject_id' column\")\n",
    "subject_ids: List[int] = cohort_df['subject_id'].dropna().astype(int).tolist()\n",
    "print(f\"Loaded {len(subject_ids)} subject IDs for the cohort from {cohort_path}.\")\n"
   ],
   "id": "c30bc4c4482220de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 32513 subject IDs for the cohort from data/initial_cohort.csv.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:15:51.331598Z",
     "start_time": "2025-09-05T06:15:47.352716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Tiny BigQuery sanity check (use a public dataset to avoid permissions issues) ---\n",
    "# This verifies your client works without touching restricted datasets like MIMIC-III.\n",
    "try:\n",
    "    test_query = \"\"\"\n",
    "    SELECT year, COUNT(1) AS n\n",
    "    FROM `bigquery-public-data.samples.natality`\n",
    "    WHERE year BETWEEN 2000 AND 2002\n",
    "    GROUP BY year\n",
    "    ORDER BY year\n",
    "    LIMIT 3\n",
    "    \"\"\"\n",
    "    _ = bq_client.query(test_query).to_dataframe()\n",
    "    print(\"BigQuery client sanity check passed on public dataset.\")\n",
    "except Exception as e:\n",
    "    print(\"BigQuery sanity check failed:\", repr(e))\n"
   ],
   "id": "2a2fd415365ea218",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery client sanity check passed on public dataset.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:15:51.691392Z",
     "start_time": "2025-09-05T06:15:51.687927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Helpers for robust BigQuery querying and parameter passing ---\n",
    "from google.api_core import exceptions as gexc\n",
    "\n",
    "def safe_bq_to_df(sql: str, job_config: Optional[bigquery.QueryJobConfig] = None) -> pd.DataFrame:\n",
    "    try:\n",
    "        return bq_client.query(sql, job_config=job_config).to_dataframe()\n",
    "    except (gexc.Forbidden, gexc.NotFound, gexc.BadRequest) as e:\n",
    "        print(f\"Query failed: {type(e).__name__}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {type(e).__name__}: {e}\")\n",
    "        return pd.DataFrame()\n"
   ],
   "id": "6f4f6195a9617a4e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:16:01.207356Z",
     "start_time": "2025-09-05T06:15:51.698654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Admissions: restrict to first hospital admission per subject ---\n",
    "from google.cloud import bigquery as bq\n",
    "\n",
    "def get_first_admissions(subject_ids: List[int]) -> pd.DataFrame:\n",
    "    sql = \"\"\"\n",
    "    SELECT subject_id, hadm_id, admittime, dischtime, deathtime, admission_type,\n",
    "           admission_location, discharge_location, diagnosis, insurance, language,\n",
    "           marital_status, ethnicity\n",
    "    FROM `physionet-data.mimiciii_clinical.admissions`\n",
    "    WHERE subject_id IN UNNEST(@subject_ids)\n",
    "    ORDER BY subject_id, admittime\n",
    "    \"\"\"\n",
    "    cfg = bq.QueryJobConfig(\n",
    "        query_parameters=[bq.ArrayQueryParameter(\"subject_ids\", \"INT64\", subject_ids)]\n",
    "    )\n",
    "    df = safe_bq_to_df(sql, job_config=cfg)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    first = (\n",
    "        df.sort_values([\"subject_id\", \"admittime\"])\\\n",
    "          .groupby(\"subject_id\", as_index=False)\\\n",
    "          .first()\n",
    "    )\n",
    "    return first\n",
    "\n",
    "first_admissions_df = get_first_admissions(subject_ids)\n",
    "if first_admissions_df.empty:\n",
    "    print(\"No admissions returned (check MIMIC access).\")\n",
    "else:\n",
    "    print(f\"First admissions loaded: {len(first_admissions_df)} rows\")\n",
    "\n",
    "# Collect first-admission hadm_ids\n",
    "hadm_ids: List[int] = first_admissions_df.get('hadm_id', pd.Series([], dtype='int')).dropna().astype(int).tolist()\n"
   ],
   "id": "7df7f250a2a4f2ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First admissions loaded: 32513 rows\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:16:10.258672Z",
     "start_time": "2025-09-05T06:16:01.288168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Demographics from patients table ---\n",
    "\n",
    "def get_demographics(subject_ids: List[int]) -> pd.DataFrame:\n",
    "    sql = \"\"\"\n",
    "    SELECT subject_id, gender, dob, dod, expire_flag\n",
    "    FROM `physionet-data.mimiciii_clinical.patients`\n",
    "    WHERE subject_id IN UNNEST(@subject_ids)\n",
    "    \"\"\"\n",
    "    cfg = bq.QueryJobConfig(\n",
    "        query_parameters=[bq.ArrayQueryParameter(\"subject_ids\", \"INT64\", subject_ids)]\n",
    "    )\n",
    "    return safe_bq_to_df(sql, job_config=cfg)\n",
    "\n",
    "demographics_df = get_demographics(subject_ids) if subject_ids else pd.DataFrame()\n",
    "print(f\"Demographics rows: {len(demographics_df)}\")\n"
   ],
   "id": "6cdd098c06bf53cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics rows: 32513\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:16:36.462244Z",
     "start_time": "2025-09-05T06:16:10.323555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Vitals in first 48h of first admission (Chartevents) ---\n",
    "# We filter by common vital labels via d_items to avoid itemid hard-coding across systems.\n",
    "\n",
    "def get_vitals_48h(hadm_ids: List[int]) -> pd.DataFrame:\n",
    "    if not hadm_ids:\n",
    "        return pd.DataFrame()\n",
    "    sql = \"\"\"\n",
    "    WITH first_adm AS (\n",
    "      SELECT hadm_id, admittime\n",
    "      FROM `physionet-data.mimiciii_clinical.admissions`\n",
    "      WHERE hadm_id IN UNNEST(@hadm_ids)\n",
    "    ), vitals AS (\n",
    "      SELECT ce.subject_id, ce.hadm_id, ce.icustay_id, ce.charttime,\n",
    "             di.label AS item_label, ce.valuenum, ce.valueuom\n",
    "      FROM `physionet-data.mimiciii_clinical.chartevents` ce\n",
    "      JOIN first_adm fa USING (hadm_id)\n",
    "      JOIN `physionet-data.mimiciii_clinical.d_items` di ON di.itemid = ce.itemid\n",
    "      WHERE ce.hadm_id IN UNNEST(@hadm_ids)\n",
    "        AND ce.valuenum IS NOT NULL\n",
    "        AND TIMESTAMP_DIFF(ce.charttime, fa.admittime, HOUR) BETWEEN 0 AND 48\n",
    "        AND (\n",
    "          REGEXP_CONTAINS(LOWER(di.label), r\"heart rate|hr\") OR\n",
    "          REGEXP_CONTAINS(LOWER(di.label), r\"respiratory rate|rr\") OR\n",
    "          REGEXP_CONTAINS(LOWER(di.label), r\"temperature\") OR\n",
    "          REGEXP_CONTAINS(LOWER(di.label), r\"(non?invasive )?systolic|sysbp|sbp\") OR\n",
    "          REGEXP_CONTAINS(LOWER(di.label), r\"(non?invasive )?diastolic|diasbp|dbp\") OR\n",
    "          REGEXP_CONTAINS(LOWER(di.label), r\"mean arterial|map\") OR\n",
    "          REGEXP_CONTAINS(LOWER(di.label), r\"spo2|o2 saturation|oxygen saturation\")\n",
    "        )\n",
    "    )\n",
    "    SELECT * FROM vitals\n",
    "    ORDER BY subject_id, charttime\n",
    "    \"\"\"\n",
    "    cfg = bq.QueryJobConfig(\n",
    "        query_parameters=[bq.ArrayQueryParameter(\"hadm_ids\", \"INT64\", hadm_ids)]\n",
    "    )\n",
    "    return safe_bq_to_df(sql, job_config=cfg)\n",
    "\n",
    "vitals_df = get_vitals_48h(hadm_ids)\n",
    "print(f\"Vitals events (<=48h): {len(vitals_df)}\")\n"
   ],
   "id": "2cb0cdaefd1fd860",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vitals events (<=48h): 7919202\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:16:51.743284Z",
     "start_time": "2025-09-05T06:16:36.508137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Labs in first 48h of first admission (Labevents only) ---\n",
    "# Filter to a common set via d_labitems labels\n",
    "\n",
    "def get_labs_48h(hadm_ids: List[int]) -> pd.DataFrame:\n",
    "    if not hadm_ids:\n",
    "        return pd.DataFrame()\n",
    "    sql = \"\"\"\n",
    "    WITH first_adm AS (\n",
    "      SELECT hadm_id, admittime\n",
    "      FROM `physionet-data.mimiciii_clinical.admissions`\n",
    "      WHERE hadm_id IN UNNEST(@hadm_ids)\n",
    "    ), labs AS (\n",
    "      SELECT le.subject_id, le.hadm_id, le.charttime,\n",
    "             dl.label AS item_label, le.valuenum, le.value AS value_text,\n",
    "             le.valueuom, le.flag\n",
    "      FROM `physionet-data.mimiciii_clinical.labevents` le\n",
    "      JOIN first_adm fa USING (hadm_id)\n",
    "      JOIN `physionet-data.mimiciii_clinical.d_labitems` dl ON dl.itemid = le.itemid\n",
    "      WHERE le.hadm_id IN UNNEST(@hadm_ids)\n",
    "        AND le.charttime IS NOT NULL\n",
    "        AND TIMESTAMP_DIFF(le.charttime, fa.admittime, HOUR) BETWEEN 0 AND 48\n",
    "        AND (\n",
    "          REGEXP_CONTAINS(LOWER(dl.label), r\"wbc|white blood\") OR\n",
    "          REGEXP_CONTAINS(LOWER(dl.label), r\"hemoglobin|hgb\") OR\n",
    "          REGEXP_CONTAINS(LOWER(dl.label), r\"hematocrit|hct\") OR\n",
    "          REGEXP_CONTAINS(LOWER(dl.label), r\"platelet\") OR\n",
    "          REGEXP_CONTAINS(LOWER(dl.label), r\"sodium|na\\\\b\") OR\n",
    "          REGEXP_CONTAINS(LOWER(dl.label), r\"potassium|k\\\\b\") OR\n",
    "          REGEXP_CONTAINS(LOWER(dl.label), r\"chloride|cl\\\\b\") OR\n",
    "          REGEXP_CONTAINS(LOWER(dl.label), r\"bicarbonate|hco3\") OR\n",
    "          REGEXP_CONTAINS(LOWER(dl.label), r\"bun|urea\") OR\n",
    "          REGEXP_CONTAINS(LOWER(dl.label), r\"creatinine\") OR\n",
    "          REGEXP_CONTAINS(LOWER(dl.label), r\"glucose\") OR\n",
    "          REGEXP_CONTAINS(LOWER(dl.label), r\"lactate\")\n",
    "        )\n",
    "    )\n",
    "    SELECT * FROM labs\n",
    "    ORDER BY subject_id, charttime\n",
    "    \"\"\"\n",
    "    cfg = bq.QueryJobConfig(\n",
    "        query_parameters=[bq.ArrayQueryParameter(\"hadm_ids\", \"INT64\", hadm_ids)]\n",
    "    )\n",
    "    return safe_bq_to_df(sql, job_config=cfg)\n",
    "\n",
    "labs_df = get_labs_48h(hadm_ids)\n",
    "print(f\"Lab events (<=48h): {len(labs_df)}\")\n"
   ],
   "id": "f287fdc1aad2c695",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab events (<=48h): 1239058\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:04.459077Z",
     "start_time": "2025-09-05T06:16:51.785513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Additional modality #1: Prescriptions in first 48h ---\n",
    "\n",
    "def get_prescriptions_48h(hadm_ids: List[int]) -> pd.DataFrame:\n",
    "    if not hadm_ids:\n",
    "        return pd.DataFrame()\n",
    "    sql = \"\"\"\n",
    "    WITH first_adm AS (\n",
    "      SELECT hadm_id, admittime\n",
    "      FROM `physionet-data.mimiciii_clinical.admissions`\n",
    "      WHERE hadm_id IN UNNEST(@hadm_ids)\n",
    "    )\n",
    "    SELECT pr.subject_id, pr.hadm_id, pr.startdate, pr.enddate,\n",
    "           pr.drug, pr.drug_type, pr.formulary_drug_cd, pr.route\n",
    "    FROM `physionet-data.mimiciii_clinical.prescriptions` pr\n",
    "    JOIN first_adm fa USING (hadm_id)\n",
    "    WHERE pr.hadm_id IN UNNEST(@hadm_ids)\n",
    "      AND pr.startdate IS NOT NULL\n",
    "      AND TIMESTAMP_DIFF(pr.startdate, fa.admittime, HOUR) BETWEEN 0 AND 48\n",
    "    ORDER BY subject_id, startdate\n",
    "    \"\"\"\n",
    "    cfg = bq.QueryJobConfig(\n",
    "        query_parameters=[bq.ArrayQueryParameter(\"hadm_ids\", \"INT64\", hadm_ids)]\n",
    "    )\n",
    "    return safe_bq_to_df(sql, job_config=cfg)\n",
    "\n",
    "prescriptions_df = get_prescriptions_48h(hadm_ids)\n",
    "print(f\"Prescriptions (<=48h): {len(prescriptions_df)}\")\n"
   ],
   "id": "a3955201221690ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prescriptions (<=48h): 619232\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:14.574081Z",
     "start_time": "2025-09-05T06:17:04.499141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Additional modality #2: Procedures in first 48h ---\n",
    "# Use procedureevents_mv which contains timestamped procedures (MV-era). Filter to start within 48h of admittime.\n",
    "\n",
    "def get_procedures_48h(hadm_ids: List[int]) -> pd.DataFrame:\n",
    "    if not hadm_ids:\n",
    "        return pd.DataFrame()\n",
    "    sql = \"\"\"\n",
    "    WITH first_adm AS (\n",
    "      SELECT hadm_id, admittime\n",
    "      FROM `physionet-data.mimiciii_clinical.admissions`\n",
    "      WHERE hadm_id IN UNNEST(@hadm_ids)\n",
    "    )\n",
    "    SELECT pe.subject_id, pe.hadm_id, pe.icustay_id,\n",
    "           pe.starttime, pe.endtime,\n",
    "           pe.itemid, di.label AS item_label,\n",
    "           pe.ordercategoryname, pe.ordercategorydescription, pe.location\n",
    "    FROM `physionet-data.mimiciii_clinical.procedureevents_mv` pe\n",
    "    JOIN first_adm fa USING (hadm_id)\n",
    "    LEFT JOIN `physionet-data.mimiciii_clinical.d_items` di ON di.itemid = pe.itemid\n",
    "    WHERE pe.hadm_id IN UNNEST(@hadm_ids)\n",
    "      AND pe.starttime IS NOT NULL\n",
    "      AND TIMESTAMP_DIFF(pe.starttime, fa.admittime, HOUR) BETWEEN 0 AND 48\n",
    "    ORDER BY subject_id, starttime\n",
    "    \"\"\"\n",
    "    cfg = bq.QueryJobConfig(\n",
    "        query_parameters=[bq.ArrayQueryParameter(\"hadm_ids\", \"INT64\", hadm_ids)]\n",
    "    )\n",
    "    return safe_bq_to_df(sql, job_config=cfg)\n",
    "\n",
    "procedures_df = get_procedures_48h(hadm_ids)\n",
    "print(f\"Procedures (<=48h): {len(procedures_df)}\")\n"
   ],
   "id": "8f07be7cf5c96c71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedures (<=48h): 71675\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:18.228247Z",
     "start_time": "2025-09-05T06:17:14.636446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Optional: Cache extracted raw tables to disk for reproducibility ---\n",
    "cache_dir = os.path.join(os.path.dirname(cohort_path), \"extracted_cache\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "if not first_admissions_df.empty:\n",
    "    first_admissions_df.to_parquet(os.path.join(cache_dir, \"first_admissions.parquet\"), index=False)\n",
    "if not demographics_df.empty:\n",
    "    demographics_df.to_parquet(os.path.join(cache_dir, \"demographics.parquet\"), index=False)\n",
    "if not vitals_df.empty:\n",
    "    vitals_df.to_parquet(os.path.join(cache_dir, \"vitals_48h.parquet\"), index=False)\n",
    "if not labs_df.empty:\n",
    "    labs_df.to_parquet(os.path.join(cache_dir, \"labs_48h.parquet\"), index=False)\n",
    "if not prescriptions_df.empty:\n",
    "    prescriptions_df.to_parquet(os.path.join(cache_dir, \"prescriptions_48h.parquet\"), index=False)\n",
    "if not procedures_df.empty:\n",
    "    procedures_df.to_parquet(os.path.join(cache_dir, \"procedures_48h.parquet\"), index=False)\n",
    "\n",
    "print(f\"Cached available extracts under: {cache_dir}\")\n"
   ],
   "id": "a1d3e29fb39e9f80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached available extracts under: data\\extracted_cache\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:18.308292Z",
     "start_time": "2025-09-05T06:17:18.277722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Quick previews ---\n",
    "for name, df in {\n",
    "    'first_admissions': first_admissions_df,\n",
    "    'demographics': demographics_df,\n",
    "    'vitals_48h': vitals_df,\n",
    "    'labs_48h': labs_df,\n",
    "    'prescriptions_48h': prescriptions_df,\n",
    "    'procedures_48h': procedures_df,\n",
    "}.items():\n",
    "    print(f\"\\n{name}: {len(df)} rows\")\n",
    "    if not df.empty:\n",
    "        display(df.head(3))\n"
   ],
   "id": "c3f04d8ba36f263",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "first_admissions: 32513 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   subject_id  hadm_id           admittime           dischtime deathtime  \\\n",
       "0           2   163353 2138-07-17 19:04:00 2138-07-21 15:48:00       NaT   \n",
       "1           3   145834 2101-10-20 19:08:00 2101-10-31 13:58:00       NaT   \n",
       "2           4   185777 2191-03-16 00:28:00 2191-03-23 18:41:00       NaT   \n",
       "\n",
       "  admission_type         admission_location         discharge_location  \\\n",
       "0        NEWBORN  PHYS REFERRAL/NORMAL DELI                       HOME   \n",
       "1      EMERGENCY       EMERGENCY ROOM ADMIT                        SNF   \n",
       "2      EMERGENCY       EMERGENCY ROOM ADMIT  HOME WITH HOME IV PROVIDR   \n",
       "\n",
       "                             diagnosis insurance language marital_status  \\\n",
       "0                              NEWBORN   Private     None           None   \n",
       "1                          HYPOTENSION  Medicare     None        MARRIED   \n",
       "2  FEVER,DEHYDRATION,FAILURE TO THRIVE   Private     None         SINGLE   \n",
       "\n",
       "  ethnicity  \n",
       "0     ASIAN  \n",
       "1     WHITE  \n",
       "2     WHITE  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>admittime</th>\n",
       "      <th>dischtime</th>\n",
       "      <th>deathtime</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>admission_location</th>\n",
       "      <th>discharge_location</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>insurance</th>\n",
       "      <th>language</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>2138-07-17 19:04:00</td>\n",
       "      <td>2138-07-21 15:48:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>Private</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ASIAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>2101-10-20 19:08:00</td>\n",
       "      <td>2101-10-31 13:58:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>SNF</td>\n",
       "      <td>HYPOTENSION</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>None</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>185777</td>\n",
       "      <td>2191-03-16 00:28:00</td>\n",
       "      <td>2191-03-23 18:41:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>HOME WITH HOME IV PROVIDR</td>\n",
       "      <td>FEVER,DEHYDRATION,FAILURE TO THRIVE</td>\n",
       "      <td>Private</td>\n",
       "      <td>None</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>WHITE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "demographics: 32513 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   subject_id gender        dob        dod  expire_flag\n",
       "0       18848      F 2042-08-21 2128-01-08            1\n",
       "1       61056      F 2067-04-11 2152-01-08            1\n",
       "2       26889      F 2115-11-04 2164-01-08            1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>dob</th>\n",
       "      <th>dod</th>\n",
       "      <th>expire_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18848</td>\n",
       "      <td>F</td>\n",
       "      <td>2042-08-21</td>\n",
       "      <td>2128-01-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61056</td>\n",
       "      <td>F</td>\n",
       "      <td>2067-04-11</td>\n",
       "      <td>2152-01-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26889</td>\n",
       "      <td>F</td>\n",
       "      <td>2115-11-04</td>\n",
       "      <td>2164-01-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vitals_48h: 7919202 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   subject_id  hadm_id  icustay_id           charttime           item_label  \\\n",
       "0           2   163353      243653 2138-07-17 20:20:00           Heart Rate   \n",
       "1           2   163353      243653 2138-07-17 20:30:00  BP Cuff [Diastolic]   \n",
       "2           2   163353      243653 2138-07-17 20:30:00   BP Cuff [Systolic]   \n",
       "\n",
       "   valuenum valueuom  \n",
       "0     148.0      bpm  \n",
       "1      35.0   cc/min  \n",
       "2      72.0   cc/min  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>item_label</th>\n",
       "      <th>valuenum</th>\n",
       "      <th>valueuom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>243653</td>\n",
       "      <td>2138-07-17 20:20:00</td>\n",
       "      <td>Heart Rate</td>\n",
       "      <td>148.0</td>\n",
       "      <td>bpm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>243653</td>\n",
       "      <td>2138-07-17 20:30:00</td>\n",
       "      <td>BP Cuff [Diastolic]</td>\n",
       "      <td>35.0</td>\n",
       "      <td>cc/min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>243653</td>\n",
       "      <td>2138-07-17 20:30:00</td>\n",
       "      <td>BP Cuff [Systolic]</td>\n",
       "      <td>72.0</td>\n",
       "      <td>cc/min</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "labs_48h: 1239058 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   subject_id  hadm_id           charttime      item_label  valuenum  \\\n",
       "0           2   163353 2138-07-17 20:48:00      Hemoglobin       0.0   \n",
       "1           2   163353 2138-07-17 20:48:00      Hematocrit       0.0   \n",
       "2           2   163353 2138-07-17 20:48:00  Platelet Count       5.0   \n",
       "\n",
       "  value_text valueuom      flag  \n",
       "0          0     g/dL  abnormal  \n",
       "1          0        %  abnormal  \n",
       "2          5     K/uL  abnormal  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>item_label</th>\n",
       "      <th>valuenum</th>\n",
       "      <th>value_text</th>\n",
       "      <th>valueuom</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>2138-07-17 20:48:00</td>\n",
       "      <td>Hemoglobin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>g/dL</td>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>2138-07-17 20:48:00</td>\n",
       "      <td>Hematocrit</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>%</td>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>2138-07-17 20:48:00</td>\n",
       "      <td>Platelet Count</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>K/uL</td>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prescriptions_48h: 619232 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   subject_id  hadm_id  startdate    enddate                      drug  \\\n",
       "0           2   163353 2138-07-18 2138-07-20  Syringe (Neonatal) *D5W*   \n",
       "1           2   163353 2138-07-18 2138-07-21           Send 500mg Vial   \n",
       "2           2   163353 2138-07-18 2138-07-21         Ampicillin Sodium   \n",
       "\n",
       "  drug_type formulary_drug_cd route  \n",
       "0      BASE         NEOSYRD5W    IV  \n",
       "1      BASE             AMPVL    IV  \n",
       "2      MAIN           AMP500I    IV  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>startdate</th>\n",
       "      <th>enddate</th>\n",
       "      <th>drug</th>\n",
       "      <th>drug_type</th>\n",
       "      <th>formulary_drug_cd</th>\n",
       "      <th>route</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>2138-07-18</td>\n",
       "      <td>2138-07-20</td>\n",
       "      <td>Syringe (Neonatal) *D5W*</td>\n",
       "      <td>BASE</td>\n",
       "      <td>NEOSYRD5W</td>\n",
       "      <td>IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>2138-07-18</td>\n",
       "      <td>2138-07-21</td>\n",
       "      <td>Send 500mg Vial</td>\n",
       "      <td>BASE</td>\n",
       "      <td>AMPVL</td>\n",
       "      <td>IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>2138-07-18</td>\n",
       "      <td>2138-07-21</td>\n",
       "      <td>Ampicillin Sodium</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>AMP500I</td>\n",
       "      <td>IV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "procedures_48h: 71675 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   subject_id  hadm_id  icustay_id           starttime             endtime  \\\n",
       "0         266   186251      293876 2168-07-10 08:30:00 2168-07-10 18:41:00   \n",
       "1         266   186251      293876 2168-07-10 08:30:00 2168-07-10 08:31:00   \n",
       "2         266   186251      293876 2168-07-10 09:27:00 2168-07-11 17:40:00   \n",
       "\n",
       "   itemid            item_label      ordercategoryname  \\\n",
       "0  225792  Invasive Ventilation            Ventilation   \n",
       "1  224385            Intubation  Intubation/Extubation   \n",
       "2  224275              20 Gauge       Peripheral Lines   \n",
       "\n",
       "  ordercategorydescription location  \n",
       "0                     Task     None  \n",
       "1             Electrolytes     None  \n",
       "2                     Task     None  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>starttime</th>\n",
       "      <th>endtime</th>\n",
       "      <th>itemid</th>\n",
       "      <th>item_label</th>\n",
       "      <th>ordercategoryname</th>\n",
       "      <th>ordercategorydescription</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>266</td>\n",
       "      <td>186251</td>\n",
       "      <td>293876</td>\n",
       "      <td>2168-07-10 08:30:00</td>\n",
       "      <td>2168-07-10 18:41:00</td>\n",
       "      <td>225792</td>\n",
       "      <td>Invasive Ventilation</td>\n",
       "      <td>Ventilation</td>\n",
       "      <td>Task</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266</td>\n",
       "      <td>186251</td>\n",
       "      <td>293876</td>\n",
       "      <td>2168-07-10 08:30:00</td>\n",
       "      <td>2168-07-10 08:31:00</td>\n",
       "      <td>224385</td>\n",
       "      <td>Intubation</td>\n",
       "      <td>Intubation/Extubation</td>\n",
       "      <td>Electrolytes</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>266</td>\n",
       "      <td>186251</td>\n",
       "      <td>293876</td>\n",
       "      <td>2168-07-10 09:27:00</td>\n",
       "      <td>2168-07-11 17:40:00</td>\n",
       "      <td>224275</td>\n",
       "      <td>20 Gauge</td>\n",
       "      <td>Peripheral Lines</td>\n",
       "      <td>Task</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:20.050214Z",
     "start_time": "2025-09-05T06:17:18.642952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- EDA: Load cached extracts (for reproducibility, independent of BigQuery access) ---\n",
    "import numpy as np\n",
    "\n",
    "cache_dir = os.path.join(os.path.dirname(cohort_path), \"extracted_cache\")\n",
    "\n",
    "def load_cached(name: str) -> pd.DataFrame:\n",
    "    path = os.path.join(cache_dir, f\"{name}.parquet\")\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            return pd.read_parquet(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "eda_first_adm = load_cached(\"first_admissions\")\n",
    "eda_demo = load_cached(\"demographics\")\n",
    "eda_vitals = load_cached(\"vitals_48h\")\n",
    "eda_labs = load_cached(\"labs_48h\")\n",
    "eda_rx = load_cached(\"prescriptions_48h\")\n",
    "eda_proc = load_cached(\"procedures_48h\")\n",
    "# Backward-compat: if procedures not present but microbiology exists, load it for EDA\n",
    "eda_micro = pd.DataFrame()\n",
    "if eda_proc.empty:\n",
    "    eda_micro = load_cached(\"microbiology_48h\")\n",
    "\n",
    "print(\"Cached tables loaded for EDA:\")\n",
    "for n, df in {\n",
    "    'first_admissions': eda_first_adm,\n",
    "    'demographics': eda_demo,\n",
    "    'vitals_48h': eda_vitals,\n",
    "    'labs_48h': eda_labs,\n",
    "    'prescriptions_48h': eda_rx,\n",
    "    'procedures_48h': eda_proc,\n",
    "    'microbiology_48h (fallback)': eda_micro,\n",
    "}.items():\n",
    "    print(f\"- {n}: {len(df)} rows\")\n"
   ],
   "id": "cdc77e2f4362128a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached tables loaded for EDA:\n",
      "- first_admissions: 32513 rows\n",
      "- demographics: 32513 rows\n",
      "- vitals_48h: 7919202 rows\n",
      "- labs_48h: 1239058 rows\n",
      "- prescriptions_48h: 619232 rows\n",
      "- procedures_48h: 71675 rows\n",
      "- microbiology_48h (fallback): 0 rows\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:20.151542Z",
     "start_time": "2025-09-05T06:17:20.096634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- EDA: Cohort overview, age, gender, admission stats ---\n",
    "from datetime import datetime\n",
    "\n",
    "def hours_between(a, b) -> float:\n",
    "    try:\n",
    "        return (pd.to_datetime(a) - pd.to_datetime(b)).total_seconds() / 3600.0\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "insights = []\n",
    "\n",
    "n_subjects = len(set(eda_first_adm['subject_id'])) if not eda_first_adm.empty else len(set(cohort_df['subject_id']))\n",
    "insights.append(f\"Cohort subjects (initial): {n_subjects}\")\n",
    "\n",
    "# Age at first admission\n",
    "age_df = pd.DataFrame()\n",
    "if not eda_first_adm.empty and not eda_demo.empty:\n",
    "    tmp = eda_first_adm[['subject_id', 'admittime']].merge(\n",
    "        eda_demo[['subject_id', 'dob', 'gender']], on='subject_id', how='left'\n",
    "    )\n",
    "    tmp['age_years'] = (pd.to_datetime(tmp['admittime']) - pd.to_datetime(tmp['dob'])).dt.days / 365.25\n",
    "    # Cap ages >= 89 to 90 per common MIMIC-III practice\n",
    "    tmp['age_years'] = tmp['age_years'].clip(lower=0)\n",
    "    tmp.loc[tmp['age_years'] >= 89, 'age_years'] = 90\n",
    "    age_df = tmp\n",
    "    if not tmp['age_years'].dropna().empty:\n",
    "        insights.append(\n",
    "            \"Age at admission (years): median {:.1f} [IQR {:.1f}-{:.1f}], >=65: {:.1f}%\".format(\n",
    "                tmp['age_years'].median(),\n",
    "                tmp['age_years'].quantile(0.25),\n",
    "                tmp['age_years'].quantile(0.75),\n",
    "                100.0 * (tmp['age_years'] >= 65).mean(),\n",
    "            )\n",
    "        )\n",
    "    if 'gender' in tmp and not tmp['gender'].dropna().empty:\n",
    "        g = tmp['gender'].value_counts(normalize=True).mul(100).round(1)\n",
    "        insights.append(\"Gender distribution (%): \" + \", \".join(f\"{k}: {v}%\" for k, v in g.to_dict().items()))\n",
    "\n",
    "# Admission types and LOS\n",
    "if not eda_first_adm.empty:\n",
    "    if 'admission_type' in eda_first_adm:\n",
    "        at = eda_first_adm['admission_type'].value_counts(normalize=True).mul(100).round(1).to_dict()\n",
    "        insights.append(\"Admission types (%): \" + \", \".join(f\"{k}: {v}%\" for k, v in at.items()))\n",
    "    los_hours = (pd.to_datetime(eda_first_adm['dischtime']) - pd.to_datetime(eda_first_adm['admittime'])).dt.total_seconds() / 3600.0\n",
    "    los_hours = los_hours.clip(lower=0)\n",
    "    if not los_hours.dropna().empty:\n",
    "        insights.append(\n",
    "            \"Hospital LOS (hours): median {:.1f} [IQR {:.1f}-{:.1f}], >=54h coverage: {:.1f}%\".format(\n",
    "                los_hours.median(),\n",
    "                los_hours.quantile(0.25),\n",
    "                los_hours.quantile(0.75),\n",
    "                100.0 * (los_hours >= 54).mean(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(\"\\nKey cohort insights:\")\n",
    "for s in insights:\n",
    "    print(\"-\", s)\n"
   ],
   "id": "8c0092316fb984b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key cohort insights:\n",
      "- Cohort subjects (initial): 32513\n",
      "- Age at admission (years): median 60.5 [IQR 38.2-75.5], >=65: 42.5%\n",
      "- Gender distribution (%): M: 56.4%, F: 43.6%\n",
      "- Admission types (%): EMERGENCY: 67.2%, NEWBORN: 16.9%, ELECTIVE: 13.5%, URGENT: 2.5%\n",
      "- Hospital LOS (hours): median 152.9 [IQR 89.0-280.4], >=54h coverage: 87.6%\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:20.989917Z",
     "start_time": "2025-09-05T06:17:20.173371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- EDA: Coverage and density per modality in first 48h ---\n",
    "from collections import defaultdict\n",
    "\n",
    "coverage = []\n",
    "if not eda_first_adm.empty:\n",
    "    subj_set = set(eda_first_adm['subject_id'])\n",
    "    hadm_set = set(eda_first_adm['hadm_id'])\n",
    "else:\n",
    "    subj_set = set(cohort_df['subject_id'])\n",
    "    hadm_set = set()\n",
    "\n",
    "def pct(x):\n",
    "    return round(100.0 * x, 1)\n",
    "\n",
    "# Helper to compute per-subject coverage\n",
    "def cov_by_subject(name: str, df: pd.DataFrame, id_col: str = 'subject_id'):\n",
    "    if df.empty:\n",
    "        print(f\"- {name}: 0.0% subjects with records\")\n",
    "        return\n",
    "    with_rec = len(set(df[id_col]) & subj_set) if subj_set else df[id_col].nunique()\n",
    "    base = len(subj_set) if subj_set else n_subjects\n",
    "    print(f\"- {name}: {pct(with_rec / base if base else 0)}% subjects with records, total rows={len(df):,}\")\n",
    "\n",
    "print(\"\\nCoverage within first 48h:\")\n",
    "cov_by_subject(\"Vitals\", eda_vitals)\n",
    "cov_by_subject(\"Labs\", eda_labs)\n",
    "cov_by_subject(\"Prescriptions\", eda_rx)\n",
    "if not eda_proc.empty:\n",
    "    cov_by_subject(\"Procedures\", eda_proc)\n",
    "elif not eda_micro.empty:\n",
    "    cov_by_subject(\"Microbiology\", eda_micro)\n",
    "\n",
    "# Event density per subject (median events/subject)\n",
    "def density(name: str, df: pd.DataFrame):\n",
    "    if df.empty:\n",
    "        return\n",
    "    cnt = df.groupby('subject_id').size()\n",
    "    print(f\"  {name} events per subject: median {cnt.median():.0f} [IQR {cnt.quantile(0.25):.0f}-{cnt.quantile(0.75):.0f}]\")\n",
    "\n",
    "density(\"Vitals\", eda_vitals)\n",
    "density(\"Labs\", eda_labs)\n",
    "density(\"Prescriptions\", eda_rx)\n",
    "if not eda_proc.empty:\n",
    "    density(\"Procedures\", eda_proc)\n",
    "elif not eda_micro.empty:\n",
    "    density(\"Microbiology\", eda_micro)\n"
   ],
   "id": "b67191c3588c29c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coverage within first 48h:\n",
      "- Vitals: 85.2% subjects with records, total rows=7,919,202\n",
      "- Labs: 95.4% subjects with records, total rows=1,239,058\n",
      "- Prescriptions: 79.2% subjects with records, total rows=619,232\n",
      "- Procedures: 29.4% subjects with records, total rows=71,675\n",
      "  Vitals events per subject: median 257 [IQR 154-375]\n",
      "  Labs events per subject: median 36 [IQR 23-53]\n",
      "  Prescriptions events per subject: median 20 [IQR 10-33]\n",
      "  Procedures events per subject: median 6 [IQR 4-11]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:21.955593Z",
     "start_time": "2025-09-05T06:17:21.029502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- EDA: Vitals and Labs distributions (first 48h) ---\n",
    "# Summarize by item label\n",
    "if not eda_vitals.empty and 'item_label' in eda_vitals:\n",
    "    vstats = (\n",
    "        eda_vitals.dropna(subset=['valuenum'])\n",
    "        .groupby('item_label')['valuenum']\n",
    "        .agg(['count', 'median', 'mean', 'std'])\n",
    "        .sort_values('count', ascending=False)\n",
    "        .head(15)\n",
    "        .round(2)\n",
    "    )\n",
    "    print(\"\\nTop 15 vitals by count with summary stats:\")\n",
    "    display(vstats)\n",
    "else:\n",
    "    print(\"No vitals available for distribution summary.\")\n",
    "\n",
    "if not eda_labs.empty and 'item_label' in eda_labs:\n",
    "    lstats = (\n",
    "        eda_labs.dropna(subset=['valuenum'])\n",
    "        .groupby('item_label')['valuenum']\n",
    "        .agg(['count', 'median', 'mean', 'std'])\n",
    "        .sort_values('count', ascending=False)\n",
    "        .head(15)\n",
    "        .round(2)\n",
    "    )\n",
    "    print(\"\\nTop 15 labs by count with summary stats:\")\n",
    "    display(lstats)\n",
    "else:\n",
    "    print(\"No labs available for distribution summary.\")\n"
   ],
   "id": "5d1ba265918f685f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 vitals by count with summary stats:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                         count  median    mean      std\n",
       "item_label                                                             \n",
       "Heart Rate                             1120336    87.0   91.60    25.00\n",
       "Respiratory Rate                        974145    18.0   18.80     5.90\n",
       "SpO2                                    531751    98.0   97.29     3.91\n",
       "O2 saturation pulseoxymetry             428941    97.0   99.27  1498.03\n",
       "HR Alarm [High]                         403301   120.0  143.55    37.57\n",
       "HR Alarm [Low]                          403227    60.0   65.23    18.33\n",
       "Arterial BP [Systolic]                  298445   117.0  118.86    25.77\n",
       "Arterial BP [Diastolic]                 298317    59.0   60.07    13.69\n",
       "SpO2 Alarm [Low]                        283158    90.0   89.99     8.62\n",
       "SpO2 Alarm [High]                       281712   100.0   99.15     7.74\n",
       "NBP [Systolic]                          280995   118.0  119.99    23.20\n",
       "NBP [Diastolic]                         280688    58.0   59.12    15.36\n",
       "Non Invasive Blood Pressure systolic    242139   118.0  120.54   245.89\n",
       "Non Invasive Blood Pressure diastolic   242040    63.0   64.92   214.63\n",
       "Arterial Blood Pressure systolic        156046   116.0  118.58    22.48"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Heart Rate</th>\n",
       "      <td>1120336</td>\n",
       "      <td>87.0</td>\n",
       "      <td>91.60</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Respiratory Rate</th>\n",
       "      <td>974145</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.80</td>\n",
       "      <td>5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpO2</th>\n",
       "      <td>531751</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.29</td>\n",
       "      <td>3.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O2 saturation pulseoxymetry</th>\n",
       "      <td>428941</td>\n",
       "      <td>97.0</td>\n",
       "      <td>99.27</td>\n",
       "      <td>1498.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HR Alarm [High]</th>\n",
       "      <td>403301</td>\n",
       "      <td>120.0</td>\n",
       "      <td>143.55</td>\n",
       "      <td>37.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HR Alarm [Low]</th>\n",
       "      <td>403227</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.23</td>\n",
       "      <td>18.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arterial BP [Systolic]</th>\n",
       "      <td>298445</td>\n",
       "      <td>117.0</td>\n",
       "      <td>118.86</td>\n",
       "      <td>25.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arterial BP [Diastolic]</th>\n",
       "      <td>298317</td>\n",
       "      <td>59.0</td>\n",
       "      <td>60.07</td>\n",
       "      <td>13.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpO2 Alarm [Low]</th>\n",
       "      <td>283158</td>\n",
       "      <td>90.0</td>\n",
       "      <td>89.99</td>\n",
       "      <td>8.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpO2 Alarm [High]</th>\n",
       "      <td>281712</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.15</td>\n",
       "      <td>7.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NBP [Systolic]</th>\n",
       "      <td>280995</td>\n",
       "      <td>118.0</td>\n",
       "      <td>119.99</td>\n",
       "      <td>23.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NBP [Diastolic]</th>\n",
       "      <td>280688</td>\n",
       "      <td>58.0</td>\n",
       "      <td>59.12</td>\n",
       "      <td>15.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non Invasive Blood Pressure systolic</th>\n",
       "      <td>242139</td>\n",
       "      <td>118.0</td>\n",
       "      <td>120.54</td>\n",
       "      <td>245.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non Invasive Blood Pressure diastolic</th>\n",
       "      <td>242040</td>\n",
       "      <td>63.0</td>\n",
       "      <td>64.92</td>\n",
       "      <td>214.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arterial Blood Pressure systolic</th>\n",
       "      <td>156046</td>\n",
       "      <td>116.0</td>\n",
       "      <td>118.58</td>\n",
       "      <td>22.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 labs by count with summary stats:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                         count  median     mean      std\n",
       "item_label                                              \n",
       "Glucose                 125003   127.0   142.56    76.66\n",
       "Hemoglobin              107141    10.8    11.06     2.41\n",
       "Hematocrit              104090    31.5    32.51     6.77\n",
       "Potassium                87845     4.1     4.16     0.76\n",
       "Platelet Count           84996   193.0   206.60   106.37\n",
       "Sodium                   83555   139.0   138.97     5.16\n",
       "Chloride                 83525   106.0   105.54     6.25\n",
       "Bicarbonate              81618    24.0    23.77     4.61\n",
       "Creatinine               80634     0.9     1.38     1.47\n",
       "Urea Nitrogen            80238    18.0    24.93    20.75\n",
       "White Blood Cells        80137    10.5    11.98    10.64\n",
       "Potassium, Whole Blood   44670     4.2     4.29     2.65\n",
       "Lactate                  39384     2.0     2.83     2.48\n",
       "Hematocrit, Calculated   26853    31.0    31.23     6.95\n",
       "Creatine Kinase (CK)     26824   197.0  1530.62  8540.82"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>125003</td>\n",
       "      <td>127.0</td>\n",
       "      <td>142.56</td>\n",
       "      <td>76.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hemoglobin</th>\n",
       "      <td>107141</td>\n",
       "      <td>10.8</td>\n",
       "      <td>11.06</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hematocrit</th>\n",
       "      <td>104090</td>\n",
       "      <td>31.5</td>\n",
       "      <td>32.51</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Potassium</th>\n",
       "      <td>87845</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.16</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Platelet Count</th>\n",
       "      <td>84996</td>\n",
       "      <td>193.0</td>\n",
       "      <td>206.60</td>\n",
       "      <td>106.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sodium</th>\n",
       "      <td>83555</td>\n",
       "      <td>139.0</td>\n",
       "      <td>138.97</td>\n",
       "      <td>5.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chloride</th>\n",
       "      <td>83525</td>\n",
       "      <td>106.0</td>\n",
       "      <td>105.54</td>\n",
       "      <td>6.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bicarbonate</th>\n",
       "      <td>81618</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.77</td>\n",
       "      <td>4.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Creatinine</th>\n",
       "      <td>80634</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urea Nitrogen</th>\n",
       "      <td>80238</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.93</td>\n",
       "      <td>20.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White Blood Cells</th>\n",
       "      <td>80137</td>\n",
       "      <td>10.5</td>\n",
       "      <td>11.98</td>\n",
       "      <td>10.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Potassium, Whole Blood</th>\n",
       "      <td>44670</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.29</td>\n",
       "      <td>2.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lactate</th>\n",
       "      <td>39384</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hematocrit, Calculated</th>\n",
       "      <td>26853</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.23</td>\n",
       "      <td>6.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Creatine Kinase (CK)</th>\n",
       "      <td>26824</td>\n",
       "      <td>197.0</td>\n",
       "      <td>1530.62</td>\n",
       "      <td>8540.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:22.655677Z",
     "start_time": "2025-09-05T06:17:22.043368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- EDA: Prescriptions (first 48h) ---\n",
    "if not eda_rx.empty:\n",
    "    top_drugs = (\n",
    "        eda_rx.assign(drug_low=eda_rx['drug'].astype(str).str.lower())\n",
    "        .groupby('drug_low')\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(20)\n",
    "    )\n",
    "    print(\"\\nTop 20 prescribed drugs (by mentions) in first 48h:\")\n",
    "    display(top_drugs)\n",
    "\n",
    "    # Simple flags for antibiotics and vasopressors in prescriptions\n",
    "    rx_lower = eda_rx['drug'].astype(str).str.lower()\n",
    "    abx_patterns = ['cillin', 'cef', 'ceph', 'penem', 'floxacin', 'vancomycin', 'metronidazole', 'piperacillin', 'tazobactam']\n",
    "    vaso_patterns = ['norepinephrine', 'levophed', 'epinephrine', 'dopamine', 'dobutamine', 'phenylephrine', 'vasopressin']\n",
    "    has_abx = rx_lower.str.contains('|'.join(abx_patterns), na=False)\n",
    "    has_vaso = rx_lower.str.contains('|'.join(vaso_patterns), na=False)\n",
    "    abx_subj = eda_rx.loc[has_abx, 'subject_id'].nunique()\n",
    "    vaso_subj = eda_rx.loc[has_vaso, 'subject_id'].nunique()\n",
    "    base = len(subj_set) if subj_set else n_subjects\n",
    "    print(f\"Antibiotics exposure (any mention): {pct(abx_subj / base if base else 0)}% of subjects\")\n",
    "    print(f\"Vasopressor exposure (any mention): {pct(vaso_subj / base if base else 0)}% of subjects\")\n",
    "else:\n",
    "    print(\"No prescriptions available for summary.\")\n"
   ],
   "id": "9ed3b410f6d03c82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 prescribed drugs (by mentions) in first 48h:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "drug_low\n",
       "potassium chloride             28964\n",
       "0.9% sodium chloride           23917\n",
       "ns                             21936\n",
       "d5w                            20332\n",
       "insulin                        17355\n",
       "magnesium sulfate              15592\n",
       "furosemide                     14961\n",
       "sodium chloride 0.9%  flush    14810\n",
       "acetaminophen                  13396\n",
       "sw                             12009\n",
       "iso-osmotic dextrose           11860\n",
       "metoprolol                     11173\n",
       "calcium gluconate              10563\n",
       "morphine sulfate               10220\n",
       "5% dextrose                     9868\n",
       "metoprolol tartrate             9383\n",
       "lorazepam                       8826\n",
       "heparin                         8586\n",
       "lr                              7763\n",
       "docusate sodium                 7684\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antibiotics exposure (any mention): 36.3% of subjects\n",
      "Vasopressor exposure (any mention): 12.3% of subjects\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:22.770356Z",
     "start_time": "2025-09-05T06:17:22.676654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- EDA: Procedures or Microbiology (first 48h) ---\n",
    "if not eda_proc.empty:\n",
    "    top_proc_cat = eda_proc['ordercategoryname'].value_counts().head(15)\n",
    "    print(\"\\nTop 15 procedure categories:\")\n",
    "    display(top_proc_cat)\n",
    "    # Heuristic ventilatory support flag\n",
    "    label_col = 'item_label' if 'item_label' in eda_proc.columns else 'ordercategorydescription'\n",
    "    lbl = eda_proc[label_col].astype(str).str.lower()\n",
    "    vent_patterns = ['vent', 'intubat', 'endotracheal', 'peep', 'tidal volume']\n",
    "    renal_patterns = ['dialysis', 'crrt', 'hemodialysis']\n",
    "    central_line_patterns = ['central line', 'cvc', 'subclavian', 'internal jugular', 'femoral line']\n",
    "    base = len(subj_set) if subj_set else n_subjects\n",
    "    def frac(patterns):\n",
    "        m = lbl.str.contains('|'.join(patterns), na=False)\n",
    "        return pct(eda_proc.loc[m, 'subject_id'].nunique() / base) if base else 0.0\n",
    "    print(f\"Ventilation-related procedure (heuristic): {frac(vent_patterns)}% of subjects\")\n",
    "    print(f\"Renal replacement (heuristic): {frac(renal_patterns)}% of subjects\")\n",
    "    print(f\"Central line (heuristic): {frac(central_line_patterns)}% of subjects\")\n",
    "elif not eda_micro.empty:\n",
    "    print(\"\\nMicrobiology summary (fallback):\")\n",
    "    top_spec = eda_micro['spec_type_desc'].value_counts().head(10)\n",
    "    display(top_spec)\n",
    "    pos = eda_micro['interpretation'].astype(str).str.upper().str.contains('S|R|I')\n",
    "    base = len(subj_set) if subj_set else n_subjects\n",
    "    any_culture = eda_micro['subject_id'].nunique()\n",
    "    print(f\"Any culture taken: {pct(any_culture / base if base else 0)}% of subjects\")\n",
    "else:\n",
    "    print(\"No procedures or microbiology available for summary.\")\n"
   ],
   "id": "50204eb72fed0b61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 procedure categories:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ordercategoryname\n",
       "Peripheral Lines         22732\n",
       "Procedures               15702\n",
       "Invasive Lines           10780\n",
       "Imaging                   9601\n",
       "Ventilation               4124\n",
       "Intubation/Extubation     3520\n",
       "Significant Events        2942\n",
       "Communication             1970\n",
       "Continuous Procedures      175\n",
       "Dialysis                   112\n",
       "Peritoneal Dialysis         15\n",
       "CRRT Filter Change           2\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventilation-related procedure (heuristic): 12.2% of subjects\n",
      "Renal replacement (heuristic): 0.8% of subjects\n",
      "Central line (heuristic): 0.0% of subjects\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:22.839993Z",
     "start_time": "2025-09-05T06:17:22.781515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- EDA: Consolidated key insights summary ---\n",
    "summary_lines = []\n",
    "summary_lines.extend(insights)\n",
    "\n",
    "# Add coverage headlines\n",
    "def cov_headline(name, df):\n",
    "    if df.empty:\n",
    "        return f\"{name}: 0% coverage\"\n",
    "    base = len(subj_set) if subj_set else n_subjects\n",
    "    frac = df['subject_id'].nunique() / base if base else 0\n",
    "    return f\"{name}: {pct(frac)}% subjects with data\"\n",
    "\n",
    "summary_lines.append(cov_headline(\"Vitals (<=48h)\", eda_vitals))\n",
    "summary_lines.append(cov_headline(\"Labs (<=48h)\", eda_labs))\n",
    "summary_lines.append(cov_headline(\"Prescriptions (<=48h)\", eda_rx))\n",
    "if not eda_proc.empty:\n",
    "    summary_lines.append(cov_headline(\"Procedures (<=48h)\", eda_proc))\n",
    "elif not eda_micro.empty:\n",
    "    summary_lines.append(cov_headline(\"Microbiology (<=48h)\", eda_micro))\n",
    "\n",
    "# Save a machine-readable report\n",
    "report_path = os.path.join(cache_dir, \"eda_summary.json\")\n",
    "try:\n",
    "    import json\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\"insights\": summary_lines}, f, indent=2)\n",
    "    print(f\"\\nEDA summary saved to {report_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save EDA summary: {e}\")\n",
    "\n",
    "print(\"\\nKey EDA insights:\")\n",
    "for line in summary_lines:\n",
    "    print(\"-\", line)\n"
   ],
   "id": "58ad3204524fb613",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EDA summary saved to data\\extracted_cache\\eda_summary.json\n",
      "\n",
      "Key EDA insights:\n",
      "- Cohort subjects (initial): 32513\n",
      "- Age at admission (years): median 60.5 [IQR 38.2-75.5], >=65: 42.5%\n",
      "- Gender distribution (%): M: 56.4%, F: 43.6%\n",
      "- Admission types (%): EMERGENCY: 67.2%, NEWBORN: 16.9%, ELECTIVE: 13.5%, URGENT: 2.5%\n",
      "- Hospital LOS (hours): median 152.9 [IQR 89.0-280.4], >=54h coverage: 87.6%\n",
      "- Vitals (<=48h): 85.2% subjects with data\n",
      "- Labs (<=48h): 95.4% subjects with data\n",
      "- Prescriptions (<=48h): 79.2% subjects with data\n",
      "- Procedures (<=48h): 29.4% subjects with data\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:32.483858Z",
     "start_time": "2025-09-05T06:17:22.851884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Labels: Mortality, Prolonged LOS (>7d), 30-day Readmission ---\n",
    "# Build labels for the first admission per subject; filter to LOS >= 54h (timeline requirement).\n",
    "from google.cloud import bigquery as bq\n",
    "import numpy as np\n",
    "\n",
    "# Helper: fetch all admissions for cohort to compute readmission\n",
    "def get_all_admissions(subject_ids: List[int]) -> pd.DataFrame:\n",
    "    sql = \"\"\"\n",
    "    SELECT subject_id, hadm_id, admittime, dischtime, deathtime\n",
    "    FROM `physionet-data.mimiciii_clinical.admissions`\n",
    "    WHERE subject_id IN UNNEST(@subject_ids)\n",
    "    ORDER BY subject_id, admittime\n",
    "    \"\"\"\n",
    "    cfg = bq.QueryJobConfig(\n",
    "        query_parameters=[bq.ArrayQueryParameter(\"subject_ids\", \"INT64\", subject_ids)]\n",
    "    )\n",
    "    return safe_bq_to_df(sql, job_config=cfg)\n",
    "\n",
    "all_adm_df = get_all_admissions(subject_ids) if subject_ids else pd.DataFrame()\n",
    "\n",
    "# Compute LOS and filter >=54h in first_admissions_df\n",
    "if not first_admissions_df.empty:\n",
    "    fadm = first_admissions_df.copy()\n",
    "    fadm['admittime'] = pd.to_datetime(fadm['admittime'])\n",
    "    fadm['dischtime'] = pd.to_datetime(fadm['dischtime'])\n",
    "    fadm['los_hours'] = (fadm['dischtime'] - fadm['admittime']).dt.total_seconds() / 3600.0\n",
    "    fadm = fadm[fadm['los_hours'] >= 54].copy()\n",
    "else:\n",
    "    fadm = pd.DataFrame(columns=['subject_id','hadm_id','admittime','dischtime','los_hours'])\n",
    "\n",
    "# Recompute hadm_ids after LOS filter\n",
    "hadm_ids_filtered: List[int] = fadm.get('hadm_id', pd.Series([], dtype='int')).dropna().astype(int).tolist()\n",
    "print(f\"First admissions with LOS>=54h: {len(hadm_ids_filtered)}\")\n",
    "\n",
    "# Subset 48h extracts to filtered hadm_ids\n",
    "if not vitals_df.empty:\n",
    "    vitals_df = vitals_df[vitals_df['hadm_id'].isin(hadm_ids_filtered)]\n",
    "if not labs_df.empty:\n",
    "    labs_df = labs_df[labs_df['hadm_id'].isin(hadm_ids_filtered)]\n",
    "if not prescriptions_df.empty:\n",
    "    prescriptions_df = prescriptions_df[prescriptions_df['hadm_id'].isin(hadm_ids_filtered)]\n",
    "if 'procedures_df' in globals() and not procedures_df.empty:\n",
    "    procedures_df = procedures_df[procedures_df['hadm_id'].isin(hadm_ids_filtered)]\n",
    "\n",
    "# Mortality: in-hospital death OR death <= 30 days after discharge\n",
    "mortality_df = pd.DataFrame()\n",
    "if not fadm.empty:\n",
    "    mort = fadm[['subject_id','hadm_id','admittime','dischtime']].copy()\n",
    "    # join death data\n",
    "    demo_for_death = demographics_df[['subject_id','dod']].copy() if not demographics_df.empty else pd.DataFrame(columns=['subject_id','dod'])\n",
    "    mort = mort.merge(demo_for_death, on='subject_id', how='left')\n",
    "    mort['dod'] = pd.to_datetime(mort['dod'])\n",
    "    # in-hospital death via admissions.deathtime if available (from all_adm_df which has deathtime)\n",
    "    if not all_adm_df.empty:\n",
    "        tmp_death = all_adm_df[['hadm_id','deathtime']].drop_duplicates()\n",
    "        tmp_death['deathtime'] = pd.to_datetime(tmp_death['deathtime'])\n",
    "        mort = mort.merge(tmp_death, on='hadm_id', how='left')\n",
    "    else:\n",
    "        mort['deathtime'] = pd.NaT\n",
    "    death_in_hosp = mort['deathtime'].notna()\n",
    "    death_within_30d = (mort['dod'].notna()) & (mort['dod'] <= (mort['dischtime'] + pd.Timedelta(days=30))) & (mort['dod'] >= mort['dischtime'])\n",
    "    mort['mortality_label'] = (death_in_hosp | death_within_30d).astype(int)\n",
    "    mortality_df = mort[['subject_id','hadm_id','mortality_label']]\n",
    "\n",
    "# Prolonged LOS: > 7 days\n",
    "prolonged_los_df = pd.DataFrame()\n",
    "if not fadm.empty:\n",
    "    pl = fadm[['subject_id','hadm_id','los_hours']].copy()\n",
    "    pl['prolonged_los_label'] = (pl['los_hours'] > 7*24).astype(int)\n",
    "    prolonged_los_df = pl[['subject_id','hadm_id','prolonged_los_label']]\n",
    "\n",
    "# 30-day readmission: second hospital admission within 30 days of first discharge\n",
    "readmit_df = pd.DataFrame()\n",
    "if not fadm.empty and not all_adm_df.empty:\n",
    "    fa = fadm[['subject_id','hadm_id','dischtime']].rename(columns={'hadm_id':'first_hadm_id','dischtime':'first_dischtime'})\n",
    "    nxt = all_adm_df.sort_values(['subject_id','admittime']).copy()\n",
    "    # compute next admission per subject after the first hadm\n",
    "    nxt['is_after_first'] = False\n",
    "    # merge to tag the first discharge\n",
    "    nxt = nxt.merge(fa[['subject_id','first_hadm_id','first_dischtime']], on='subject_id', how='left')\n",
    "    nxt['is_after_first'] = (nxt['admittime'] > nxt['first_dischtime'])\n",
    "    # next admission within 30 days exists?\n",
    "    within_30 = nxt[nxt['is_after_first']].copy()\n",
    "    within_30['within_30d'] = within_30['admittime'] <= (within_30['first_dischtime'] + pd.Timedelta(days=30))\n",
    "    rn = within_30.groupby('subject_id')['within_30d'].any().reset_index().rename(columns={'within_30d':'readmit_30d'})\n",
    "    readmit_df = fa[['subject_id','first_hadm_id']].merge(rn, on='subject_id', how='left').fillna({'readmit_30d': False})\n",
    "    readmit_df['readmission_label'] = readmit_df['readmit_30d'].astype(int)\n",
    "    readmit_df = readmit_df.rename(columns={'first_hadm_id':'hadm_id'})[['subject_id','hadm_id','readmission_label']]\n",
    "\n",
    "# Combine labels\n",
    "labels_df = fadm[['subject_id','hadm_id']].copy()\n",
    "labels_df = labels_df.merge(mortality_df, on=['subject_id','hadm_id'], how='left')\n",
    "labels_df = labels_df.merge(prolonged_los_df, on=['subject_id','hadm_id'], how='left')\n",
    "labels_df = labels_df.merge(readmit_df, on=['subject_id','hadm_id'], how='left')\n",
    "labels_df = labels_df.fillna({'mortality_label':0, 'prolonged_los_label':0, 'readmission_label':0}).astype({\n",
    "    'mortality_label': int,\n",
    "    'prolonged_los_label': int,\n",
    "    'readmission_label': int,\n",
    "})\n",
    "print(labels_df.head())\n"
   ],
   "id": "cb3751f92ef7164c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First admissions with LOS>=54h: 28473\n",
      "   subject_id  hadm_id  mortality_label  prolonged_los_label  \\\n",
      "0           2   163353                0                    0   \n",
      "1           3   145834                0                    1   \n",
      "2           4   185777                0                    1   \n",
      "3           5   178980                0                    0   \n",
      "4           7   118037                0                    0   \n",
      "\n",
      "   readmission_label  \n",
      "0                  0  \n",
      "1                  0  \n",
      "2                  0  \n",
      "3                  0  \n",
      "4                  0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almog Luz\\AppData\\Local\\Temp\\ipykernel_22864\\165805816.py:86: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  readmit_df = fa[['subject_id','first_hadm_id']].merge(rn, on='subject_id', how='left').fillna({'readmit_30d': False})\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:48.767313Z",
     "start_time": "2025-09-05T06:17:32.538870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Feature engineering: aggregate 0-48h features and build a subject-level matrix ---\n",
    "from collections import defaultdict\n",
    "\n",
    "# Demographics features\n",
    "feat_demo = pd.DataFrame()\n",
    "if not fadm.empty and not demographics_df.empty:\n",
    "    # Use ethnicity from first admissions; patients table doesn't have it\n",
    "    tmp = fadm[['subject_id','admittime','ethnicity']].merge(\n",
    "        demographics_df[['subject_id','gender','dob']], on='subject_id', how='left'\n",
    "    )\n",
    "    # Age (cap at 90)\n",
    "    tmp['age'] = (pd.to_datetime(tmp['admittime']) - pd.to_datetime(tmp['dob'])).dt.days / 365.25\n",
    "    tmp['age'] = tmp['age'].clip(lower=0)\n",
    "    tmp.loc[tmp['age'] >= 89, 'age'] = 90\n",
    "    # Gender one-hot\n",
    "    tmp['gender_M'] = (tmp['gender'].astype(str).str.upper() == 'M').astype(int)\n",
    "    tmp['gender_F'] = (tmp['gender'].astype(str).str.upper() == 'F').astype(int)\n",
    "    # Ethnicity buckets and one-hot (keep small number of columns)\n",
    "    def _eth_bucket(x: str) -> str:\n",
    "        s = str(x).lower()\n",
    "        if 'white' in s:\n",
    "            return 'WHITE'\n",
    "        if 'black' in s:\n",
    "            return 'BLACK'\n",
    "        if 'asian' in s:\n",
    "            return 'ASIAN'\n",
    "        if 'hisp' in s or 'latino' in s or 'latina' in s:\n",
    "            return 'HISPANIC'\n",
    "        return 'OTHER'\n",
    "    tmp['eth_bucket'] = tmp['ethnicity'].apply(_eth_bucket)\n",
    "    eth_dummies = pd.get_dummies(tmp['eth_bucket'], prefix='eth', dtype=int)\n",
    "    feat_demo = pd.concat([tmp[['subject_id','age','gender_M','gender_F']], eth_dummies], axis=1)\n",
    "# Helper to aggregate events\n",
    "\n",
    "def aggregate_events(df: pd.DataFrame, value_col: str, time_col: str, label_col: str) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=['subject_id'])\n",
    "    d = df.dropna(subset=[value_col]).copy()\n",
    "    if d.empty:\n",
    "        return pd.DataFrame(columns=['subject_id'])\n",
    "    # last value per item by time\n",
    "    d[time_col] = pd.to_datetime(d[time_col])\n",
    "    last_vals = d.sort_values([\"subject_id\", label_col, time_col]).groupby(['subject_id', label_col]).tail(1)\n",
    "    agg = d.groupby(['subject_id', label_col])[value_col].agg(['mean','min','max'])\n",
    "    agg = agg.reset_index()\n",
    "    last = last_vals[['subject_id', label_col, value_col]].rename(columns={value_col:'last'})\n",
    "    wide = agg.merge(last, on=['subject_id', label_col], how='left')\n",
    "    # pivot\n",
    "    wide_cols = []\n",
    "    for stat in ['mean','min','max','last']:\n",
    "        pivot = wide.pivot_table(index='subject_id', columns=label_col, values=stat)\n",
    "        pivot.columns = [f\"{str(c)}__{stat}\" for c in pivot.columns]\n",
    "        wide_cols.append(pivot)\n",
    "    out = pd.concat(wide_cols, axis=1)\n",
    "    out = out.reset_index()\n",
    "    return out\n",
    "\n",
    "feat_vitals = aggregate_events(vitals_df, value_col='valuenum', time_col='charttime', label_col='item_label') if 'vitals_df' in globals() else pd.DataFrame()\n",
    "feat_labs = aggregate_events(labs_df, value_col='valuenum', time_col='charttime', label_col='item_label') if 'labs_df' in globals() else pd.DataFrame()\n",
    "\n",
    "# Prescriptions: simple pharmacotherapy flags\n",
    "feat_rx = pd.DataFrame()\n",
    "if 'prescriptions_df' in globals() and not prescriptions_df.empty:\n",
    "    rx = prescriptions_df.copy()\n",
    "    rx['drug_low'] = rx['drug'].astype(str).str.lower()\n",
    "    def any_pattern(series, patterns):\n",
    "        return series.str.contains('|'.join(patterns), na=False)\n",
    "    abx_patterns = ['cillin','cef','ceph','penem','floxacin','vancomycin','metronidazole','piperacillin','tazobactam']\n",
    "    insulin_patterns = ['insulin']\n",
    "    diuretic_patterns = ['furosemide','lasix','bumetanide','torsemide','hydrochlorothiazide','hctz','spironolactone']\n",
    "    steroid_patterns = ['predni','methylpred','hydrocortisone','dexamethasone']\n",
    "    grp = rx.groupby('subject_id')\n",
    "    feat_rx = pd.DataFrame({\n",
    "        'subject_id': grp.size().index,\n",
    "        'rx_total_mentions': grp.size().values,\n",
    "        'rx_unique_drugs': grp['drug_low'].nunique().values,\n",
    "    })\n",
    "    # flags\n",
    "    rx_flags = rx[['subject_id','drug_low']].copy()\n",
    "    flags = rx_flags.groupby('subject_id').agg({\n",
    "        'drug_low': lambda s: pd.Series({\n",
    "            'rx_any_abx': any_pattern(s, abx_patterns).any(),\n",
    "            'rx_any_insulin': any_pattern(s, insulin_patterns).any(),\n",
    "            'rx_any_diuretic': any_pattern(s, diuretic_patterns).any(),\n",
    "            'rx_any_steroid': any_pattern(s, steroid_patterns).any(),\n",
    "        })\n",
    "    })\n",
    "    flags = pd.DataFrame(list(flags['drug_low'].values), index=flags.index).reset_index().rename(columns={'index':'subject_id'})\n",
    "    for c in flags.columns:\n",
    "        if c != 'subject_id':\n",
    "            flags[c] = flags[c].astype(int)\n",
    "    feat_rx = feat_rx.merge(flags, on='subject_id', how='left')\n",
    "\n",
    "# Procedures: heuristic flags\n",
    "feat_proc = pd.DataFrame()\n",
    "if 'procedures_df' in globals() and not procedures_df.empty:\n",
    "    p = procedures_df.copy()\n",
    "    label_col = 'item_label' if 'item_label' in p.columns else 'ordercategorydescription'\n",
    "    p['lbl_low'] = p[label_col].astype(str).str.lower()\n",
    "    def flag_frac(df, patterns):\n",
    "        return df.groupby('subject_id').apply(lambda g: g['lbl_low'].str.contains('|'.join(patterns), na=False).any())\n",
    "    vent = flag_frac(p, ['vent','intubat','endotracheal','peep','tidal volume'])\n",
    "    rrt = flag_frac(p, ['dialysis','crrt','hemodialysis'])\n",
    "    cl = flag_frac(p, ['central line','cvc','subclavian','internal jugular','femoral line'])\n",
    "    feat_proc = pd.DataFrame({\n",
    "        'subject_id': vent.index,\n",
    "        'proc_vent_any': vent.astype(int).values,\n",
    "        'proc_rrt_any': rrt.astype(int).values,\n",
    "        'proc_central_line_any': cl.astype(int).values,\n",
    "    })\n",
    "\n",
    "# Merge all features on subject_id\n",
    "features = feat_demo.copy()\n",
    "for df in [feat_vitals, feat_labs, feat_rx, feat_proc]:\n",
    "    if not df.empty:\n",
    "        features = features.merge(df, on='subject_id', how='left')\n",
    "\n",
    "features = features.drop_duplicates('subject_id')\n",
    "features = features.set_index('subject_id')\n",
    "features = features.apply(pd.to_numeric, errors='ignore')\n",
    "# Align to labeled cohort\n",
    "features = features.loc[features.index.intersection(labels_df['subject_id'])]\n",
    "# Ensure all column names are strings (sklearn requires homogeneous string feature names)\n",
    "features.columns = features.columns.map(str)\n",
    "print(f\"Feature matrix shape: {features.shape}\")\n"
   ],
   "id": "81847f27d3840ec4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almog Luz\\AppData\\Local\\Temp\\ipykernel_22864\\3926683724.py:101: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('subject_id').apply(lambda g: g['lbl_low'].str.contains('|'.join(patterns), na=False).any())\n",
      "C:\\Users\\Almog Luz\\AppData\\Local\\Temp\\ipykernel_22864\\3926683724.py:101: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('subject_id').apply(lambda g: g['lbl_low'].str.contains('|'.join(patterns), na=False).any())\n",
      "C:\\Users\\Almog Luz\\AppData\\Local\\Temp\\ipykernel_22864\\3926683724.py:101: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('subject_id').apply(lambda g: g['lbl_low'].str.contains('|'.join(patterns), na=False).any())\n",
      "C:\\Users\\Almog Luz\\AppData\\Local\\Temp\\ipykernel_22864\\3926683724.py:120: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  features = features.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (28473, 1097)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:56.019934Z",
     "start_time": "2025-09-05T06:17:48.951500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Train 3 separate calibrated models and save artifacts ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import joblib\n",
    "\n",
    "# Prepare labels indexed by subject_id\n",
    "labels_idx = labels_df[['subject_id','mortality_label','prolonged_los_label','readmission_label']].drop_duplicates().set_index('subject_id')\n",
    "# Keep rows with features\n",
    "common_idx = features.index.intersection(labels_idx.index)\n",
    "X = features.loc[common_idx]\n",
    "Y = labels_idx.loc[common_idx]\n",
    "# Also ensure X columns are strings explicitly\n",
    "X.columns = X.columns.map(str)\n",
    "\n",
    "# Train/val/test split (consistent across targets) with robust stratification fallback\n",
    "from collections import Counter as _Counter\n",
    "_y_mort = Y['mortality_label']\n",
    "_stratify_ok = (_y_mort.nunique() >= 2) and min(_Counter(_y_mort.values).values()) >= 2\n",
    "if _stratify_ok:\n",
    "    train_ids, test_ids = train_test_split(common_idx, test_size=0.2, random_state=42, stratify=_y_mort)\n",
    "    _y_train = Y.loc[train_ids, 'mortality_label']\n",
    "    _stratify_ok2 = (_y_train.nunique() >= 2) and min(_Counter(_y_train.values).values()) >= 2\n",
    "    if _stratify_ok2:\n",
    "        train_ids, val_ids = train_test_split(train_ids, test_size=0.2, random_state=42, stratify=Y.loc[train_ids, 'mortality_label'])\n",
    "    else:\n",
    "        print(\"Warning: insufficient class balance for stratified val split; using unstratified split.\")\n",
    "        train_ids, val_ids = train_test_split(train_ids, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    print(\"Warning: insufficient class balance for stratified splits; using unstratified splits.\")\n",
    "    train_ids, test_ids = train_test_split(common_idx, test_size=0.2, random_state=42)\n",
    "    train_ids, val_ids = train_test_split(train_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, X_test = X.loc[train_ids], X.loc[val_ids], X.loc[test_ids]\n",
    "\n",
    "# Prepare preprocessor\n",
    "preprocessor = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "])\n",
    "\n",
    "X_train_t = preprocessor.fit_transform(X_train)\n",
    "X_val_t = preprocessor.transform(X_val)\n",
    "X_test_t = preprocessor.transform(X_test)\n",
    "\n",
    "# Helper to train and calibrate a model for a single target\n",
    "\n",
    "def train_calibrated(X_tr, y_tr, X_val, y_val):\n",
    "    import numpy as _np\n",
    "    # Fallback if training labels are single-class\n",
    "    if len(_np.unique(y_tr)) < 2:\n",
    "        print(\"Warning: single-class training labels; using DummyClassifier.\")\n",
    "        dummy = DummyClassifier(strategy='constant', constant=int(_np.unique(y_tr)[0]))\n",
    "        dummy.fit(X_tr, y_tr)\n",
    "        return dummy\n",
    "    base = LogisticRegression(max_iter=500, n_jobs=None)\n",
    "    base.fit(X_tr, y_tr)\n",
    "    # If validation has a single class, skip calibration to avoid errors\n",
    "    if len(_np.unique(y_val)) < 2:\n",
    "        return base\n",
    "    # scikit-learn >=1.4 uses 'estimator' instead of 'base_estimator'\n",
    "    calib = CalibratedClassifierCV(estimator=base, method='sigmoid', cv='prefit')\n",
    "    calib.fit(X_val, y_val)\n",
    "    return calib\n",
    "\n",
    "models = {}\n",
    "metrics = {}\n",
    "for target, col in [('mortality','mortality_label'), ('prolonged_los','prolonged_los_label'), ('readmission','readmission_label')]:\n",
    "    y_train = Y.loc[train_ids, col].values\n",
    "    y_val = Y.loc[val_ids, col].values\n",
    "    y_test = Y.loc[test_ids, col].values\n",
    "    model = train_calibrated(X_train_t, y_train, X_val_t, y_val)\n",
    "    models[target] = model\n",
    "    # eval\n",
    "    proba = model.predict_proba(X_test_t)[:,1]\n",
    "    metrics[target] = {\n",
    "        'roc_auc': float(roc_auc_score(y_test, proba)) if len(np.unique(y_test))>1 else np.nan,\n",
    "        'pr_auc': float(average_precision_score(y_test, proba)) if len(np.unique(y_test))>1 else np.nan,\n",
    "        'brier': float(brier_score_loss(y_test, proba)),\n",
    "        'positives_test': int(y_test.sum()),\n",
    "        'n_test': int(len(y_test)),\n",
    "    }\n",
    "\n",
    "print(\"\\nTest metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(k, v)\n",
    "\n",
    "# Save artifacts\n",
    "models_dir = os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "feature_cols = list(X.columns)\n",
    "joblib.dump(preprocessor, os.path.join(models_dir, 'preprocessor.joblib'))\n",
    "joblib.dump(models['mortality'], os.path.join(models_dir, 'model_mortality.joblib'))\n",
    "joblib.dump(models['prolonged_los'], os.path.join(models_dir, 'model_prolonged_los.joblib'))\n",
    "joblib.dump(models['readmission'], os.path.join(models_dir, 'model_readmission.joblib'))\n",
    "\n",
    "# Save feature columns for alignment in unseen evaluation\n",
    "import json\n",
    "with open(os.path.join(models_dir, 'feature_columns.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(feature_cols, f)\n",
    "print(f\"Saved models and artifacts to: {models_dir}\")\n"
   ],
   "id": "52ea93c20148e72d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['APOTININ CC/HR__mean' 'Amicar cc/hr__mean' 'FEM ART MAP__mean'\n",
      " 'Mucomyst mg/hr__mean' 'NIMBEX MG/KG/HR__mean'\n",
      " 'Pantoprazole   mg/hr__mean' 'Protonix       mg/hr__mean'\n",
      " 'Solumedrol  mg/kg/hr__mean' 'TPA MG/HR__mean' 'approtinin cc/hr__mean'\n",
      " 'left radial MAP__mean' 'nicardipine mg/hr__mean' 'APOTININ CC/HR__min'\n",
      " 'Amicar cc/hr__min' 'FEM ART MAP__min' 'Mucomyst mg/hr__min'\n",
      " 'NIMBEX MG/KG/HR__min' 'Pantoprazole   mg/hr__min'\n",
      " 'Protonix       mg/hr__min' 'Solumedrol  mg/kg/hr__min' 'TPA MG/HR__min'\n",
      " 'approtinin cc/hr__min' 'left radial MAP__min' 'nicardipine mg/hr__min'\n",
      " 'APOTININ CC/HR__max' 'Amicar cc/hr__max' 'FEM ART MAP__max'\n",
      " 'Mucomyst mg/hr__max' 'NIMBEX MG/KG/HR__max' 'Pantoprazole   mg/hr__max'\n",
      " 'Protonix       mg/hr__max' 'Solumedrol  mg/kg/hr__max' 'TPA MG/HR__max'\n",
      " 'approtinin cc/hr__max' 'left radial MAP__max' 'nicardipine mg/hr__max'\n",
      " 'APOTININ CC/HR__last' 'Amicar cc/hr__last' 'FEM ART MAP__last'\n",
      " 'Mucomyst mg/hr__last' 'NIMBEX MG/KG/HR__last'\n",
      " 'Pantoprazole   mg/hr__last' 'Protonix       mg/hr__last'\n",
      " 'Solumedrol  mg/kg/hr__last' 'TPA MG/HR__last' 'approtinin cc/hr__last'\n",
      " 'left radial MAP__last' 'nicardipine mg/hr__last' 'Chloride, Stool__mean'\n",
      " 'Chloride, Stool__min' 'Chloride, Stool__max' 'Chloride, Stool__last']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['APOTININ CC/HR__mean' 'Amicar cc/hr__mean' 'FEM ART MAP__mean'\n",
      " 'Mucomyst mg/hr__mean' 'NIMBEX MG/KG/HR__mean'\n",
      " 'Pantoprazole   mg/hr__mean' 'Protonix       mg/hr__mean'\n",
      " 'Solumedrol  mg/kg/hr__mean' 'TPA MG/HR__mean' 'approtinin cc/hr__mean'\n",
      " 'left radial MAP__mean' 'nicardipine mg/hr__mean' 'APOTININ CC/HR__min'\n",
      " 'Amicar cc/hr__min' 'FEM ART MAP__min' 'Mucomyst mg/hr__min'\n",
      " 'NIMBEX MG/KG/HR__min' 'Pantoprazole   mg/hr__min'\n",
      " 'Protonix       mg/hr__min' 'Solumedrol  mg/kg/hr__min' 'TPA MG/HR__min'\n",
      " 'approtinin cc/hr__min' 'left radial MAP__min' 'nicardipine mg/hr__min'\n",
      " 'APOTININ CC/HR__max' 'Amicar cc/hr__max' 'FEM ART MAP__max'\n",
      " 'Mucomyst mg/hr__max' 'NIMBEX MG/KG/HR__max' 'Pantoprazole   mg/hr__max'\n",
      " 'Protonix       mg/hr__max' 'Solumedrol  mg/kg/hr__max' 'TPA MG/HR__max'\n",
      " 'approtinin cc/hr__max' 'left radial MAP__max' 'nicardipine mg/hr__max'\n",
      " 'APOTININ CC/HR__last' 'Amicar cc/hr__last' 'FEM ART MAP__last'\n",
      " 'Mucomyst mg/hr__last' 'NIMBEX MG/KG/HR__last'\n",
      " 'Pantoprazole   mg/hr__last' 'Protonix       mg/hr__last'\n",
      " 'Solumedrol  mg/kg/hr__last' 'TPA MG/HR__last' 'approtinin cc/hr__last'\n",
      " 'left radial MAP__last' 'nicardipine mg/hr__last' 'Chloride, Stool__mean'\n",
      " 'Chloride, Stool__min' 'Chloride, Stool__max' 'Chloride, Stool__last']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\.venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['APOTININ CC/HR__mean' 'Amicar cc/hr__mean' 'FEM ART MAP__mean'\n",
      " 'Mucomyst mg/hr__mean' 'NIMBEX MG/KG/HR__mean'\n",
      " 'Pantoprazole   mg/hr__mean' 'Protonix       mg/hr__mean'\n",
      " 'Solumedrol  mg/kg/hr__mean' 'TPA MG/HR__mean' 'approtinin cc/hr__mean'\n",
      " 'left radial MAP__mean' 'nicardipine mg/hr__mean' 'APOTININ CC/HR__min'\n",
      " 'Amicar cc/hr__min' 'FEM ART MAP__min' 'Mucomyst mg/hr__min'\n",
      " 'NIMBEX MG/KG/HR__min' 'Pantoprazole   mg/hr__min'\n",
      " 'Protonix       mg/hr__min' 'Solumedrol  mg/kg/hr__min' 'TPA MG/HR__min'\n",
      " 'approtinin cc/hr__min' 'left radial MAP__min' 'nicardipine mg/hr__min'\n",
      " 'APOTININ CC/HR__max' 'Amicar cc/hr__max' 'FEM ART MAP__max'\n",
      " 'Mucomyst mg/hr__max' 'NIMBEX MG/KG/HR__max' 'Pantoprazole   mg/hr__max'\n",
      " 'Protonix       mg/hr__max' 'Solumedrol  mg/kg/hr__max' 'TPA MG/HR__max'\n",
      " 'approtinin cc/hr__max' 'left radial MAP__max' 'nicardipine mg/hr__max'\n",
      " 'APOTININ CC/HR__last' 'Amicar cc/hr__last' 'FEM ART MAP__last'\n",
      " 'Mucomyst mg/hr__last' 'NIMBEX MG/KG/HR__last'\n",
      " 'Pantoprazole   mg/hr__last' 'Protonix       mg/hr__last'\n",
      " 'Solumedrol  mg/kg/hr__last' 'TPA MG/HR__last' 'approtinin cc/hr__last'\n",
      " 'left radial MAP__last' 'nicardipine mg/hr__last' 'Chloride, Stool__mean'\n",
      " 'Chloride, Stool__min' 'Chloride, Stool__max' 'Chloride, Stool__last']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\.venv\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\.venv\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test metrics:\n",
      "mortality {'roc_auc': 0.8215776353344071, 'pr_auc': 0.379138522657292, 'brier': 0.08379785195207613, 'positives_test': 622, 'n_test': 5695}\n",
      "prolonged_los {'roc_auc': 0.7291081045953189, 'pr_auc': 0.7160317495940125, 'brier': 0.24947432700972497, 'positives_test': 2967, 'n_test': 5695}\n",
      "readmission {'roc_auc': 0.5566588510947464, 'pr_auc': 0.04830008962497373, 'brier': 0.03836180003795895, 'positives_test': 226, 'n_test': 5695}\n",
      "Saved models and artifacts to: C:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\project\\models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\.venv\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T06:17:56.167176Z",
     "start_time": "2025-09-05T06:17:56.164235Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8090e709887b40b1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
