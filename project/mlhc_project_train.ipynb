{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f50582c5233013f6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# MLHC: Lean Training Notebook\n",
    "\n",
    "Minimal end-to-end training: configure, (optionally) build labels, train models, inspect metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3707e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path[0]= c:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\project\\project\n"
     ]
    }
   ],
   "source": [
    "# Path setup so 'project' package imports regardless of IDE CWD\n",
    "import sys, os, pathlib\n",
    "cands = [pathlib.Path.cwd(), pathlib.Path.cwd().parent, pathlib.Path.cwd()/ 'project', pathlib.Path.cwd().parent / 'project']\n",
    "for p in cands:\n",
    "    if p.is_dir() and str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "print('sys.path[0]=', sys.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28605645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 20250905_113628\n",
      "Artifacts root: project\\runs\\20250905_113628\n",
      "Models dir: project\\runs\\20250905_113628\\models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ml-for-healthcare-2025',\n",
       " 'data/initial_cohort.csv',\n",
       " None,\n",
       " True,\n",
       " 'project\\\\runs\\\\20250905_113628',\n",
       " 'project\\\\runs\\\\20250905_113628\\\\models')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User configuration\n",
    "import os, pathlib, datetime as _dt\n",
    "# Default copied from exploration notebook; change if your project differs.\n",
    "GCP_PROJECT_ID = 'ml-for-healthcare-2025'\n",
    "# Because this notebook lives inside the 'project/' directory, the cohort CSV is directly under 'data/'\n",
    "# Use a robust resolver that tests a few candidate paths so it also works if run from repo root.\n",
    "_COHORT_CANDIDATES = [\n",
    "    'data/initial_cohort.csv',  # notebook cwd inside project/\n",
    "    os.path.join('project','data','initial_cohort.csv'),  # if run from repo root\n",
    "    os.path.join('..','project','data','initial_cohort.csv'),  # defensive\n",
    "]\n",
    "INITIAL_COHORT_CSV = None\n",
    "for c in _COHORT_CANDIDATES:\n",
    "    if os.path.exists(c):\n",
    "        INITIAL_COHORT_CSV = c\n",
    "        break\n",
    "if INITIAL_COHORT_CSV is None:\n",
    "    # leave as first candidate; downstream cell will still raise a clear error\n",
    "    INITIAL_COHORT_CSV = _COHORT_CANDIDATES[0]\n",
    "LABELS_CSV = None  # if provided, skip generation; must have subject_id + *_label columns\n",
    "GENERATE_LABELS = True  # build labels if LABELS_CSV is None\n",
    "# Per-run artifacts directory (local time is fine here; uniqueness matters more than timezone)\n",
    "RUN_ID = _dt.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# Determine where to place runs/ to avoid accidental project/project nesting.\n",
    "# If we're already inside the 'project' directory (common when opening the notebook directly),\n",
    "# create 'runs/' right here. If running from repo root (where a 'project' dir exists), place\n",
    "# runs under project/runs. Fallback to a plain 'runs' if neither heuristic matches.\n",
    "if os.path.basename(os.getcwd()) == 'project':\n",
    "    RUNS_ROOT = 'runs'\n",
    "elif os.path.isdir('project'):\n",
    "    RUNS_ROOT = os.path.join('project', 'runs')\n",
    "else:\n",
    "    RUNS_ROOT = 'runs'\n",
    "# Collapse any accidental duplicate segment (idempotent safeguard)\n",
    "RUNS_ROOT = RUNS_ROOT.replace('project' + os.sep + 'project', 'project')\n",
    "ARTIFACTS_DIR = os.path.join(RUNS_ROOT, RUN_ID)\n",
    "OUTPUT_MODELS_DIR = os.path.join(ARTIFACTS_DIR,'models')\n",
    "os.makedirs(OUTPUT_MODELS_DIR, exist_ok=True)\n",
    "# Expose models dir for downstream inference scripts (unseen evaluation) via env var\n",
    "os.environ['MLHC_MODELS_DIR'] = os.path.abspath(OUTPUT_MODELS_DIR)\n",
    "print(f'Run ID: {RUN_ID}')\n",
    "print('Artifacts root:', ARTIFACTS_DIR)\n",
    "print('Models dir:', OUTPUT_MODELS_DIR)\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "GCP_PROJECT_ID, INITIAL_COHORT_CSV, LABELS_CSV, GENERATE_LABELS, ARTIFACTS_DIR, OUTPUT_MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7738617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort size: 32513 from data/initial_cohort.csv\n",
      "Saved cohort snapshot to project\\runs\\20250905_113628\\data\\cohort_snapshot.csv\n",
      "BigQuery client ready.\n",
      "BigQuery client ready.\n"
     ]
    }
   ],
   "source": [
    "# Load cohort & init BigQuery client (robust path resolution)\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd, json\n",
    "if not os.path.exists(INITIAL_COHORT_CSV):\n",
    "    raise FileNotFoundError(\n",
    "        'Missing cohort CSV. Looked for: ' + INITIAL_COHORT_CSV + '\\n'\n",
    "        'If running from repo root, set INITIAL_COHORT_CSV to project/data/initial_cohort.csv; '\n",
    "        'if running inside project/, set to data/initial_cohort.csv.'\n",
    "    )\n",
    "cohort_df = pd.read_csv(INITIAL_COHORT_CSV)\n",
    "if 'subject_id' not in cohort_df.columns:\n",
    "    raise ValueError('INITIAL_COHORT_CSV must contain subject_id')\n",
    "subject_ids = cohort_df['subject_id'].dropna().astype(int).tolist()\n",
    "print(f'Cohort size: {len(subject_ids)} from {INITIAL_COHORT_CSV}')\n",
    "os.makedirs(os.path.join(ARTIFACTS_DIR,'data'), exist_ok=True)\n",
    "cohort_copy_path = os.path.join(ARTIFACTS_DIR,'data','cohort_snapshot.csv')\n",
    "cohort_df.to_csv(cohort_copy_path, index=False)\n",
    "print('Saved cohort snapshot to', cohort_copy_path)\n",
    "try:\n",
    "    bq_client = bigquery.Client(project=GCP_PROJECT_ID)\n",
    "    print('BigQuery client ready.')\n",
    "except Exception as e:\n",
    "    bq_client = None\n",
    "    print('Warning: BigQuery client not initialized ->', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a390a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated labels file: project\\runs\\20250905_113628\\data\\labels.csv (n=28473)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "subject_id",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "hadm_id",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "mortality_label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "prolonged_los_label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "readmission_label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "926da7ff-2f40-4cf3-b81c-54cda136893d",
       "rows": [
        [
         "0",
         "2",
         "163353",
         "0",
         "0",
         "0"
        ],
        [
         "1",
         "3",
         "145834",
         "0",
         "1",
         "0"
        ],
        [
         "2",
         "4",
         "185777",
         "0",
         "1",
         "0"
        ],
        [
         "3",
         "5",
         "178980",
         "0",
         "0",
         "0"
        ],
        [
         "4",
         "7",
         "118037",
         "0",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>mortality_label</th>\n",
       "      <th>prolonged_los_label</th>\n",
       "      <th>readmission_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>185777</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>178980</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>118037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  hadm_id  mortality_label  prolonged_los_label  \\\n",
       "0           2   163353                0                    0   \n",
       "1           3   145834                0                    1   \n",
       "2           4   185777                0                    1   \n",
       "3           5   178980                0                    0   \n",
       "4           7   118037                0                    0   \n",
       "\n",
       "   readmission_label  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build labels if needed (requires BigQuery access)\n",
    "import datetime as _dt\n",
    "if LABELS_CSV is None and GENERATE_LABELS:\n",
    "    if bq_client is None:\n",
    "        raise RuntimeError('Cannot generate labels without BigQuery client/auth. Provide LABELS_CSV instead.')\n",
    "    from project.labels import build_and_save_labels\n",
    "    os.makedirs(os.path.join(ARTIFACTS_DIR,'data'), exist_ok=True)\n",
    "    LABELS_CSV = os.path.join(ARTIFACTS_DIR,'data', 'labels.csv')\n",
    "    labels_df = build_and_save_labels(bq_client, subject_ids, LABELS_CSV)\n",
    "    print(f'Generated labels file: {LABELS_CSV} (n={len(labels_df)})')\n",
    "elif LABELS_CSV is not None:\n",
    "    if not os.path.exists(LABELS_CSV):\n",
    "        raise FileNotFoundError(f'Provided LABELS_CSV not found: {LABELS_CSV}')\n",
    "    labels_df = pd.read_csv(LABELS_CSV)\n",
    "    print(f'Loaded existing labels (n={len(labels_df)})')\n",
    "else:\n",
    "    raise ValueError('LABELS_CSV is None and GENERATE_LABELS is False -> nothing to train.')\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e64ceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported project.train from c:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\project\\train.py\n",
      "First line of _fit_model(): def _fit_model(X_t, y: np.ndarray) -> CalibratedClassifierCV:\n",
      "[1/5] Loading labels CSV ...\n",
      "Copied labels to c:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\project\\project\\runs\\20250905_113628\\labels.csv\n",
      "[2/5] Building / loading feature matrices ...\n",
      "[2/5] Building / loading feature matrices ...\n",
      "Loaded features from cache directory (skipped BigQuery): c:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\project\\data\\extracted_cache\n",
      "Loaded features from cache directory (skipped BigQuery): c:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\project\\data\\extracted_cache\n",
      "[3/5] Fitting preprocessing pipeline on 28473 patients x 1117 features ...\n",
      "[3/5] Fitting preprocessing pipeline on 28473 patients x 1117 features ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159a07bed4c743c98020ddb41e488162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[4/5] Training targets:   0%|          | 0/3 [00:00<?, ?model/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CalibratedClassifierCV created with method=isotonic (using 'estimator' kwarg).\n",
      "CalibratedClassifierCV created with method=isotonic (using 'estimator' kwarg).\n",
      "CalibratedClassifierCV created with method=isotonic (using 'estimator' kwarg).\n",
      "CalibratedClassifierCV created with method=isotonic (using 'estimator' kwarg).\n",
      "CalibratedClassifierCV created with method=isotonic (using 'estimator' kwarg).\n",
      "[5/5] Writing metrics.json\n",
      "Training complete. Metrics summary below (saved to project\\runs\\20250905_113628\\models\\metrics.json ):\n",
      "[5/5] Writing metrics.json\n",
      "Training complete. Metrics summary below (saved to project\\runs\\20250905_113628\\models\\metrics.json ):\n"
     ]
    }
   ],
   "source": [
    "# Train models (with diagnostic wrapper) -- force reload to pick up latest changes in train.py\n",
    "import importlib, sys, os, glob, json, traceback\n",
    "# Ensure fresh copy of training module so old 'base_estimator' usage is not retained\n",
    "if 'project.train' in sys.modules:\n",
    "    import project.train as _train_mod\n",
    "    importlib.reload(_train_mod)\n",
    "    print('Reloaded project.train from', _train_mod.__file__)\n",
    "else:\n",
    "    import project.train as _train_mod\n",
    "    print('Imported project.train from', _train_mod.__file__)\n",
    "from project.train import train_from_labels  # now reflects reloaded code\n",
    "\n",
    "# Sanity: show _fit_model source first line to confirm it uses \"estimator=\"\n",
    "import inspect\n",
    "_fit_src = inspect.getsource(_train_mod._fit_model).splitlines()[0]\n",
    "print('First line of _fit_model():', _fit_src)\n",
    "\n",
    "PROFILE = True  # Toggle: set to False for a normal (quieter) run\n",
    "\n",
    "if LABELS_CSV is None:\n",
    "    raise RuntimeError('LABELS_CSV is not set; cannot proceed with training.')\n",
    "try:\n",
    "    metrics = train_from_labels(\n",
    "        GCP_PROJECT_ID,\n",
    "        LABELS_CSV,\n",
    "        OUTPUT_MODELS_DIR,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        use_cache_first=True,\n",
    "        profile=PROFILE,\n",
    "    )\n",
    "    metrics_path = os.path.join(OUTPUT_MODELS_DIR, 'metrics.json')\n",
    "    print('Training complete. Metrics summary below (saved to', metrics_path, '):')\n",
    "    metrics\n",
    "except Exception as e:\n",
    "    print('Training failed:', type(e).__name__, str(e))\n",
    "    print('\\n=== TRACEBACK ===')\n",
    "    traceback.print_exc()\n",
    "    print('\\n=== ENV / PATH DIAGNOSTICS ===')\n",
    "    print('Working directory:', os.getcwd())\n",
    "    print('Existing cohort paths:')\n",
    "    for p in ['data/initial_cohort.csv','project/data/initial_cohort.csv','../project/data/initial_cohort.csv']:\n",
    "        print(' -', p, 'FOUND' if os.path.exists(p) else 'missing')\n",
    "    print('\\nLabel file:', LABELS_CSV, 'FOUND' if os.path.exists(LABELS_CSV) else 'missing')\n",
    "    print('\\nModels dir (after failure) contents:')\n",
    "    if os.path.exists(OUTPUT_MODELS_DIR):\n",
    "        for fp in glob.glob(os.path.join(OUTPUT_MODELS_DIR,'*')):\n",
    "            print(' -', os.path.basename(fp))\n",
    "    else:\n",
    "        print(' (models dir not created)')\n",
    "    print('\\nPython version:', sys.version)\n",
    "    print('sys.path[0:5]=', sys.path[:5])\n",
    "    # Show current CalibratedClassifierCV signature to aid debugging\n",
    "    import inspect as _insp\n",
    "    from sklearn.calibration import CalibratedClassifierCV as _CC\n",
    "    print('CalibratedClassifierCV signature:', _insp.signature(_CC))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9df2c664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts root contents:\n",
      "20250905_113628/\n",
      "  labels.csv\n",
      "  data/\n",
      "    cohort_snapshot.csv\n",
      "    labels.csv\n",
      "  models/\n",
      "    feature_columns.json\n",
      "    feature_importance_mortality.csv\n",
      "    feature_importance_prolonged_los.csv\n",
      "    feature_importance_readmission.csv\n",
      "    metrics.json\n",
      "    model_mortality.joblib\n",
      "    model_prolonged_los.joblib\n",
      "    model_readmission.joblib\n",
      "    mortality_calibration.png\n",
      "    mortality_curves.json\n",
      "    mortality_pr.png\n",
      "    mortality_roc.png\n",
      "    preprocessor.joblib\n",
      "    prolonged_los_calibration.png\n",
      "    prolonged_los_curves.json\n",
      "    prolonged_los_pr.png\n",
      "    prolonged_los_roc.png\n",
      "    readmission_calibration.png\n",
      "    readmission_curves.json\n",
      "    readmission_pr.png\n",
      "    readmission_roc.png\n",
      "\n",
      "metrics.json:\n",
      "{\n",
      "  \"mortality\": {\n",
      "    \"n\": 5695,\n",
      "    \"positives\": 622,\n",
      "    \"prevalence\": 0.10921861281826163,\n",
      "    \"roc_auc\": 0.8260170006648907,\n",
      "    \"pr_auc\": 0.38438358294712693,\n",
      "    \"brier\": 0.08057889225707435,\n",
      "    \"ece\": 0.006782419778244072,\n",
      "    \"threshold\": 0.22849839012180953,\n",
      "    \"precision\": 0.3634336677814939,\n",
      "    \"recall\": 0.5241157556270096,\n",
      "    \"f1\": 0.4292297564186965,\n",
      "    \"specificity\": 0.8874433274196728,\n",
      "    \"accuracy\": 0.8477611940298507,\n",
      "    \"cm\": {\n",
      "      \"tp\": 326,\n",
      "      \"tn\": 4502,\n",
      "      \"fp\": 571,\n",
      "      \"fn\": 296\n",
      "    },\n",
      "    \"calibration_bins\": {\n",
      "      \"bin\": [\n",
      "        \"(0.00476, 0.00998]\",\n",
      "        \"(0.00998, 0.0126]\",\n",
      "        \"(0.0126, 0.0225]\",\n",
      "        \"(0.0225, 0.0373]\",\n",
      "        \"(0.0373, 0.0586]\",\n",
      "        \"(0.0586, 0.0891]\",\n",
      "        \"(0.0891, 0.131]\",\n",
      "        \"(0.131, 0.187]\",\n",
      "        \"(0.187, 0.297]\",\n",
      "        \"(0.297, 0.72]\"\n",
      "      ],\n",
      "      \"count\": [\n",
      "        596,\n",
      "        557,\n",
      "        556,\n",
      "        570,\n",
      "        572,\n",
      "        566,\n",
      "        569,\n",
      "        570,\n",
      "        569,\n",
      "        570\n",
      "      ],\n",
      "      \"mean_proba\": [\n",
      "        0.008839087292667861,\n",
      "        0.010722977566107563,\n",
      "        0.016298837691210294,\n",
      "        0.029257552239295385,\n",
      "        0.04702898463005374,\n",
      "        0.07175772529981383,\n",
      "        0.10890487674866789,\n",
      "        0.15696739116120467,\n",
      "        0.23834313020970332,\n",
      "        0.4209828068061293\n",
      "      ],\n",
      "      \"empirical_rate\": [\n",
      "        0.008389261744966443,\n",
      "        0.003590664272890485,\n",
      "        0.008992805755395683,\n",
      "        0.02631578947368421,\n",
      "        0.06293706293706294,\n",
      "        0.07243816254416961,\n",
      "        0.11247803163444639,\n",
      "        0.14035087719298245,\n",
      "        0.23022847100175747,\n",
      "        0.4263157894736842\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"prolonged_los\": {\n",
      "    \"n\": 5695,\n",
      "    \"positives\": 2976,\n",
      "    \"prevalence\": 0.5225636523266023,\n",
      "    \"roc_auc\": 0.7951995886177319,\n",
      "    \"pr_auc\": 0.7896459134913858,\n",
      "    \"brier\": 0.18377743719083622,\n",
      "    \"ece\": 0.01508101757515366,\n",
      "    \"threshold\": 0.3667808066841511,\n",
      "    \"precision\": 0.6702321941038352,\n",
      "    \"recall\": 0.863239247311828,\n",
      "    \"f1\": 0.75458951387869,\n",
      "    \"specificity\": 0.5351232070614197,\n",
      "    \"accuracy\": 0.7065847234416155,\n",
      "    \"cm\": {\n",
      "      \"tp\": 2569,\n",
      "      \"tn\": 1455,\n",
      "      \"fp\": 1264,\n",
      "      \"fn\": 407\n",
      "    },\n",
      "    \"calibration_bins\": {\n",
      "      \"bin\": [\n",
      "        \"(0.0277, 0.18]\",\n",
      "        \"(0.18, 0.284]\",\n",
      "        \"(0.284, 0.347]\",\n",
      "        \"(0.347, 0.438]\",\n",
      "        \"(0.438, 0.527]\",\n",
      "        \"(0.527, 0.615]\",\n",
      "        \"(0.615, 0.711]\",\n",
      "        \"(0.711, 0.819]\",\n",
      "        \"(0.819, 0.856]\",\n",
      "        \"(0.856, 0.922]\"\n",
      "      ],\n",
      "      \"count\": [\n",
      "        570,\n",
      "        570,\n",
      "        584,\n",
      "        554,\n",
      "        578,\n",
      "        561,\n",
      "        571,\n",
      "        574,\n",
      "        570,\n",
      "        563\n",
      "      ],\n",
      "      \"mean_proba\": [\n",
      "        0.11688271144228782,\n",
      "        0.2288100081958119,\n",
      "        0.31691674198753694,\n",
      "        0.393137830260799,\n",
      "        0.4822146591160266,\n",
      "        0.5729920432487348,\n",
      "        0.6612972689585948,\n",
      "        0.772376762224132,\n",
      "        0.8399721320097767,\n",
      "        0.8635739378349736\n",
      "      ],\n",
      "      \"empirical_rate\": [\n",
      "        0.10175438596491228,\n",
      "        0.24736842105263157,\n",
      "        0.2893835616438356,\n",
      "        0.36101083032490977,\n",
      "        0.48788927335640137,\n",
      "        0.5632798573975044,\n",
      "        0.6690017513134852,\n",
      "        0.8013937282229965,\n",
      "        0.843859649122807,\n",
      "        0.8650088809946714\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"readmission\": {\n",
      "    \"n\": 5695,\n",
      "    \"positives\": 246,\n",
      "    \"prevalence\": 0.04319578577699736,\n",
      "    \"roc_auc\": 0.590764024725951,\n",
      "    \"pr_auc\": 0.05734875552458063,\n",
      "    \"brier\": 0.04122352127296507,\n",
      "    \"ece\": 0.006519636617833986,\n",
      "    \"threshold\": 0.05434221982698793,\n",
      "    \"precision\": 0.06623058053965658,\n",
      "    \"recall\": 0.32926829268292684,\n",
      "    \"f1\": 0.1102791014295439,\n",
      "    \"specificity\": 0.7904202605982749,\n",
      "    \"accuracy\": 0.7705004389815627,\n",
      "    \"cm\": {\n",
      "      \"tp\": 81,\n",
      "      \"tn\": 4307,\n",
      "      \"fp\": 1142,\n",
      "      \"fn\": 165\n",
      "    },\n",
      "    \"calibration_bins\": {\n",
      "      \"bin\": [\n",
      "        \"(-0.001, 0.0203]\",\n",
      "        \"(0.0203, 0.0268]\",\n",
      "        \"(0.0268, 0.0332]\",\n",
      "        \"(0.0332, 0.041]\",\n",
      "        \"(0.041, 0.0453]\",\n",
      "        \"(0.0453, 0.0491]\",\n",
      "        \"(0.0491, 0.0515]\",\n",
      "        \"(0.0515, 0.055]\",\n",
      "        \"(0.055, 0.0623]\",\n",
      "        \"(0.0623, 0.152]\"\n",
      "      ],\n",
      "      \"count\": [\n",
      "        589,\n",
      "        555,\n",
      "        566,\n",
      "        579,\n",
      "        561,\n",
      "        567,\n",
      "        609,\n",
      "        531,\n",
      "        569,\n",
      "        569\n",
      "      ],\n",
      "      \"mean_proba\": [\n",
      "        0.017344892796623327,\n",
      "        0.023602427681660914,\n",
      "        0.02996905870508248,\n",
      "        0.037243131584786496,\n",
      "        0.042941478544379616,\n",
      "        0.04753207520364051,\n",
      "        0.05059089757931934,\n",
      "        0.05328590173361789,\n",
      "        0.058444884146896296,\n",
      "        0.07340058395270177\n",
      "      ],\n",
      "      \"empirical_rate\": [\n",
      "        0.025466893039049237,\n",
      "        0.03063063063063063,\n",
      "        0.022968197879858657,\n",
      "        0.04835924006908463,\n",
      "        0.0374331550802139,\n",
      "        0.03527336860670194,\n",
      "        0.04926108374384237,\n",
      "        0.05649717514124294,\n",
      "        0.050966608084358524,\n",
      "        0.07557117750439367\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Inspect saved artifacts\n",
    "import json, glob\n",
    "print('Artifacts root contents:')\n",
    "for root, dirs, files in os.walk(ARTIFACTS_DIR):\n",
    "    level = root.replace(ARTIFACTS_DIR, '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    print(f\"{indent}{os.path.basename(root) or ARTIFACTS_DIR}/\")\n",
    "    subindent = '  ' * (level + 1)\n",
    "    for f in files:\n",
    "        print(f\"{subindent}{f}\")\n",
    "metrics_path = os.path.join(OUTPUT_MODELS_DIR, 'metrics.json')\n",
    "if os.path.exists(metrics_path):\n",
    "    print('\\nmetrics.json:')\n",
    "    print(open(metrics_path,'r',encoding='utf-8').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: detect accidental nested project/project structure and report\n",
    "nested_issue = False\n",
    "cwd = os.getcwd()\n",
    "# Look for a 'project/project' pattern relative to repo root heuristics\n",
    "suspect_path = os.path.join('project','project')\n",
    "if os.path.isdir(suspect_path):\n",
    "    nested_issue = True\n",
    "    print('\\n[WARNING] Detected nested directory:', suspect_path)\n",
    "    print('This likely came from earlier path logic. Current run artifacts are stored in:', ARTIFACTS_DIR)\n",
    "    print('Recommended manual cleanup (run from repo root):')\n",
    "    print('  - Move any needed runs from project/project/runs/* to project/runs/')\n",
    "    print('  - Then remove the extra nested directory: project/project')\n",
    "else:\n",
    "    print('\\nNo nested project/project directory detected.')\n",
    "\n",
    "{'nested_issue': nested_issue, 'runs_root': os.path.abspath(os.path.join(ARTIFACTS_DIR, '..'))}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
