{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791201e3",
   "metadata": {},
   "source": [
    "# MLHC Training Pipeline\n",
    "\n",
    "> End‑to‑end model development for ICU outcomes (mortality, prolonged length of stay, 30‑day readmission) starting from precomputed first‑48h features (`features_full.parquet`) and labels (`labels.csv`).\n",
    "\n",
    "\n",
    "## Section Map\n",
    "1. Environment Setup – paths, seed, versions.\n",
    "2. Cohort & Admissions – load + strictly align features & labels; variance check.\n",
    "3. (Legacy) Quick Split Preview – initial readmission‑stratified split (diagnostic only).\n",
    "4. Feature / Metric Utilities – helper metric functions (F1 / cost / Fβ).\n",
    "5. Target Specs & Canonical Split – define three targets & build one shared stratified train/val/test split.\n",
    "6. Split Persistence & Baseline Characteristics – persist subject ID lists + wide Table 1.\n",
    "7. Model Training & Tuning – Optuna XGBoost CV, final + base (calibration) models, isotonic calibration, threshold (max F1).\n",
    "8. Metrics Aggregation & Artifact Index – consolidate per‑target metrics & artifact paths.\n",
    "9. ROC & PR Curves (Calibrated) – discrimination visualizations.\n",
    "10. Reliability (Calibration) Curves – probability calibration assessment.\n",
    "11. Manual Calibration Example – illustrative standalone isotonic workflow.\n",
    "12. Metrics Summary & Persistence – tuned vs baseline; full vs calibrated; exports.\n",
    "13. Inference & Enriched Metrics – merged summary for downstream consumption.\n",
    "14. SHAP Feature Attribution – global feature importance from calibrated base models.\n",
    "\n",
    "### Outputs (Key Files in `project/artifacts/`)\n",
    "- `model_{target}.joblib` + `isotonic_{target}.joblib` + `threshold_{target}.txt`\n",
    "- `model_full_{target}.joblib` (train+val retrain)\n",
    "- `metrics_{target}.json`, `metrics_all.json`, `metrics_summary*.csv`\n",
    "- `train/val/test_subject_ids.txt`, `data_split_manifest.json`\n",
    "- `table1_patient_characteristics.csv`, `table1_patient_characteristics_pivoted.csv`\n",
    "- `full_vs_calibrated_comparison.csv`, `auc_diff_significance.csv` (if bootstrap completed)\n",
    "- `shap_feature_importance_{target}.csv`, `shap_values_{target}.npy`, `shap_summary_{target}.png`\n",
    "\n",
    "> Proceed sequentially; rerun Sections 7–15 after modifying features, labels, or hyperparameter search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment & core imports\n",
    "import os, sys, json, random, platform, importlib, datetime, pathlib\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# Add project root to path if notebook is under notebooks/\n",
    "ROOT = pathlib.Path(__file__).resolve().parents[1] if '__file__' in globals() else pathlib.Path.cwd().parents[0]\n",
    "if str(ROOT) not in sys.path: sys.path.insert(0, str(ROOT))\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "PROJECT_ROOT = (Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd())\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'project/artifacts'\n",
    "RUNS_ROOT = PROJECT_ROOT / 'runs'\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data dir exists: {(DATA_DIR).exists()}\")\n",
    "VERSIONS = {'python': sys.version.split()[0], 'platform': platform.platform()}\n",
    "for pkg in ['xgboost','optuna','shap','sklearn','pandas','numpy']:\n",
    "    try:\n",
    "        m = importlib.import_module(pkg if pkg != 'sklearn' else 'sklearn')\n",
    "        VERSIONS[pkg] = getattr(m,'__version__','?')\n",
    "    except Exception as e:\n",
    "        VERSIONS[pkg] = f'NA({e})'\n",
    "print('Versions:', json.dumps(VERSIONS, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560f373",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "Configure paths, deterministic seed, and collect package versions for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c3dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load canonical labels produced by duckdb_extraction notebook\n",
    "import pandas as pd, random\n",
    "from pathlib import Path\n",
    "\n",
    "# Force single authoritative path (output of duckdb_extraction)\n",
    "LABELS_PATH = (PROJECT_ROOT / 'project' / 'artifacts' / 'labels.csv')\n",
    "if not LABELS_PATH.exists():\n",
    "    raise FileNotFoundError(f'Expected labels at {LABELS_PATH}. Run duckdb_extraction notebook first to generate labels.csv there.')\n",
    "\n",
    "labels_df = pd.read_csv(LABELS_PATH)\n",
    "# Normalize required columns\n",
    "required_cols = {'subject_id','hadm_id','readmission_label','mortality_label','prolonged_los_label'}\n",
    "missing = required_cols - set(labels_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f'Missing expected label columns in {LABELS_PATH}: {missing}')\n",
    "\n",
    "labels_df = labels_df.drop_duplicates('subject_id')\n",
    "for col in ['readmission_label','mortality_label','prolonged_los_label']:\n",
    "    labels_df[col] = labels_df[col].astype(int)\n",
    "assert labels_df['subject_id'].isna().sum()==0\n",
    "prev_readmit = labels_df['readmission_label'].mean()\n",
    "print(f'Labels source: {LABELS_PATH} | shape={labels_df.shape} | readmission_prevalence={prev_readmit:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4f32a",
   "metadata": {},
   "source": [
    "## 2. Cohort & Admissions\n",
    "Load pre‑built feature matrix; enforce index uniqueness and full subject overlap with labels (hard fail if any mismatch). Compute variance preservation check after realignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature matrix (regenerate if tiny/corrupt) + strict safe alignment\n",
    "import pandas as pd, numpy as np, json, os\n",
    "from pathlib import Path\n",
    "\n",
    "feature_path = ARTIFACTS_DIR / 'features_full.parquet'\n",
    "if not feature_path.exists():\n",
    "    raise FileNotFoundError(f'Missing feature parquet: {feature_path}')\n",
    "\n",
    "feature_df = pd.read_parquet(feature_path)\n",
    "regenerated = False  # placeholder if regeneration logic later added\n",
    "\n",
    "# Ensure subject_id index\n",
    "if 'subject_id' in feature_df.columns:\n",
    "    if feature_df['subject_id'].duplicated().any():\n",
    "        raise RuntimeError('Duplicate subject_id rows in feature parquet.')\n",
    "    feature_df = feature_df.set_index('subject_id')\n",
    "\n",
    "# Canonical integer index (fix KeyError mismatch from mixed string/int earlier)\n",
    "try:\n",
    "    feature_df.index = feature_df.index.astype(int)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'Failed to coerce feature index to int: {e}')\n",
    "if feature_df.index.has_duplicates:\n",
    "    raise RuntimeError('Feature index has duplicates after int coercion.')\n",
    "\n",
    "# Labels already loaded upstream into labels_df\n",
    "label_subjects = labels_df['subject_id'].astype(int)\n",
    "\n",
    "feat_ids = feature_df.index\n",
    "missing_in_features = set(label_subjects) - set(feat_ids)\n",
    "extra_in_features = set(feat_ids) - set(label_subjects)\n",
    "overlap_frac = 1 - (len(missing_in_features)/len(label_subjects)) if len(label_subjects) else 0\n",
    "print(f\"[cohort-diff] label_subjects={len(label_subjects)} feature_subjects={len(feat_ids)}\")\n",
    "print(f\"[cohort-diff] missing_in_features={len(missing_in_features)} extra_in_features={len(extra_in_features)}\")\n",
    "print(f\"[align] overlap_frac={overlap_frac:.4f}\")\n",
    "\n",
    "if missing_in_features:\n",
    "    raise RuntimeError(f'Missing {len(missing_in_features)} subjects (sample: {list(missing_in_features)[:10]})')\n",
    "\n",
    "# Pre-alignment variance snapshot (for collapse detection)\n",
    "pre_var = feature_df.var(numeric_only=True, ddof=0)\n",
    "\n",
    "# Strict ordering alignment\n",
    "feature_df_aligned = feature_df.loc[label_subjects.values]\n",
    "\n",
    "post_var = feature_df_aligned.var(numeric_only=True, ddof=0)\n",
    "collapsed = [c for c in pre_var.index if pre_var[c] > 0 and post_var[c] == 0]\n",
    "if collapsed:\n",
    "    print(f'WARNING: {len(collapsed)} columns lost variance post-alignment (e.g. {collapsed[:5]}).')\n",
    "else:\n",
    "    print('[align] Variance preserved.')\n",
    "\n",
    "print('Features aligned shape:', feature_df_aligned.shape, '| regenerated' if regenerated else '')\n",
    "\n",
    "# Expose for downstream cells\n",
    "features_loaded = feature_df_aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa08550",
   "metadata": {},
   "source": [
    "## 3. Quick Split Preview\n",
    "Perform an initial stratified split on readmission only (legacy diagnostic). Canonical multi‑target split is created later; this section is retained for prevalence sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/valid/test split (60/20/20) + class weight factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "readmit_y = labels_df['readmission_label'].astype(int).to_numpy()\n",
    "subject_index = feature_df.index.to_numpy()\n",
    "X = feature_df.values\n",
    "X_tr, X_temp, y_tr, y_temp, sid_tr, sid_temp = train_test_split(\n",
    "    X, readmit_y, subject_index, test_size=0.4, stratify=readmit_y, random_state=SEED)\n",
    "X_val, X_te, y_val, y_te, sid_val, sid_te = train_test_split(\n",
    "    X_temp, y_temp, sid_temp, test_size=0.5, stratify=y_temp, random_state=SEED)\n",
    "pos_rate = y_tr.mean(); scale_pos_weight = (1-pos_rate)/max(pos_rate,1e-6)\n",
    "print(f'Split -> train {X_tr.shape} valid {X_val.shape} test {X_te.shape} | pos_rate_train={pos_rate:.4f} | spw≈{scale_pos_weight:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281cc739",
   "metadata": {},
   "source": [
    "## 4. Feature / Metric Utilities\n",
    "Utility functions (e.g., cost‑aware + F/FBeta metrics) used later for threshold selection and diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb41202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "C_FP = 1.0; C_FN = 5.0\n",
    "beta = 2.0\n",
    "\n",
    "def metrics_at(proba, y, thr):\n",
    "    pred = (proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    cost = C_FP*fp + C_FN*fn\n",
    "    f1 = f1_score(y, pred)\n",
    "    prec = tp/(tp+fp+1e-9); rec = tp/(tp+fn+1e-9)\n",
    "    fbeta = (1+beta**2)*prec*rec/(beta**2*prec+rec+1e-9)\n",
    "    return dict(f1=f1, precision=prec, recall=rec, cost=cost, fbeta=fbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739ba7d",
   "metadata": {},
   "source": [
    "## 5. Target Specs & Canonical Split\n",
    "Define the three binary targets and create a single shared train/val/test split (stratified on the first target) reused for all models to ensure comparable evaluation and fairness alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea02b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 42\n",
    "rng_global = np.random.default_rng(SEED)\n",
    "\n",
    "TARGET_SPECS = [\n",
    "    ('readmission','readmission_label'),\n",
    "    ('mortality','mortality_label'),\n",
    "    ('prolonged_los','prolonged_los_label'),\n",
    "]\n",
    "\n",
    "\n",
    "# Verify presence of target columns\n",
    "missing_targets = [lbl for _,lbl in TARGET_SPECS if lbl not in labels_df.columns]\n",
    "if missing_targets:\n",
    "    raise ValueError(f\"Missing target label columns: {missing_targets}\")\n",
    "\n",
    "# Shared subject index + base split for comparability\n",
    "subject_index = feature_df.index.to_numpy()\n",
    "X = feature_df.values\n",
    "primary_y = labels_df[TARGET_SPECS[0][1]].astype(int).to_numpy()  # stratify on first target\n",
    "X_tr, X_temp, y_tr_primary, y_temp_primary, sid_tr, sid_temp = train_test_split(X, primary_y, subject_index, test_size=0.4, stratify=primary_y, random_state=SEED)\n",
    "X_val, X_te, y_val_primary, y_te_primary, sid_val, sid_te = train_test_split(X_temp, y_temp_primary, sid_temp, test_size=0.5, stratify=y_temp_primary, random_state=SEED)\n",
    "print(f'Shared split shapes -> train {X_tr.shape} val {X_val.shape} test {X_te.shape}')\n",
    "\n",
    "subj_to_pos = {sid:i for i,sid in enumerate(subject_index)}\n",
    "idx_tr = np.array([subj_to_pos[s] for s in sid_tr])\n",
    "idx_val = np.array([subj_to_pos[s] for s in sid_val])\n",
    "idx_te = np.array([subj_to_pos[s] for s in sid_te])\n",
    "metrics_all = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist authoritative split subject IDs for downstream fairness / reproducibility\n",
    "import json, time\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "split_manifest = {\n",
    "    'created_utc': time.time(),\n",
    "    'seed': SEED,\n",
    "    'counts': {\n",
    "        'train': int(len(sid_tr)),\n",
    "        'val': int(len(sid_val)),\n",
    "        'test': int(len(sid_te)),\n",
    "        'total': int(len(subject_index)),\n",
    "    },\n",
    "    'subject_ids': {\n",
    "        'train': [int(x) for x in sid_tr.tolist()],\n",
    "        'val': [int(x) for x in sid_val.tolist()],\n",
    "        'test': [int(x) for x in sid_te.tolist()],\n",
    "    },\n",
    "}\n",
    "# Lightweight plain‑text convenience files (one id per line)\n",
    "with open(ARTIFACTS_DIR / 'train_subject_ids.txt','w') as f:\n",
    "    f.write('\\n'.join(str(int(x)) for x in sid_tr))\n",
    "with open(ARTIFACTS_DIR / 'val_subject_ids.txt','w') as f:\n",
    "    f.write('\\n'.join(str(int(x)) for x in sid_val))\n",
    "with open(ARTIFACTS_DIR / 'test_subject_ids.txt','w') as f:\n",
    "    f.write('\\n'.join(str(int(x)) for x in sid_te))\n",
    "with open(ARTIFACTS_DIR / 'data_split_manifest.json','w') as f:\n",
    "    json.dump(split_manifest, f, indent=2)\n",
    "print('Persisted split subject id artifacts ->', (ARTIFACTS_DIR / 'data_split_manifest.json').resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce642105",
   "metadata": {},
   "source": [
    "## 6. Split Persistence & Baseline Characteristics\n",
    "Persist split subject IDs (JSON + txt) for downstream reproducibility and fairness. Then generate Table 1 (wide + pivot) summarizing demographics, outcomes, and key physiologic variables across splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a647fd0",
   "metadata": {},
   "source": [
    "#### 6b. Pivoted Enriched Table 1\n",
    "The pivoted version orients characteristics as rows and splits (Train / Val / Test) as columns. Added fields: weight, height, ethnicity distribution (%), length of stay (LOS) summary, and additional vitals/labs if present. Continuous variables are reported as `mean (SD)`. Categorical distributions are reported as `% of split`. Missing variables are skipped automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c9b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-target baseline (logistic) for reference per target\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "baseline_results = {}\n",
    "X_base = X_tr  # using training split from shared split\n",
    "X_val_base = X_val\n",
    "\n",
    "for tgt_name, label_col in TARGET_SPECS:\n",
    "    y_tr_t = labels_df[label_col].astype(int).to_numpy()[idx_tr]\n",
    "    y_val_t = labels_df[label_col].astype(int).to_numpy()[idx_val]\n",
    "    pipe = Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy='median')),\n",
    "        (\"sc\", StandardScaler(with_mean=False)),\n",
    "        (\"lr\", LogisticRegression(max_iter=500, class_weight='balanced', solver='liblinear'))\n",
    "    ])\n",
    "    pipe.fit(X_base, y_tr_t)\n",
    "    val_proba = pipe.predict_proba(X_val_base)[:,1]\n",
    "    auc_val = roc_auc_score(y_val_t, val_proba)\n",
    "    baseline_results[tgt_name] = float(auc_val)\n",
    "print('Baseline logistic validation AUCs per target:', baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49c3de",
   "metadata": {},
   "source": [
    "## 7. Model Training & Tuning\n",
    "Hyperparameter optimization (Optuna) + final & calibration model training with isotonic calibration and F1‑optimal threshold selection on validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2550bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-target tuning & training pipeline (updated to persist final_model + artifact paths)\n",
    "import optuna, numpy as np, json, joblib\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Hyperparameter search configuration\n",
    "N_TRIALS_PER_TARGET = 1  # increase for better tuning\n",
    "N_FOLDS = 5\n",
    "MAX_ROUNDS = 400\n",
    "SPEED_SAMPLE_MAX = 12000\n",
    "\n",
    "def make_objective(X_train_full, y_train_full):\n",
    "    def objective(trial: optuna.Trial):\n",
    "        params = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "            'min_child_weight': trial.suggest_float('min_child_weight', 1.0, 8.0),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_lambda': trial.suggest_float('lambda', 1e-3, 5.0, log=True),\n",
    "            'reg_alpha': trial.suggest_float('alpha', 1e-3, 2.0, log=True),\n",
    "            'gamma': trial.suggest_float('gamma', 0.0, 4.0),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 120, MAX_ROUNDS),\n",
    "        }\n",
    "        fold_aucs = []\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "        for fold,(tr_idx_local, va_idx_local) in enumerate(skf.split(X_train_full, y_train_full), 1):\n",
    "            Xtr_f, Xva_f = X_train_full[tr_idx_local], X_train_full[va_idx_local]\n",
    "            ytr_f, yva_f = y_train_full[tr_idx_local], y_train_full[va_idx_local]\n",
    "            if Xtr_f.shape[0] > SPEED_SAMPLE_MAX:\n",
    "                pos_idx = np.where(ytr_f==1)[0]\n",
    "                neg_idx = np.where(ytr_f==0)[0]\n",
    "                keep_pos = pos_idx\n",
    "                remaining = SPEED_SAMPLE_MAX - len(keep_pos)\n",
    "                if remaining < len(neg_idx):\n",
    "                    keep_neg = np.random.default_rng(SEED+fold).choice(neg_idx, size=remaining, replace=False)\n",
    "                else:\n",
    "                    keep_neg = neg_idx\n",
    "                keep = np.concatenate([keep_pos, keep_neg])\n",
    "                np.random.default_rng(SEED+fold).shuffle(keep)\n",
    "                Xtr_f = Xtr_f[keep]; ytr_f = ytr_f[keep]\n",
    "            pos_rate_fold = ytr_f.mean(); spw = (1-pos_rate_fold)/max(pos_rate_fold,1e-6)\n",
    "            model = XGBClassifier(objective='binary:logistic', tree_method='hist', scale_pos_weight=spw, eval_metric='auc', verbosity=0, **params)\n",
    "            model.fit(Xtr_f, ytr_f, verbose=False)\n",
    "            proba = model.predict_proba(Xva_f)[:,1]\n",
    "            fold_auc = roc_auc_score(yva_f, proba)\n",
    "            fold_aucs.append(fold_auc)\n",
    "        mean_auc = float(np.mean(fold_aucs))\n",
    "        trial.set_user_attr('fold_aucs', fold_aucs)\n",
    "        trial.set_user_attr('cv_mean_auc', mean_auc)\n",
    "        return mean_auc\n",
    "    return objective\n",
    "\n",
    "for tgt_name, label_col in TARGET_SPECS:\n",
    "    print('\\n==== Target:', tgt_name, '('+label_col+') ====')\n",
    "    y_all = labels_df[label_col].astype(int).to_numpy()\n",
    "    y_tr_t = y_all[idx_tr]; y_val_t = y_all[idx_val]; y_te_t = y_all[idx_te]\n",
    "    pos_rate = y_tr_t.mean(); scale_pos_weight = (1-pos_rate)/max(pos_rate,1e-6)\n",
    "    print(f'Pos rate train: {pos_rate:.4f} -> spw {scale_pos_weight:.2f}')\n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "    objective = make_objective(X_tr, y_tr_t)\n",
    "    study.optimize(objective, n_trials=N_TRIALS_PER_TARGET, show_progress_bar=False)\n",
    "    best_params = study.best_params.copy()\n",
    "    print('Best CV mean AUC:', round(study.best_value,4))\n",
    "    print('Best Params:', best_params)\n",
    "    # Final model retrained on train + validation (for potential deployment / ensembling)\n",
    "    X_tr_full_t = np.vstack([X_tr, X_val])\n",
    "    y_tr_full_t = np.concatenate([y_tr_t, y_val_t])\n",
    "    final_model = XGBClassifier(objective='binary:logistic', tree_method='hist', learning_rate=best_params['learning_rate'], n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], min_child_weight=best_params['min_child_weight'], subsample=best_params['subsample'], colsample_bytree=best_params['colsample_bytree'], reg_lambda=best_params['lambda'], reg_alpha=best_params['alpha'], gamma=best_params['gamma'], scale_pos_weight=scale_pos_weight, eval_metric='auc', verbosity=0)\n",
    "    final_model.fit(X_tr_full_t, y_tr_full_t)\n",
    "    # Calibration base model (train-only) to avoid leaking validation into both model weights and calibrator\n",
    "    base_model = XGBClassifier(objective='binary:logistic', tree_method='hist', learning_rate=best_params['learning_rate'], n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], min_child_weight=best_params['min_child_weight'], subsample=best_params['subsample'], colsample_bytree=best_params['colsample_bytree'], reg_lambda=best_params['lambda'], reg_alpha=best_params['alpha'], gamma=best_params['gamma'], scale_pos_weight=scale_pos_weight, eval_metric='logloss', verbosity=0)\n",
    "    base_model.fit(X_tr, y_tr_t)\n",
    "    val_raw = base_model.predict_proba(X_val)[:,1]\n",
    "    iso = IsotonicRegression(out_of_bounds='clip'); iso.fit(val_raw, y_val_t)\n",
    "    val_cal = iso.transform(val_raw)\n",
    "    # Threshold selection on calibrated validation probabilities (maximize F1)\n",
    "    ths = np.linspace(0.01,0.9,300)\n",
    "    best_thr = None; best_f1 = -1; best_val_metrics = None\n",
    "    def _metrics_at(proba, y, thr):\n",
    "        pred = (proba >= thr).astype(int)\n",
    "        tp = ((pred==1)&(y==1)).sum(); fp = ((pred==1)&(y==0)).sum(); fn = ((pred==0)&(y==1)).sum()\n",
    "        prec = tp/(tp+fp+1e-9); rec = tp/(tp+fn+1e-9)\n",
    "        f1 = 2*prec*rec/(prec+rec+1e-9)\n",
    "        return dict(precision=float(prec), recall=float(rec), f1=float(f1))\n",
    "    for t in ths:\n",
    "        m = _metrics_at(val_cal, y_val_t, t)\n",
    "        if m['f1'] > best_f1:\n",
    "            best_f1 = m['f1']; best_thr = float(t); best_val_metrics = m\n",
    "    print('Selected threshold (validation calibrated):', best_thr, best_val_metrics)\n",
    "    # Test evaluation using calibrated base model\n",
    "    test_cal = iso.transform(base_model.predict_proba(X_te)[:,1])\n",
    "    auc = roc_auc_score(y_te_t, test_cal)\n",
    "    pr_auc = average_precision_score(y_te_t, test_cal)\n",
    "    brier = brier_score_loss(y_te_t, test_cal)\n",
    "    test_pred = (test_cal >= best_thr).astype(int)\n",
    "    tp = ((test_pred==1)&(y_te_t==1)).sum(); fp = ((test_pred==1)&(y_te_t==0)).sum(); fn = ((test_pred==0)&(y_te_t==1)).sum()\n",
    "    prec_test = tp/(tp+fp+1e-9); rec_test = tp/(tp+fn+1e-9)\n",
    "    f1_test = 2*prec_test*rec_test/(prec_test+rec_test+1e-9)\n",
    "    metrics = {'auc': float(auc), 'pr_auc': float(pr_auc), 'brier': float(brier), 'threshold': best_thr, 'f1_at_threshold': float(f1_test), 'precision_at_threshold': float(prec_test), 'recall_at_threshold': float(rec_test), 'validation_threshold_info': {**best_val_metrics, 'threshold': best_thr}, 'optuna_best_value_cv_mean_auc': float(study.best_value), 'optuna_best_params': best_params, 'train_rows': int(X_tr.shape[0]), 'val_rows': int(X_val.shape[0]), 'test_rows': int(X_te.shape[0])}\n",
    "    metrics_all[tgt_name] = metrics\n",
    "    print('Test metrics:', json.dumps(metrics, indent=2))\n",
    "    # Persist artifacts (both final_model and calibration pipeline components)\n",
    "    ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "    final_path = ARTIFACTS_DIR / f'model_full_{tgt_name}.joblib'  # trained on train+val\n",
    "    base_path = ARTIFACTS_DIR / f'model_{tgt_name}.joblib'        # train-only (paired with isotonic)\n",
    "    iso_path = ARTIFACTS_DIR / f'isotonic_{tgt_name}.joblib'\n",
    "    joblib.dump(final_model, final_path)\n",
    "    joblib.dump(base_model, base_path)\n",
    "    joblib.dump(iso, iso_path)\n",
    "    with open(ARTIFACTS_DIR / f'metrics_{tgt_name}.json','w') as f: json.dump(metrics, f, indent=2)\n",
    "    with open(ARTIFACTS_DIR / f'threshold_{tgt_name}.txt','w') as f: f.write(str(best_thr))\n",
    "    with open(ARTIFACTS_DIR / f'best_params_{tgt_name}.json','w') as f: json.dump(best_params, f, indent=2)\n",
    "    print(f'Artifact paths ({tgt_name}):')\n",
    "    for p in [final_path, base_path, iso_path, ARTIFACTS_DIR / f'metrics_{tgt_name}.json', ARTIFACTS_DIR / f'threshold_{tgt_name}.txt', ARTIFACTS_DIR / f'best_params_{tgt_name}.json']:\n",
    "        print('  -', p.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c0043b",
   "metadata": {},
   "source": [
    "## 8. Metrics Aggregation & Artifact Index\n",
    "Persist per‑target metrics + consolidated index of models, calibrators, thresholds, and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9caa9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "# Combined metrics + artifact index\n",
    "with open(ARTIFACTS_DIR / 'metrics_all.json','w') as f: json.dump(metrics_all, f, indent=2)\n",
    "artifact_index = {}\n",
    "for tgt_name,_ in TARGET_SPECS:\n",
    "    artifact_index[tgt_name] = {\n",
    "        'final_model': str((ARTIFACTS_DIR / f'model_full_{tgt_name}.joblib').resolve()),\n",
    "        'base_model': str((ARTIFACTS_DIR / f'model_{tgt_name}.joblib').resolve()),\n",
    "        'isotonic': str((ARTIFACTS_DIR / f'isotonic_{tgt_name}.joblib').resolve()),\n",
    "        'metrics': str((ARTIFACTS_DIR / f'metrics_{tgt_name}.json').resolve()),\n",
    "        'threshold': str((ARTIFACTS_DIR / f'threshold_{tgt_name}.txt').resolve()),\n",
    "        'best_params': str((ARTIFACTS_DIR / f'best_params_{tgt_name}.json').resolve())\n",
    "    }\n",
    "with open(ARTIFACTS_DIR / 'artifact_index.json','w') as f: json.dump(artifact_index, f, indent=2)\n",
    "print('All target metrics written ->', (ARTIFACTS_DIR / 'metrics_all.json').resolve())\n",
    "print('Artifact index written ->', (ARTIFACTS_DIR / 'artifact_index.json').resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d964627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize saved artifacts for all targets\n",
    "import json, os\n",
    "artifact_summary = {}\n",
    "for tgt_name, _ in TARGET_SPECS:\n",
    "    tgt_files = [f for f in os.listdir(ARTIFACTS_DIR) if f.startswith(('model_'+tgt_name,'isotonic_'+tgt_name,'metrics_'+tgt_name,'threshold_'+tgt_name,'best_params_'+tgt_name))]\n",
    "    artifact_summary[tgt_name] = tgt_files\n",
    "print(json.dumps(artifact_summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5bedf",
   "metadata": {},
   "source": [
    "## 9. ROC & PR Curves (Calibrated)\n",
    "Visual diagnostics for calibrated probability discrimination across targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16676a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC & PR curves (calibrated) for all targets\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc as sk_auc\n",
    "from joblib import load as jobload\n",
    "\n",
    "roc_data = {}\n",
    "pr_data = {}\n",
    "for tgt_name, label_col in TARGET_SPECS:\n",
    "    model_path = ARTIFACTS_DIR / f'model_{tgt_name}.joblib'\n",
    "    iso_path = ARTIFACTS_DIR / f'isotonic_{tgt_name}.joblib'\n",
    "    if not (model_path.exists() and iso_path.exists()):\n",
    "        print('Missing artifacts for', tgt_name)\n",
    "        continue\n",
    "    model = jobload(model_path)\n",
    "    iso = jobload(iso_path)\n",
    "    y_all = labels_df[label_col].astype(int).to_numpy()\n",
    "    y_te_t = y_all[idx_te]\n",
    "    raw = model.predict_proba(X_te)[:,1]\n",
    "    cal = iso.transform(raw)\n",
    "    fpr,tpr,_ = roc_curve(y_te_t, cal)\n",
    "    prec,rec,_ = precision_recall_curve(y_te_t, cal)\n",
    "    roc_auc = sk_auc(fpr,tpr)\n",
    "    pr_auc = sk_auc(rec,prec)\n",
    "    roc_data[tgt_name] = (fpr,tpr,roc_auc)\n",
    "    pr_data[tgt_name] = (rec,prec,pr_auc)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "for tgt,(fpr,tpr,roc_auc) in roc_data.items():\n",
    "    axes[0].plot(fpr,tpr,label=f\"{tgt} AUC={roc_auc:.3f}\")\n",
    "axes[0].plot([0,1],[0,1],'--',color='grey'); axes[0].set_title('ROC (Calibrated)'); axes[0].legend(); axes[0].set_xlabel('FPR'); axes[0].set_ylabel('TPR')\n",
    "for tgt,(rec,prec,pr_auc) in pr_data.items():\n",
    "    axes[1].plot(rec,prec,label=f\"{tgt} PR AUC={pr_auc:.3f}\")\n",
    "axes[1].set_title('PR (Calibrated)'); axes[1].legend(); axes[1].set_xlabel('Recall'); axes[1].set_ylabel('Precision')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b693a61",
   "metadata": {},
   "source": [
    "## 10. Reliability (Calibration) Curves\n",
    "Assess probability calibration pre/post isotonic across test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7400eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curves for all targets (using shared_inference unified loading)\n",
    "from project.inference import get_model_and_calibrator, apply_calibration, _resolve_models_dir\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "\n",
    "ARTIFACTS_DIR = _resolve_models_dir()\n",
    "\n",
    "calibration_targets = []  # (tgt_name, y_true_test, raw_probs, calibrated_probs)\n",
    "for tgt_name, label_col in TARGET_SPECS:\n",
    "    try:\n",
    "        model, calibrator = get_model_and_calibrator(tgt_name, models_dir=ARTIFACTS_DIR)\n",
    "    except FileNotFoundError:\n",
    "        print('Skip (missing artifacts):', tgt_name)\n",
    "        continue\n",
    "    y_all = labels_df[label_col].astype(int).to_numpy(); y_te_t = y_all[idx_te]\n",
    "    raw = model.predict_proba(X_te)[:,1]\n",
    "    cal = apply_calibration(raw, calibrator)\n",
    "    calibration_targets.append((tgt_name, y_te_t, raw, cal))\n",
    "\n",
    "if not calibration_targets:\n",
    "    raise RuntimeError('No targets available for calibration plotting.')\n",
    "\n",
    "fig, axes = plt.subplots(1, len(calibration_targets), figsize=(5*len(calibration_targets),4), sharey=True)\n",
    "if len(calibration_targets) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax,(tgt,y_true,raw,cal) in zip(axes, calibration_targets):\n",
    "    fr_raw, mp_raw = calibration_curve(y_true, raw, n_bins=15, strategy='quantile')\n",
    "    fr_cal, mp_cal = calibration_curve(y_true, cal, n_bins=15, strategy='quantile')\n",
    "    ax.plot(mp_raw, fr_raw, 'o-', label='Raw', alpha=0.7)\n",
    "    ax.plot(mp_cal, fr_cal, 'o-', label='Calibrated', alpha=0.7)\n",
    "    ax.plot([0,1],[0,1], '--', color='gray')\n",
    "    ax.set_title(f'Calibration: {tgt}')\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('Observed')\n",
    "    ax.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Report average absolute calibration shift per target\n",
    "for tgt, y_true, raw, cal in calibration_targets:\n",
    "    shift = float(np.mean(np.abs(raw - cal)))\n",
    "    print(f\"{tgt}: mean |raw-cal| = {shift:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432add8f",
   "metadata": {},
   "source": [
    "## 11. (Legacy) Manual Calibration Example\n",
    "Illustrative manual isotonic calibration + threshold derivation (kept for reference; main pipeline already calibrates per‑target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ab5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual isotonic calibration (sklearn XGBClassifier base, no DMatrix)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import numpy as np\n",
    "\n",
    "# Base model: train-only (exclude validation for calibration fairness)\n",
    "params = study.best_params.copy()\n",
    "base_model = XGBClassifier(\n",
    "    objective='binary:logistic', tree_method='hist',\n",
    "    learning_rate=params['learning_rate'],\n",
    "    n_estimators=params['n_estimators'],\n",
    "    max_depth=params['max_depth'],\n",
    "    min_child_weight=params['min_child_weight'],\n",
    "    subsample=params['subsample'],\n",
    "    colsample_bytree=params['colsample_bytree'],\n",
    "    reg_lambda=params['lambda'],\n",
    "    reg_alpha=params['alpha'],\n",
    "    gamma=params['gamma'],\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric='logloss',\n",
    "    verbosity=0\n",
    ")\n",
    "base_model.fit(X_tr, y_tr)\n",
    "val_proba_raw = base_model.predict_proba(X_val)[:,1]\n",
    "iso = IsotonicRegression(out_of_bounds='clip')\n",
    "iso.fit(val_proba_raw, y_val)\n",
    "print('Isotonic calibration fitted on validation set.')\n",
    "\n",
    "def predict_calibrated(X):\n",
    "    return iso.transform(base_model.predict_proba(X)[:,1])\n",
    "\n",
    "# Derive operating threshold on calibrated validation probabilities\n",
    "val_cal = predict_calibrated(X_val)\n",
    "ths = np.linspace(0.01,0.9,300)\n",
    "threshold_info = None\n",
    "for t in ths:\n",
    "    m = metrics_at(val_cal, y_val, t)\n",
    "    if (threshold_info is None) or (m['f1'] > threshold_info['f1']):\n",
    "        threshold_info = {**m, 'threshold': float(t)}\n",
    "print('Selected threshold (calibrated validation):', threshold_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ffa2fb",
   "metadata": {},
   "source": [
    "## 12. Metrics Summary & Persistence\n",
    "Create comparative summary tables (tuned vs baseline; full vs calibrated) and persist to artifacts directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified summary table: tuned vs baseline metrics per target\n",
    "import json, pandas as pd\n",
    "rows = []\n",
    "for tgt_name, _ in TARGET_SPECS:\n",
    "    metrics_path = ARTIFACTS_DIR / f'metrics_{tgt_name}.json'\n",
    "    if not metrics_path.exists():\n",
    "        continue\n",
    "    with open(metrics_path) as f:\n",
    "        m = json.load(f)\n",
    "    rows.append({\n",
    "        'target': tgt_name,\n",
    "        'auc': m['auc'],\n",
    "        'pr_auc': m['pr_auc'],\n",
    "        'brier': m['brier'],\n",
    "        'threshold': m['threshold'],\n",
    "        'f1_at_threshold': m['f1_at_threshold'],\n",
    "        'precision_at_threshold': m['precision_at_threshold'],\n",
    "        'recall_at_threshold': m['recall_at_threshold'],\n",
    "        'baseline_val_auc': baseline_results.get(tgt_name)\n",
    "    })\n",
    "summary_df = pd.DataFrame(rows)\n",
    "print(summary_df.sort_values('target'))\n",
    "summary_df.to_csv(ARTIFACTS_DIR / 'metrics_summary.csv', index=False)\n",
    "print('Wrote metrics_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42bb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare full (train+val) vs base calibrated models on test\n",
    "from joblib import load as jobload\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "comparison_rows = []\n",
    "missing_any = False\n",
    "for tgt_name, label_col in TARGET_SPECS:\n",
    "    y_all = labels_df[label_col].astype(int).to_numpy()\n",
    "    y_te_t = y_all[idx_te]\n",
    "    full_path = ARTIFACTS_DIR / f'model_full_{tgt_name}.joblib'\n",
    "    base_path = ARTIFACTS_DIR / f'model_{tgt_name}.joblib'\n",
    "    iso_path = ARTIFACTS_DIR / f'isotonic_{tgt_name}.joblib'\n",
    "    if not (full_path.exists() and base_path.exists() and iso_path.exists()):\n",
    "        print('Skipping', tgt_name, 'missing one of required artifacts')\n",
    "        missing_any = True\n",
    "        continue\n",
    "    full_model = jobload(full_path)\n",
    "    base_model = jobload(base_path)\n",
    "    iso = jobload(iso_path)\n",
    "    raw_full = full_model.predict_proba(X_te)[:,1]\n",
    "    raw_base = base_model.predict_proba(X_te)[:,1]\n",
    "    cal_base = iso.transform(raw_base)\n",
    "    row = {\n",
    "        'target': tgt_name,\n",
    "        'full_raw_auc': roc_auc_score(y_te_t, raw_full),\n",
    "        'full_raw_pr_auc': average_precision_score(y_te_t, raw_full),\n",
    "        'base_raw_auc': roc_auc_score(y_te_t, raw_base),\n",
    "        'base_cal_auc': roc_auc_score(y_te_t, cal_base),\n",
    "        'base_cal_pr_auc': average_precision_score(y_te_t, cal_base),\n",
    "        'base_cal_brier': brier_score_loss(y_te_t, cal_base)\n",
    "    }\n",
    "    comparison_rows.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "if len(comparison_df):\n",
    "    display(comparison_df.sort_values('target'))\n",
    "    comparison_df.to_csv(ARTIFACTS_DIR / 'full_vs_calibrated_comparison.csv', index=False)\n",
    "    print('Wrote full_vs_calibrated_comparison.csv')\n",
    "else:\n",
    "    print('No comparison rows generated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd5b50",
   "metadata": {},
   "source": [
    "## 13. Inference & Enriched Metrics\n",
    "Merge primary metrics with comparison outputs for downstream reporting; quick sanity preview of artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge comparison with metrics summary (enriched)\n",
    "import pandas as pd, json\n",
    "summary_path = ARTIFACTS_DIR / 'metrics_summary.csv'\n",
    "comp_path = ARTIFACTS_DIR / 'full_vs_calibrated_comparison.csv'\n",
    "if summary_path.exists() and comp_path.exists():\n",
    "    summary_df = pd.read_csv(summary_path)\n",
    "    comp_df = pd.read_csv(comp_path)\n",
    "    merged = summary_df.merge(comp_df, on='target', how='left')\n",
    "    merged.to_csv(ARTIFACTS_DIR / 'metrics_summary_enriched.csv', index=False)\n",
    "    print('Wrote metrics_summary_enriched.csv')\n",
    "    display(merged.sort_values('target'))\n",
    "else:\n",
    "    print('Missing one of summary or comparison CSV; skip enrichment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88049aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC Difference Significance (Bootstrap between full_raw and base_cal)\n",
    "import numpy as np, pandas as pd\n",
    "from joblib import load as jobload\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "RESULTS = []\n",
    "N_BOOT = 2000  # increase for tighter CI (runtime ~ O(N_BOOT))\n",
    "RNG = np.random.default_rng(42)\n",
    "\n",
    "for tgt_name, label_col in TARGET_SPECS:\n",
    "    full_path = ARTIFACTS_DIR / f'model_full_{tgt_name}.joblib'\n",
    "    base_path = ARTIFACTS_DIR / f'model_{tgt_name}.joblib'\n",
    "    iso_path = ARTIFACTS_DIR / f'isotonic_{tgt_name}.joblib'\n",
    "    if not (full_path.exists() and base_path.exists() and iso_path.exists()):\n",
    "        print('Skip significance for', tgt_name, '(missing artifacts)')\n",
    "        continue\n",
    "    y_all = labels_df[label_col].astype(int).to_numpy()\n",
    "    y_te_t = y_all[idx_te]\n",
    "    full_model = jobload(full_path)\n",
    "    base_model = jobload(base_path)\n",
    "    iso = jobload(iso_path)\n",
    "    raw_full = full_model.predict_proba(X_te)[:,1]\n",
    "    cal_base = iso.transform(base_model.predict_proba(X_te)[:,1])\n",
    "    auc_full = roc_auc_score(y_te_t, raw_full)\n",
    "    auc_cal = roc_auc_score(y_te_t, cal_base)\n",
    "    diff = auc_full - auc_cal\n",
    "    n = len(y_te_t)\n",
    "    # Bootstrap\n",
    "    diffs = np.empty(N_BOOT)\n",
    "    for b in range(N_BOOT):\n",
    "        idx = RNG.integers(0, n, size=n)\n",
    "        y_b = y_te_t[idx]\n",
    "        rf_b = raw_full[idx]\n",
    "        cb_b = cal_base[idx]\n",
    "        try:\n",
    "            diffs[b] = roc_auc_score(y_b, rf_b) - roc_auc_score(y_b, cb_b)\n",
    "        except ValueError:\n",
    "            # In rare case bootstrap sample has only one class\n",
    "            diffs[b] = np.nan\n",
    "    diffs = diffs[~np.isnan(diffs)]\n",
    "    if len(diffs) < N_BOOT * 0.9:\n",
    "        print('Warning: many degenerate bootstrap samples for', tgt_name)\n",
    "    lower, upper = np.percentile(diffs, [2.5, 97.5])\n",
    "    # Two-sided p-value: proportion of bootstrap diffs with opposite sign or more extreme\n",
    "    if diff >= 0:\n",
    "        p = (np.sum(diffs <= 0) + 1) / (len(diffs) + 1)\n",
    "    else:\n",
    "        p = (np.sum(diffs >= 0) + 1) / (len(diffs) + 1)\n",
    "    RESULTS.append({\n",
    "        'target': tgt_name,\n",
    "        'auc_full_raw': auc_full,\n",
    "        'auc_base_cal': auc_cal,\n",
    "        'diff_full_minus_cal': diff,\n",
    "        'ci_95_lower': lower,\n",
    "        'ci_95_upper': upper,\n",
    "        'approx_p_two_sided': p * 2 if p * 2 <= 1 else 1.0,\n",
    "        'n_test': n,\n",
    "        'n_boot_effective': int(len(diffs))\n",
    "    })\n",
    "\n",
    "if RESULTS:\n",
    "    sig_df = pd.DataFrame(RESULTS).sort_values('target')\n",
    "    display(sig_df)\n",
    "    sig_df.to_csv(ARTIFACTS_DIR / 'auc_diff_significance.csv', index=False)\n",
    "    print('Wrote auc_diff_significance.csv')\n",
    "else:\n",
    "    print('No targets processed for significance analysis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7b382",
   "metadata": {},
   "source": [
    "## 14. SHAP Feature Attribution\n",
    "Compute post‑hoc feature attributions using SHAP (TreeExplainer) on the trained base XGBoost models (the ones paired with isotonic calibration). Rationale: calibrated probabilities are a monotonic transformation of raw model scores; isotonic preserves rank order locally, so SHAP on the raw base model approximates contributions to the calibrated output.\n",
    "\n",
    "Methodology:\n",
    "- Use `TreeExplainer` for efficiency on gradient boosted trees.\n",
    "- Sample up to 4,000 training instances (stratified) as background to reduce memory while preserving class signal.\n",
    "- Compute SHAP values on the held‑out test set only (avoid leakage and align with evaluation metrics).\n",
    "- Aggregate mean |SHAP| per feature for global importance; persist full matrix for potential future local explanations.\n",
    "- Save: CSV (importance ordering), raw `.npy` dense SHAP array, and PNG summary bar plot per target.\n",
    "\n",
    "Notes:\n",
    "- For very wide feature spaces, you can optionally sparsify or threshold low‑variance columns earlier to reduce runtime.\n",
    "- If features contain derived aggregations (e.g., max/min/mean vitals), interpret top drivers in clinical context before decision support use.\n",
    "- Future extension: compute per‑demographic group mean |SHAP| to explore feature usage disparity alongside performance fairness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0779e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP computation for each target (base model perspective)\n",
    "import numpy as np, pandas as pd, json, os, gc\n",
    "from joblib import load as jobload\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "X_test = X_te  # test matrix (numpy array)\n",
    "feature_names = list(feature_df.columns)\n",
    "\n",
    "# Background sample (stratified on primary target) to cap complexity\n",
    "MAX_BACKGROUND = 4000\n",
    "background_indices = idx_tr\n",
    "y_primary = labels_df[TARGET_SPECS[0][1]].astype(int).to_numpy()\n",
    "train_labels_primary = y_primary[idx_tr]\n",
    "\n",
    "if len(background_indices) > MAX_BACKGROUND:\n",
    "    df_bg = pd.DataFrame({'idx': background_indices, 'y': train_labels_primary})\n",
    "    pos = df_bg[df_bg.y==1]; neg = df_bg[df_bg.y==0]\n",
    "    # Keep min(all positives, half budget)\n",
    "    target_pos = min(len(pos), MAX_BACKGROUND//2)\n",
    "    pos_sample = pos.sample(n=target_pos, random_state=SEED) if len(pos) > target_pos else pos\n",
    "    remaining = MAX_BACKGROUND - len(pos_sample)\n",
    "    neg_sample = neg.sample(n=min(remaining, len(neg)), random_state=SEED)\n",
    "    bg_sample = pd.concat([pos_sample, neg_sample]).sample(frac=1, random_state=SEED)\n",
    "    background_indices = bg_sample['idx'].to_numpy()\n",
    "\n",
    "X_background = X[background_indices]\n",
    "print(f'SHAP background size: {X_background.shape[0]} (from train)')\n",
    "\n",
    "shap_artifacts = []\n",
    "\n",
    "for tgt_name, label_col in TARGET_SPECS:\n",
    "    base_path = ARTIFACTS_DIR / f'model_{tgt_name}.joblib'\n",
    "    if not base_path.exists():\n",
    "        print(f'Skip SHAP for {tgt_name}: missing base model')\n",
    "        continue\n",
    "    print(f'Computing SHAP for target: {tgt_name}')\n",
    "    try:\n",
    "        model = jobload(base_path)\n",
    "        explainer = shap.TreeExplainer(model, feature_perturbation='tree_path_dependent')\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "    except Exception as e:\n",
    "        print(f'Failed SHAP for {tgt_name}: {e}')\n",
    "        continue\n",
    "\n",
    "    # Handle potential list (multi-class); our case is binary so expect 2-d\n",
    "    if isinstance(shap_values, list):\n",
    "        # If shap returns list (e.g., per class), aggregate class 1 if available\n",
    "        shap_values = shap_values[1] if len(shap_values) > 1 else shap_values[0]\n",
    "\n",
    "    abs_mean = np.abs(shap_values).mean(axis=0)\n",
    "    importance_df = pd.DataFrame({'feature': feature_names, 'mean_abs_shap': abs_mean}).sort_values('mean_abs_shap', ascending=False)\n",
    "    csv_path = ARTIFACTS_DIR / f'shap_feature_importance_{tgt_name}.csv'\n",
    "    npy_path = ARTIFACTS_DIR / f'shap_values_{tgt_name}.npy'\n",
    "    importance_df.to_csv(csv_path, index=False)\n",
    "    np.save(npy_path, shap_values)\n",
    "\n",
    "    # Horizontal bar plot (top 25)\n",
    "    top_k = min(25, len(importance_df))\n",
    "    top_imp = importance_df.head(top_k)\n",
    "    plt.figure(figsize=(8, max(5, top_k*0.25)))\n",
    "    plt.barh(top_imp['feature'][::-1], top_imp['mean_abs_shap'][::-1], color='steelblue')\n",
    "    plt.xlabel('Mean |SHAP| (test)')\n",
    "    plt.title(f'SHAP Global Importance (Top {top_k}) - {tgt_name}')\n",
    "    plt.tight_layout()\n",
    "    png_bar = ARTIFACTS_DIR / f'shap_summary_{tgt_name}.png'\n",
    "    plt.savefig(png_bar, dpi=160)\n",
    "    plt.show()\n",
    "\n",
    "    # Beeswarm plot\n",
    "    png_bee = None\n",
    "    try:\n",
    "        shap.summary_plot(shap_values, features=X_test, feature_names=feature_names, max_display=25, show=False)\n",
    "        plt.tight_layout()\n",
    "        png_bee = ARTIFACTS_DIR / f'shap_beeswarm_{tgt_name}.png'\n",
    "        plt.savefig(png_bee, dpi=160)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f'Beeswarm failed ({tgt_name}): {e}')\n",
    "\n",
    "    shap_artifacts.append({\n",
    "        'target': tgt_name,\n",
    "        'importance_csv': str(csv_path),\n",
    "        'values_npy': str(npy_path),\n",
    "        'bar_png': str(png_bar),\n",
    "        'beeswarm_png': str(png_bee) if png_bee else None\n",
    "    })\n",
    "\n",
    "    # Free memory early for large matrices\n",
    "    del shap_values; gc.collect()\n",
    "\n",
    "print('SHAP artifacts written:')\n",
    "print(json.dumps(shap_artifacts, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
