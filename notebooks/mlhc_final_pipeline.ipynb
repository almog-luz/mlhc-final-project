{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791201e3",
   "metadata": {},
   "source": [
    "End-to-end (current focus: Readmission with XGBoost + Optuna). Structure prepared for mortality & prolonged LOS later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63c1e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\n",
      "Data dir exists: True\n",
      "Versions: {\n",
      "  \"python\": \"3.13.3\",\n",
      "  \"platform\": \"Windows-10-10.0.19045-SP0\",\n",
      "  \"xgboost\": \"2.1.1\",\n",
      "  \"optuna\": \"4.5.0\",\n",
      "  \"shap\": \"0.48.0\",\n",
      "  \"sklearn\": \"1.7.1\",\n",
      "  \"pandas\": \"2.3.1\",\n",
      "  \"numpy\": \"2.2.6\"\n",
      "}\n",
      "Versions: {\n",
      "  \"python\": \"3.13.3\",\n",
      "  \"platform\": \"Windows-10-10.0.19045-SP0\",\n",
      "  \"xgboost\": \"2.1.1\",\n",
      "  \"optuna\": \"4.5.0\",\n",
      "  \"shap\": \"0.48.0\",\n",
      "  \"sklearn\": \"1.7.1\",\n",
      "  \"pandas\": \"2.3.1\",\n",
      "  \"numpy\": \"2.2.6\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Environment & core imports\n",
    "import os, sys, json, random, platform, importlib, datetime, pathlib\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# Add project root to path if notebook is under notebooks/\n",
    "ROOT = pathlib.Path(__file__).resolve().parents[1] if '__file__' in globals() else pathlib.Path.cwd().parents[0]\n",
    "if str(ROOT) not in sys.path: sys.path.insert(0, str(ROOT))\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "PROJECT_ROOT = (Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd())\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'project/artifacts'\n",
    "RUNS_ROOT = PROJECT_ROOT / 'runs'\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data dir exists: {(DATA_DIR).exists()}\")\n",
    "VERSIONS = {'python': sys.version.split()[0], 'platform': platform.platform()}\n",
    "for pkg in ['xgboost','optuna','shap','sklearn','pandas','numpy']:\n",
    "    try:\n",
    "        m = importlib.import_module(pkg if pkg != 'sklearn' else 'sklearn')\n",
    "        VERSIONS[pkg] = getattr(m,'__version__','?')\n",
    "    except Exception as e:\n",
    "        VERSIONS[pkg] = f'NA({e})'\n",
    "print('Versions:', json.dumps(VERSIONS, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560f373",
   "metadata": {},
   "source": [
    "### Labels\n",
    "Load readmission labels (or synthesize) and report prevalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c3dfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels source: c:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\project\\artifacts\\labels.csv | shape=(28340, 5) | readmission_prevalence=0.0435\n"
     ]
    }
   ],
   "source": [
    "# Load canonical labels produced by duckdb_extraction notebook\n",
    "import pandas as pd, random\n",
    "from pathlib import Path\n",
    "\n",
    "# Force single authoritative path (output of duckdb_extraction)\n",
    "LABELS_PATH = (PROJECT_ROOT / 'project' / 'artifacts' / 'labels.csv')\n",
    "if not LABELS_PATH.exists():\n",
    "    raise FileNotFoundError(f'Expected labels at {LABELS_PATH}. Run duckdb_extraction notebook first to generate labels.csv there.')\n",
    "\n",
    "labels_df = pd.read_csv(LABELS_PATH)\n",
    "# Normalize required columns\n",
    "required_cols = {'subject_id','hadm_id','readmission_label','mortality_label','prolonged_los_label'}\n",
    "missing = required_cols - set(labels_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f'Missing expected label columns in {LABELS_PATH}: {missing}')\n",
    "\n",
    "labels_df = labels_df.drop_duplicates('subject_id')\n",
    "for col in ['readmission_label','mortality_label','prolonged_los_label']:\n",
    "    labels_df[col] = labels_df[col].astype(int)\n",
    "assert labels_df['subject_id'].isna().sum()==0\n",
    "prev_readmit = labels_df['readmission_label'].mean()\n",
    "print(f'Labels source: {LABELS_PATH} | shape={labels_df.shape} | readmission_prevalence={prev_readmit:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4f32a",
   "metadata": {},
   "source": [
    "### Features\n",
    "Load (or regenerate) prepared feature matrix aligned to subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9d838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cohort-diff] label_subjects=28340 feature_subjects=28340\n",
      "[cohort-diff] missing_in_features=0 extra_in_features=0\n",
      "[align] labels=28340 features=28340 missing_in_features=0 extra_in_features=0 overlap_frac=1.0000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Some label subjects not found in features (after explicit check).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     feature_df_aligned = \u001b[43mfeature_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_subjects\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1420\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1418\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index with multidimensional key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1360\u001b[39m, in \u001b[36m_LocIndexer._getitem_iterable\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1359\u001b[39m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m keyarr, indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._reindex_with_indexers(\n\u001b[32m   1362\u001b[39m     {axis: [keyarr, indexer]}, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1363\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1558\u001b[39m, in \u001b[36m_LocIndexer._get_listlike_indexer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1556\u001b[39m axis_name = \u001b[38;5;28mself\u001b[39m.obj._get_axis_name(axis)\n\u001b[32m-> \u001b[39m\u001b[32m1558\u001b[39m keyarr, indexer = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Almog Luz\\Documents\\GitHub\\mlhc-final-project\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index([ 1000, 10000, 10003, 10004, 10005, 10007, 10008, 10009,  1001, 10011,\\n       ...\\n        9996,  9997, 99973, 99982, 99983, 99985, 99991, 99992, 99995, 99999], dtype='int64', name='subject_id', length=28340)] are in the [index]\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     82\u001b[39m     feature_df_aligned = feature_df.loc[label_subjects.values]\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSome label subjects not found in features (after explicit check).\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Decide on imputation strategy\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Prefer leaving NaN for model pipeline to handle; if you must fill, quantify impact first.\u001b[39;00m\n\u001b[32m     88\u001b[39m missing_before = feature_df_aligned.isna().mean().mean()\n",
      "\u001b[31mRuntimeError\u001b[39m: Some label subjects not found in features (after explicit check)."
     ]
    }
   ],
   "source": [
    "# Load feature matrix (regenerate if tiny/corrupt) + strict safe alignment\n",
    "import pandas as pd, numpy as np, json, os\n",
    "from pathlib import Path\n",
    "\n",
    "feature_path = ARTIFACTS_DIR / 'features_full.parquet'\n",
    "if not feature_path.exists():\n",
    "    raise FileNotFoundError(f'Missing feature parquet: {feature_path}')\n",
    "\n",
    "feature_df = pd.read_parquet(feature_path)\n",
    "regenerated = False  # placeholder if regeneration logic later added\n",
    "\n",
    "# Ensure subject_id index\n",
    "if 'subject_id' in feature_df.columns:\n",
    "    if feature_df['subject_id'].duplicated().any():\n",
    "        raise RuntimeError('Duplicate subject_id rows in feature parquet.')\n",
    "    feature_df = feature_df.set_index('subject_id')\n",
    "\n",
    "# Canonical integer index (fix KeyError mismatch from mixed string/int earlier)\n",
    "try:\n",
    "    feature_df.index = feature_df.index.astype(int)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'Failed to coerce feature index to int: {e}')\n",
    "if feature_df.index.has_duplicates:\n",
    "    raise RuntimeError('Feature index has duplicates after int coercion.')\n",
    "\n",
    "# Labels already loaded upstream into labels_df\n",
    "label_subjects = labels_df['subject_id'].astype(int)\n",
    "\n",
    "feat_ids = feature_df.index\n",
    "missing_in_features = set(label_subjects) - set(feat_ids)\n",
    "extra_in_features = set(feat_ids) - set(label_subjects)\n",
    "overlap_frac = 1 - (len(missing_in_features)/len(label_subjects)) if len(label_subjects) else 0\n",
    "print(f\"[cohort-diff] label_subjects={len(label_subjects)} feature_subjects={len(feat_ids)}\")\n",
    "print(f\"[cohort-diff] missing_in_features={len(missing_in_features)} extra_in_features={len(extra_in_features)}\")\n",
    "print(f\"[align] overlap_frac={overlap_frac:.4f}\")\n",
    "\n",
    "if missing_in_features:\n",
    "    raise RuntimeError(f'Missing {len(missing_in_features)} subjects (sample: {list(missing_in_features)[:10]})')\n",
    "\n",
    "# Pre-alignment variance snapshot (for collapse detection)\n",
    "pre_var = feature_df.var(numeric_only=True, ddof=0)\n",
    "\n",
    "# Strict ordering alignment\n",
    "feature_df_aligned = feature_df.loc[label_subjects.values]\n",
    "\n",
    "post_var = feature_df_aligned.var(numeric_only=True, ddof=0)\n",
    "collapsed = [c for c in pre_var.index if pre_var[c] > 0 and post_var[c] == 0]\n",
    "if collapsed:\n",
    "    print(f'WARNING: {len(collapsed)} columns lost variance post-alignment (e.g. {collapsed[:5]}).')\n",
    "else:\n",
    "    print('[align] Variance preserved.')\n",
    "\n",
    "print('Features aligned shape:', feature_df_aligned.shape, '| regenerated' if regenerated else '')\n",
    "\n",
    "# Expose for downstream cells\n",
    "features_loaded = feature_df_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdcfb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Flat / Near-Flat Feature Scan (post-load)\n",
    "import pandas as pd, numpy as np, math, json, os, time\n",
    "from pathlib import Path\n",
    "\n",
    "# Expect a DataFrame named `features_loaded` or `features` from previous cell.\n",
    "# Try common variable names.\n",
    "_df = None\n",
    "for name in ['features_loaded','features','X','features_full', \"feature_df\"]:\n",
    "    if name in globals():\n",
    "        obj = globals()[name]\n",
    "        if isinstance(obj, pd.DataFrame):\n",
    "            _df = obj\n",
    "            break\n",
    "if _df is None:\n",
    "    raise RuntimeError('No feature DataFrame (features_loaded/features/X) found for flatness scan.')\n",
    "\n",
    "print(f'[flatness-scan] Feature matrix shape: {_df.shape}')\n",
    "\n",
    "# Numeric subset only\n",
    "num_df = _df.select_dtypes(include=[np.number])\n",
    "if num_df.empty:\n",
    "    raise RuntimeError('No numeric columns to scan.')\n",
    "\n",
    "std_series = num_df.std(ddof=0)\n",
    "var_series = num_df.var(ddof=0)\n",
    "missing_frac = num_df.isna().mean()\n",
    "unique_counts = num_df.nunique(dropna=True)\n",
    "\n",
    "# Dominant value fraction (highest frequency / count)\n",
    "dom_frac = {}\n",
    "for c in num_df.columns:\n",
    "    vc = num_df[c].value_counts(dropna=True)\n",
    "    if vc.empty:\n",
    "        dom_frac[c] = math.nan\n",
    "    else:\n",
    "        dom_frac[c] = float(vc.iloc[0] / vc.sum())\n",
    "\n",
    "dom_frac = pd.Series(dom_frac)\n",
    "\n",
    "flat = var_series[var_series == 0.0].index.tolist()\n",
    "near_flat = var_series[(var_series > 0.0) & (var_series < 1e-10)].index.tolist()\n",
    "very_low_var = var_series[(var_series >= 1e-10) & (var_series < 1e-6)].index.tolist()\n",
    "\n",
    "dominant_high = dom_frac[dom_frac >= 0.995].index.tolist()\n",
    "\n",
    "print(f'[flatness-scan] zero-variance: {len(flat)}  near-zero(<1e-10): {len(near_flat)}  very-low(<1e-6): {len(very_low_var)}  dominant(top>=99.5%): {len(dominant_high)}')\n",
    "\n",
    "# Summaries\n",
    "def summary_list(cols, label, limit=15):\n",
    "    if not cols:\n",
    "        print(f'  {label}: none')\n",
    "        return\n",
    "    print(f'  {label} (showing up to {limit}):')\n",
    "    for c in cols[:limit]:\n",
    "        print(f'    - {c} | var={var_series[c]:.3e} std={std_series[c]:.3e} miss={missing_frac[c]:.3f} uniq={unique_counts[c]} dom={dom_frac[c]:.3f}')\n",
    "\n",
    "summary_list(flat, 'Zero variance')\n",
    "summary_list(near_flat, 'Near-zero variance (<1e-10)')\n",
    "summary_list(very_low_var, 'Very-low variance (<1e-6)')\n",
    "summary_list(dominant_high, 'Dominant single value (>=99.5%)')\n",
    "\n",
    "# Aggregate distribution of std for a quick health view\n",
    "bins = [-1,0,1e-10,1e-6,1e-3,1e-1,1,10,100,1e6]\n",
    "std_hist = std_series.groupby(pd.cut(std_series, bins=bins)).size()\n",
    "print('\\n[flatness-scan] Std distribution buckets:')\n",
    "print(std_hist)\n",
    "\n",
    "# Optional: produce a DataFrame for downstream interactive analysis\n",
    "flatness_report = pd.DataFrame({\n",
    "    'var': var_series,\n",
    "    'std': std_series,\n",
    "    'missing_frac': missing_frac,\n",
    "    'n_unique': unique_counts,\n",
    "    'dominant_frac': dom_frac,\n",
    "})\n",
    "flatness_report['is_zero_var'] = flatness_report['var'] == 0.0\n",
    "flatness_report['is_near_zero_var'] = (flatness_report['var'] > 0) & (flatness_report['var'] < 1e-10)\n",
    "flatness_report['is_dominant'] = flatness_report['dominant_frac'] >= 0.995\n",
    "\n",
    "print('\\n[flatness-scan] flatness_report head:')\n",
    "display(flatness_report.head())\n",
    "\n",
    "# Flag if excessive collapse\n",
    "zero_ratio = len(flat) / len(num_df.columns)\n",
    "if zero_ratio > 0.2:\n",
    "    print(f'WARNING: High fraction of zero-variance columns ({zero_ratio:.1%})')\n",
    "\n",
    "print('[flatness-scan] Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcbb028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep variance trace: rebuild features fresh and compare to loaded parquet\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "ART_DIR = ARTIFACTS_DIR if 'ARTIFACTS_DIR' in globals() else Path.cwd()/ 'project' / 'artifacts'\n",
    "parquet_path = ART_DIR / 'features_full.parquet'\n",
    "if not parquet_path.exists():\n",
    "    raise FileNotFoundError(f'Missing parquet at {parquet_path}')\n",
    "\n",
    "loaded_df = None\n",
    "for name in ['features_loaded','features','X','features_full']:\n",
    "    if name in globals() and isinstance(globals()[name], pd.DataFrame):\n",
    "        loaded_df = globals()[name]\n",
    "        break\n",
    "if loaded_df is None:\n",
    "    loaded_df = pd.read_parquet(parquet_path)\n",
    "\n",
    "print('[variance-trace] Loaded (disk) shape:', loaded_df.shape)\n",
    "print('[variance-trace] Loaded nunique>1 count:', (loaded_df.nunique(dropna=True)>1).sum())\n",
    "\n",
    "# Attempt rebuild using pipeline (training-side) WITHOUT persistence.\n",
    "try:\n",
    "    import duckdb\n",
    "\n",
    "    PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "    DATA_DIR = PROJECT_ROOT / 'data'\n",
    "    ARTIFACTS_DIR = PROJECT_ROOT / 'project' / 'artifacts'\n",
    "    DB_PATH = DATA_DIR / 'mimiciii.duckdb'  # Adjust if your filename differs\n",
    "    SUBJECT_SAMPLE_LIMIT = None  # e.g., 500 for a quick run; set None for full\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    con = duckdb.connect(str(DB_PATH))\n",
    "    print('Connected to DuckDB.')\n",
    "    # (Optional) list tables to verify schema\n",
    "    try:\n",
    "        tbls = con.execute(\"SHOW TABLES\").fetchdf()\n",
    "        print('Available tables:', tbls['name'].tolist())\n",
    "    except Exception as e:\n",
    "        print('Could not list tables:', e)\n",
    "\n",
    "    import project.pipeline_core as pc\n",
    "    importlib.reload(pc)\n",
    "    # Need a cohort subject list: derive from labels or index if present\n",
    "    subj_source = None\n",
    "    if 'labels_df' in globals() and isinstance(labels_df, pd.DataFrame) and not labels_df.empty:\n",
    "        subj_source = labels_df['subject_id'].astype(int).tolist()\n",
    "    elif 'labels' in globals() and isinstance(labels, pd.DataFrame) and not labels.empty:\n",
    "        subj_source = labels['subject_id'].astype(int).tolist()\n",
    "    else:\n",
    "        # fallback from loaded features index or column\n",
    "        if loaded_df.index.name == 'subject_id' or 'subject_id' in loaded_df.columns:\n",
    "            subj_source = (loaded_df.index if loaded_df.index.name=='subject_id' else loaded_df['subject_id']).astype(int).tolist()\n",
    "    if subj_source is None:\n",
    "        raise RuntimeError('Could not determine subject list for rebuild.')\n",
    "    # Connect (assumes `con` live from earlier cell)\n",
    "    rebuilt_feats, rebuilt_labels, dbg2 = pc.run_training_side_pipeline(\n",
    "        con,\n",
    "        cohort_subject_ids=subj_source,\n",
    "        debug=True\n",
    "    )\n",
    "    print('[variance-trace] Rebuilt shape:', rebuilt_feats.shape)\n",
    "    print('[variance-trace] Rebuilt nunique>1 count:', (rebuilt_feats.nunique(dropna=True)>1).sum())\n",
    "    # Align to same columns (union) and compare variance patterns\n",
    "    common = [c for c in loaded_df.columns if c in rebuilt_feats.columns]\n",
    "    print('[variance-trace] Common column count:', len(common))\n",
    "    if len(common)==0:\n",
    "        print('No overlapping columns to compare.')\n",
    "    else:\n",
    "        loaded_var = loaded_df[common].nunique(dropna=True)\n",
    "        rebuilt_var = rebuilt_feats[common].nunique(dropna=True)\n",
    "        both_constant = ((loaded_var<=1) & (rebuilt_var<=1)).sum()\n",
    "        loaded_only_constant = ((loaded_var<=1) & (rebuilt_var>1)).sum()\n",
    "        rebuilt_only_constant = ((rebuilt_var<=1) & (loaded_var>1)).sum()\n",
    "        print(f'[variance-trace] both_constant={both_constant} loaded_only_constant={loaded_only_constant} rebuilt_only_constant={rebuilt_only_constant}')\n",
    "        if loaded_only_constant>0:\n",
    "            offenders = [c for c in common if (loaded_var[c]<=1 and rebuilt_var[c]>1)][:15]\n",
    "            print('[variance-trace] Example columns constant on disk but variable when rebuilt:', offenders)\n",
    "        if rebuilt_only_constant>0:\n",
    "            offenders2 = [c for c in common if (rebuilt_var[c]<=1 and loaded_var[c]>1)][:15]\n",
    "            print('[variance-trace] Example columns constant when rebuilt but variable on disk:', offenders2)\n",
    "except Exception as e:\n",
    "    print('Variance trace rebuild failed ->', e)\n",
    "\n",
    "print('[variance-trace] Complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa08550",
   "metadata": {},
   "source": [
    "### Train/Validation/Test Split\n",
    "Create 60/20/20 stratified split and compute imbalance weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/valid/test split (60/20/20) + class weight factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "readmit_y = labels_df['readmission_label'].astype(int).to_numpy()\n",
    "subject_index = feature_df.index.to_numpy()\n",
    "X = feature_df.values\n",
    "X_tr, X_temp, y_tr, y_temp, sid_tr, sid_temp = train_test_split(\n",
    "    X, readmit_y, subject_index, test_size=0.4, stratify=readmit_y, random_state=SEED)\n",
    "X_val, X_te, y_val, y_te, sid_val, sid_te = train_test_split(\n",
    "    X_temp, y_temp, sid_temp, test_size=0.5, stratify=y_temp, random_state=SEED)\n",
    "pos_rate = y_tr.mean(); scale_pos_weight = (1-pos_rate)/max(pos_rate,1e-6)\n",
    "print(f'Split -> train {X_tr.shape} valid {X_val.shape} test {X_te.shape} | pos_rate_train={pos_rate:.4f} | spw≈{scale_pos_weight:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281cc739",
   "metadata": {},
   "source": [
    "### Metrics Helpers\n",
    "Utility functions to compute threshold-dependent metrics and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb41202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "C_FP = 1.0; C_FN = 5.0\n",
    "beta = 2.0\n",
    "\n",
    "def metrics_at(proba, y, thr):\n",
    "    pred = (proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "    cost = C_FP*fp + C_FN*fn\n",
    "    f1 = f1_score(y, pred)\n",
    "    prec = tp/(tp+fp+1e-9); rec = tp/(tp+fn+1e-9)\n",
    "    fbeta = (1+beta**2)*prec*rec/(beta**2*prec+rec+1e-9)\n",
    "    return dict(f1=f1, precision=prec, recall=rec, cost=cost, fbeta=fbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739ba7d",
   "metadata": {},
   "source": [
    "### Split data to train/validation/test with targets\n",
    "Stratified 60/20/20 split (shared across targets for matched cohorts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea02b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 42\n",
    "rng_global = np.random.default_rng(SEED)\n",
    "\n",
    "TARGET_SPECS = [\n",
    "    ('readmission','readmission_label'),\n",
    "    ('mortality','mortality_label'),\n",
    "    ('prolonged_los','prolonged_los_label'),\n",
    "]\n",
    "\n",
    "\n",
    "# Verify presence of target columns\n",
    "missing_targets = [lbl for _,lbl in TARGET_SPECS if lbl not in labels_df.columns]\n",
    "if missing_targets:\n",
    "    raise ValueError(f\"Missing target label columns: {missing_targets}\")\n",
    "\n",
    "# Shared subject index + base split for comparability\n",
    "subject_index = feature_df.index.to_numpy()\n",
    "X = feature_df.values\n",
    "primary_y = labels_df[TARGET_SPECS[0][1]].astype(int).to_numpy()  # stratify on first target\n",
    "X_tr, X_temp, y_tr_primary, y_temp_primary, sid_tr, sid_temp = train_test_split(X, primary_y, subject_index, test_size=0.4, stratify=primary_y, random_state=SEED)\n",
    "X_val, X_te, y_val_primary, y_te_primary, sid_val, sid_te = train_test_split(X_temp, y_temp_primary, sid_temp, test_size=0.5, stratify=y_temp_primary, random_state=SEED)\n",
    "print(f'Shared split shapes -> train {X_tr.shape} val {X_val.shape} test {X_te.shape}')\n",
    "\n",
    "subj_to_pos = {sid:i for i,sid in enumerate(subject_index)}\n",
    "idx_tr = np.array([subj_to_pos[s] for s in sid_tr])\n",
    "idx_val = np.array([subj_to_pos[s] for s in sid_val])\n",
    "idx_te = np.array([subj_to_pos[s] for s in sid_te])\n",
    "metrics_all = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce642105",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c9b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-target baseline (logistic) for reference per target\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "baseline_results = {}\n",
    "X_base = X_tr  # using training split from shared split\n",
    "X_val_base = X_val\n",
    "\n",
    "for tgt_name, label_col in TARGET_SPECS:\n",
    "    y_tr_t = labels_df[label_col].astype(int).to_numpy()[idx_tr]\n",
    "    y_val_t = labels_df[label_col].astype(int).to_numpy()[idx_val]\n",
    "    pipe = Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy='median')),\n",
    "        (\"sc\", StandardScaler(with_mean=False)),\n",
    "        (\"lr\", LogisticRegression(max_iter=500, class_weight='balanced', solver='liblinear'))\n",
    "    ])\n",
    "    pipe.fit(X_base, y_tr_t)\n",
    "    val_proba = pipe.predict_proba(X_val_base)[:,1]\n",
    "    auc_val = roc_auc_score(y_val_t, val_proba)\n",
    "    baseline_results[tgt_name] = float(auc_val)\n",
    "print('Baseline logistic validation AUCs per target:', baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49c3de",
   "metadata": {},
   "source": [
    "### Multi-Target Hyperparameter Tuning\n",
    "We run an Optuna study separately for each target (readmission, mortality, prolonged LOS) using the same feature matrix & subject-aligned train/val/test split for comparability. Each target gets:\n",
    "1. Optuna CV objective (5-fold on training portion only)\n",
    "2. Final model retrained on train+validation\n",
    "3. Isotonic calibration on validation\n",
    "4. Threshold selection (max F1 on calibrated validation)\n",
    "5. Test evaluation & artifact persistence\n",
    "All artifacts written to `artifacts/` with target-specific prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2550bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-target tuning & training pipeline (updated to persist final_model + artifact paths)\n",
    "import optuna, numpy as np, json, joblib\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Hyperparameter search configuration\n",
    "N_TRIALS_PER_TARGET = 5  # increase for better tuning\n",
    "N_FOLDS = 5\n",
    "MAX_ROUNDS = 400\n",
    "SPEED_SAMPLE_MAX = 12000\n",
    "\n",
    "def make_objective(X_train_full, y_train_full):\n",
    "    def objective(trial: optuna.Trial):\n",
    "        params = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "            'min_child_weight': trial.suggest_float('min_child_weight', 1.0, 8.0),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_lambda': trial.suggest_float('lambda', 1e-3, 5.0, log=True),\n",
    "            'reg_alpha': trial.suggest_float('alpha', 1e-3, 2.0, log=True),\n",
    "            'gamma': trial.suggest_float('gamma', 0.0, 4.0),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 120, MAX_ROUNDS),\n",
    "        }\n",
    "        fold_aucs = []\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "        for fold,(tr_idx_local, va_idx_local) in enumerate(skf.split(X_train_full, y_train_full), 1):\n",
    "            Xtr_f, Xva_f = X_train_full[tr_idx_local], X_train_full[va_idx_local]\n",
    "            ytr_f, yva_f = y_train_full[tr_idx_local], y_train_full[va_idx_local]\n",
    "            if Xtr_f.shape[0] > SPEED_SAMPLE_MAX:\n",
    "                pos_idx = np.where(ytr_f==1)[0]\n",
    "                neg_idx = np.where(ytr_f==0)[0]\n",
    "                keep_pos = pos_idx\n",
    "                remaining = SPEED_SAMPLE_MAX - len(keep_pos)\n",
    "                if remaining < len(neg_idx):\n",
    "                    keep_neg = np.random.default_rng(SEED+fold).choice(neg_idx, size=remaining, replace=False)\n",
    "                else:\n",
    "                    keep_neg = neg_idx\n",
    "                keep = np.concatenate([keep_pos, keep_neg])\n",
    "                np.random.default_rng(SEED+fold).shuffle(keep)\n",
    "                Xtr_f = Xtr_f[keep]; ytr_f = ytr_f[keep]\n",
    "            pos_rate_fold = ytr_f.mean(); spw = (1-pos_rate_fold)/max(pos_rate_fold,1e-6)\n",
    "            model = XGBClassifier(objective='binary:logistic', tree_method='hist', scale_pos_weight=spw, eval_metric='auc', verbosity=0, **params)\n",
    "            model.fit(Xtr_f, ytr_f, verbose=False)\n",
    "            proba = model.predict_proba(Xva_f)[:,1]\n",
    "            fold_auc = roc_auc_score(yva_f, proba)\n",
    "            fold_aucs.append(fold_auc)\n",
    "        mean_auc = float(np.mean(fold_aucs))\n",
    "        trial.set_user_attr('fold_aucs', fold_aucs)\n",
    "        trial.set_user_attr('cv_mean_auc', mean_auc)\n",
    "        return mean_auc\n",
    "    return objective\n",
    "\n",
    "for tgt_name, label_col in TARGET_SPECS:\n",
    "    print('\\n==== Target:', tgt_name, '('+label_col+') ====')\n",
    "    y_all = labels_df[label_col].astype(int).to_numpy()\n",
    "    y_tr_t = y_all[idx_tr]; y_val_t = y_all[idx_val]; y_te_t = y_all[idx_te]\n",
    "    pos_rate = y_tr_t.mean(); scale_pos_weight = (1-pos_rate)/max(pos_rate,1e-6)\n",
    "    print(f'Pos rate train: {pos_rate:.4f} -> spw {scale_pos_weight:.2f}')\n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "    objective = make_objective(X_tr, y_tr_t)\n",
    "    study.optimize(objective, n_trials=N_TRIALS_PER_TARGET, show_progress_bar=False)\n",
    "    best_params = study.best_params.copy()\n",
    "    print('Best CV mean AUC:', round(study.best_value,4))\n",
    "    print('Best Params:', best_params)\n",
    "    # Final model retrained on train + validation (for potential deployment / ensembling)\n",
    "    X_tr_full_t = np.vstack([X_tr, X_val])\n",
    "    y_tr_full_t = np.concatenate([y_tr_t, y_val_t])\n",
    "    final_model = XGBClassifier(objective='binary:logistic', tree_method='hist', learning_rate=best_params['learning_rate'], n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], min_child_weight=best_params['min_child_weight'], subsample=best_params['subsample'], colsample_bytree=best_params['colsample_bytree'], reg_lambda=best_params['lambda'], reg_alpha=best_params['alpha'], gamma=best_params['gamma'], scale_pos_weight=scale_pos_weight, eval_metric='auc', verbosity=0)\n",
    "    final_model.fit(X_tr_full_t, y_tr_full_t)\n",
    "    # Calibration base model (train-only) to avoid leaking validation into both model weights and calibrator\n",
    "    base_model = XGBClassifier(objective='binary:logistic', tree_method='hist', learning_rate=best_params['learning_rate'], n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], min_child_weight=best_params['min_child_weight'], subsample=best_params['subsample'], colsample_bytree=best_params['colsample_bytree'], reg_lambda=best_params['lambda'], reg_alpha=best_params['alpha'], gamma=best_params['gamma'], scale_pos_weight=scale_pos_weight, eval_metric='logloss', verbosity=0)\n",
    "    base_model.fit(X_tr, y_tr_t)\n",
    "    val_raw = base_model.predict_proba(X_val)[:,1]\n",
    "    iso = IsotonicRegression(out_of_bounds='clip'); iso.fit(val_raw, y_val_t)\n",
    "    val_cal = iso.transform(val_raw)\n",
    "    # Threshold selection on calibrated validation probabilities (maximize F1)\n",
    "    ths = np.linspace(0.01,0.9,300)\n",
    "    best_thr = None; best_f1 = -1; best_val_metrics = None\n",
    "    def _metrics_at(proba, y, thr):\n",
    "        pred = (proba >= thr).astype(int)\n",
    "        tp = ((pred==1)&(y==1)).sum(); fp = ((pred==1)&(y==0)).sum(); fn = ((pred==0)&(y==1)).sum()\n",
    "        prec = tp/(tp+fp+1e-9); rec = tp/(tp+fn+1e-9)\n",
    "        f1 = 2*prec*rec/(prec+rec+1e-9)\n",
    "        return dict(precision=float(prec), recall=float(rec), f1=float(f1))\n",
    "    for t in ths:\n",
    "        m = _metrics_at(val_cal, y_val_t, t)\n",
    "        if m['f1'] > best_f1:\n",
    "            best_f1 = m['f1']; best_thr = float(t); best_val_metrics = m\n",
    "    print('Selected threshold (validation calibrated):', best_thr, best_val_metrics)\n",
    "    # Test evaluation using calibrated base model\n",
    "    test_cal = iso.transform(base_model.predict_proba(X_te)[:,1])\n",
    "    auc = roc_auc_score(y_te_t, test_cal)\n",
    "    pr_auc = average_precision_score(y_te_t, test_cal)\n",
    "    brier = brier_score_loss(y_te_t, test_cal)\n",
    "    test_pred = (test_cal >= best_thr).astype(int)\n",
    "    tp = ((test_pred==1)&(y_te_t==1)).sum(); fp = ((test_pred==1)&(y_te_t==0)).sum(); fn = ((test_pred==0)&(y_te_t==1)).sum()\n",
    "    prec_test = tp/(tp+fp+1e-9); rec_test = tp/(tp+fn+1e-9)\n",
    "    f1_test = 2*prec_test*rec_test/(prec_test+rec_test+1e-9)\n",
    "    metrics = {'auc': float(auc), 'pr_auc': float(pr_auc), 'brier': float(brier), 'threshold': best_thr, 'f1_at_threshold': float(f1_test), 'precision_at_threshold': float(prec_test), 'recall_at_threshold': float(rec_test), 'validation_threshold_info': {**best_val_metrics, 'threshold': best_thr}, 'optuna_best_value_cv_mean_auc': float(study.best_value), 'optuna_best_params': best_params, 'train_rows': int(X_tr.shape[0]), 'val_rows': int(X_val.shape[0]), 'test_rows': int(X_te.shape[0])}\n",
    "    metrics_all[tgt_name] = metrics\n",
    "    print('Test metrics:', json.dumps(metrics, indent=2))\n",
    "    # Persist artifacts (both final_model and calibration pipeline components)\n",
    "    ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "    final_path = ARTIFACTS_DIR / f'model_full_{tgt_name}.joblib'  # trained on train+val\n",
    "    base_path = ARTIFACTS_DIR / f'model_{tgt_name}.joblib'        # train-only (paired with isotonic)\n",
    "    iso_path = ARTIFACTS_DIR / f'isotonic_{tgt_name}.joblib'\n",
    "    joblib.dump(final_model, final_path)\n",
    "    joblib.dump(base_model, base_path)\n",
    "    joblib.dump(iso, iso_path)\n",
    "    with open(ARTIFACTS_DIR / f'metrics_{tgt_name}.json','w') as f: json.dump(metrics, f, indent=2)\n",
    "    with open(ARTIFACTS_DIR / f'threshold_{tgt_name}.txt','w') as f: f.write(str(best_thr))\n",
    "    with open(ARTIFACTS_DIR / f'best_params_{tgt_name}.json','w') as f: json.dump(best_params, f, indent=2)\n",
    "    print(f'Artifact paths ({tgt_name}):')\n",
    "    for p in [final_path, base_path, iso_path, ARTIFACTS_DIR / f'metrics_{tgt_name}.json', ARTIFACTS_DIR / f'threshold_{tgt_name}.txt', ARTIFACTS_DIR / f'best_params_{tgt_name}.json']:\n",
    "        print('  -', p.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c0043b",
   "metadata": {},
   "source": [
    "### Artifact Semantics Clarification\n",
    "\n",
    "\n",
    "We now save two model variants per target:\n",
    "\n",
    "\n",
    "\n",
    "- `model_full_<target>.joblib`: trained on train+validation (use this if you want the strongest raw discriminator without isotonic calibration or if you later wish to recalibrate on a fresh hold-out).\n",
    "\n",
    "- `model_<target>.joblib` + `isotonic_<target>.joblib`: calibration pair. The base model was trained only on the training split; isotonic regression was fit on the validation split to avoid using the same samples twice (avoids optimistic calibration). For inference with calibrated probabilities, load both and apply: `calibrated = isotonic.transform(base_model.predict_proba(X)[:,1])` then threshold using `threshold_<target>.txt`.\n",
    "\n",
    "\n",
    "\n",
    "The reported test metrics come from the calibrated base model (not the full model) to reflect honest calibration performance. Both variants are provided; choose according to deployment needs. An `artifact_index.json` enumerates absolute paths for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9caa9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "# Combined metrics + artifact index\n",
    "with open(ARTIFACTS_DIR / 'metrics_all.json','w') as f: json.dump(metrics_all, f, indent=2)\n",
    "artifact_index = {}\n",
    "for tgt_name,_ in TARGET_SPECS:\n",
    "    artifact_index[tgt_name] = {\n",
    "        'final_model': str((ARTIFACTS_DIR / f'model_full_{tgt_name}.joblib').resolve()),\n",
    "        'base_model': str((ARTIFACTS_DIR / f'model_{tgt_name}.joblib').resolve()),\n",
    "        'isotonic': str((ARTIFACTS_DIR / f'isotonic_{tgt_name}.joblib').resolve()),\n",
    "        'metrics': str((ARTIFACTS_DIR / f'metrics_{tgt_name}.json').resolve()),\n",
    "        'threshold': str((ARTIFACTS_DIR / f'threshold_{tgt_name}.txt').resolve()),\n",
    "        'best_params': str((ARTIFACTS_DIR / f'best_params_{tgt_name}.json').resolve())\n",
    "    }\n",
    "with open(ARTIFACTS_DIR / 'artifact_index.json','w') as f: json.dump(artifact_index, f, indent=2)\n",
    "print('All target metrics written ->', (ARTIFACTS_DIR / 'metrics_all.json').resolve())\n",
    "print('Artifact index written ->', (ARTIFACTS_DIR / 'artifact_index.json').resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d964627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize saved artifacts for all targets\n",
    "import json, os\n",
    "artifact_summary = {}\n",
    "for tgt_name, _ in TARGET_SPECS:\n",
    "    tgt_files = [f for f in os.listdir(ARTIFACTS_DIR) if f.startswith(('model_'+tgt_name,'isotonic_'+tgt_name,'metrics_'+tgt_name,'threshold_'+tgt_name,'best_params_'+tgt_name))]\n",
    "    artifact_summary[tgt_name] = tgt_files\n",
    "print(json.dumps(artifact_summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5bedf",
   "metadata": {},
   "source": [
    "### Inspect Trials\n",
    "Overview of trials and (optional) optimization history plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16676a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC & PR curves (calibrated) for all targets\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc as sk_auc\n",
    "from joblib import load as jobload\n",
    "\n",
    "roc_data = {}\n",
    "pr_data = {}\n",
    "for tgt_name, label_col in TARGET_SPECS:\n",
    "    model_path = ARTIFACTS_DIR / f'model_{tgt_name}.joblib'\n",
    "    iso_path = ARTIFACTS_DIR / f'isotonic_{tgt_name}.joblib'\n",
    "    if not (model_path.exists() and iso_path.exists()):\n",
    "        print('Missing artifacts for', tgt_name)\n",
    "        continue\n",
    "    model = jobload(model_path)\n",
    "    iso = jobload(iso_path)\n",
    "    y_all = labels_df[label_col].astype(int).to_numpy()\n",
    "    y_te_t = y_all[idx_te]\n",
    "    raw = model.predict_proba(X_te)[:,1]\n",
    "    cal = iso.transform(raw)\n",
    "    fpr,tpr,_ = roc_curve(y_te_t, cal)\n",
    "    prec,rec,_ = precision_recall_curve(y_te_t, cal)\n",
    "    roc_auc = sk_auc(fpr,tpr)\n",
    "    pr_auc = sk_auc(rec,prec)\n",
    "    roc_data[tgt_name] = (fpr,tpr,roc_auc)\n",
    "    pr_data[tgt_name] = (rec,prec,pr_auc)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "for tgt,(fpr,tpr,roc_auc) in roc_data.items():\n",
    "    axes[0].plot(fpr,tpr,label=f\"{tgt} AUC={roc_auc:.3f}\")\n",
    "axes[0].plot([0,1],[0,1],'--',color='grey'); axes[0].set_title('ROC (Calibrated)'); axes[0].legend(); axes[0].set_xlabel('FPR'); axes[0].set_ylabel('TPR')\n",
    "for tgt,(rec,prec,pr_auc) in pr_data.items():\n",
    "    axes[1].plot(rec,prec,label=f\"{tgt} PR AUC={pr_auc:.3f}\")\n",
    "axes[1].set_title('PR (Calibrated)'); axes[1].legend(); axes[1].set_xlabel('Recall'); axes[1].set_ylabel('Precision')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b693a61",
   "metadata": {},
   "source": [
    "### Final Model Training\n",
    "Train final booster on combined train+validation using best params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7400eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curves for all targets (using shared_inference unified loading)\n",
    "from project.shared_inference import get_model_and_calibrator, apply_calibration, _resolve_models_dir\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "\n",
    "ARTIFACTS_DIR = _resolve_models_dir()\n",
    "\n",
    "calibration_targets = []  # (tgt_name, y_true_test, raw_probs, calibrated_probs)\n",
    "for tgt_name, label_col in TARGET_SPECS:\n",
    "    try:\n",
    "        model, calibrator = get_model_and_calibrator(tgt_name, models_dir=ARTIFACTS_DIR)\n",
    "    except FileNotFoundError:\n",
    "        print('Skip (missing artifacts):', tgt_name)\n",
    "        continue\n",
    "    y_all = labels_df[label_col].astype(int).to_numpy(); y_te_t = y_all[idx_te]\n",
    "    raw = model.predict_proba(X_te)[:,1]\n",
    "    cal = apply_calibration(raw, calibrator)\n",
    "    calibration_targets.append((tgt_name, y_te_t, raw, cal))\n",
    "\n",
    "if not calibration_targets:\n",
    "    raise RuntimeError('No targets available for calibration plotting.')\n",
    "\n",
    "fig, axes = plt.subplots(1, len(calibration_targets), figsize=(5*len(calibration_targets),4), sharey=True)\n",
    "if len(calibration_targets) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax,(tgt,y_true,raw,cal) in zip(axes, calibration_targets):\n",
    "    fr_raw, mp_raw = calibration_curve(y_true, raw, n_bins=15, strategy='quantile')\n",
    "    fr_cal, mp_cal = calibration_curve(y_true, cal, n_bins=15, strategy='quantile')\n",
    "    ax.plot(mp_raw, fr_raw, 'o-', label='Raw', alpha=0.7)\n",
    "    ax.plot(mp_cal, fr_cal, 'o-', label='Calibrated', alpha=0.7)\n",
    "    ax.plot([0,1],[0,1], '--', color='gray')\n",
    "    ax.set_title(f'Calibration: {tgt}')\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('Observed')\n",
    "    ax.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Report average absolute calibration shift per target\n",
    "for tgt, y_true, raw, cal in calibration_targets:\n",
    "    shift = float(np.mean(np.abs(raw - cal)))\n",
    "    print(f\"{tgt}: mean |raw-cal| = {shift:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432add8f",
   "metadata": {},
   "source": [
    "# SHAP top features per target (sampled)\n",
    "import numpy as np, pandas as pd\n",
    "from joblib import load as jobload\n",
    "shap_results = {}\n",
    "try:\n",
    "    import shap\n",
    "    shap.initjs()\n",
    "    SAMPLE = 400\n",
    "    for tgt_name,_ in TARGET_SPECS:\n",
    "        model_path = ARTIFACTS_DIR / f'model_{tgt_name}.joblib'\n",
    "        if not model_path.exists():\n",
    "            continue\n",
    "        model = jobload(model_path)\n",
    "        sample_idx = np.random.default_rng(42).choice(len(X_tr), size=min(SAMPLE, len(X_tr)), replace=False)\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_vals = explainer.shap_values(X_tr[sample_idx])\n",
    "        mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "        order = np.argsort(-mean_abs)[:20]\n",
    "        feat_cols = list(feature_df.columns)\n",
    "        top_df = pd.DataFrame({'feature':[feat_cols[i] for i in order], 'importance':mean_abs[order]})\n",
    "        shap_results[tgt_name] = top_df.head(10).to_dict(orient='records')\n",
    "        ax = top_df.iloc[::-1].plot(kind='barh', x='feature', y='importance', figsize=(6,5), title=f'SHAP Top 20 {tgt_name}')\n",
    "        ax.set_xlabel('Mean |SHAP|')\n",
    "        plt.tight_layout(); plt.show()\n",
    "    print('Top SHAP features (first 10 each):')\n",
    "    for k,v in shap_results.items():\n",
    "        print(k, v[:5])\n",
    "except Exception as e:\n",
    "    print('SHAP skipped:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ab5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual isotonic calibration (sklearn XGBClassifier base, no DMatrix)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import numpy as np\n",
    "\n",
    "# Base model: train-only (exclude validation for calibration fairness)\n",
    "params = study.best_params.copy()\n",
    "base_model = XGBClassifier(\n",
    "    objective='binary:logistic', tree_method='hist',\n",
    "    learning_rate=params['learning_rate'],\n",
    "    n_estimators=params['n_estimators'],\n",
    "    max_depth=params['max_depth'],\n",
    "    min_child_weight=params['min_child_weight'],\n",
    "    subsample=params['subsample'],\n",
    "    colsample_bytree=params['colsample_bytree'],\n",
    "    reg_lambda=params['lambda'],\n",
    "    reg_alpha=params['alpha'],\n",
    "    gamma=params['gamma'],\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric='logloss',\n",
    "    verbosity=0\n",
    ")\n",
    "base_model.fit(X_tr, y_tr)\n",
    "val_proba_raw = base_model.predict_proba(X_val)[:,1]\n",
    "iso = IsotonicRegression(out_of_bounds='clip')\n",
    "iso.fit(val_proba_raw, y_val)\n",
    "print('Isotonic calibration fitted on validation set.')\n",
    "\n",
    "def predict_calibrated(X):\n",
    "    return iso.transform(base_model.predict_proba(X)[:,1])\n",
    "\n",
    "# Derive operating threshold on calibrated validation probabilities\n",
    "val_cal = predict_calibrated(X_val)\n",
    "ths = np.linspace(0.01,0.9,300)\n",
    "threshold_info = None\n",
    "for t in ths:\n",
    "    m = metrics_at(val_cal, y_val, t)\n",
    "    if (threshold_info is None) or (m['f1'] > threshold_info['f1']):\n",
    "        threshold_info = {**m, 'threshold': float(t)}\n",
    "print('Selected threshold (calibrated validation):', threshold_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ffa2fb",
   "metadata": {},
   "source": [
    "### Multi-Outcome Expansion\n",
    "Train & calibrate additional models for mortality and prolonged LOS using the same feature matrix and pipeline pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified summary table: tuned vs baseline metrics per target\n",
    "import json, pandas as pd\n",
    "rows = []\n",
    "for tgt_name, _ in TARGET_SPECS:\n",
    "    metrics_path = ARTIFACTS_DIR / f'metrics_{tgt_name}.json'\n",
    "    if not metrics_path.exists():\n",
    "        continue\n",
    "    with open(metrics_path) as f:\n",
    "        m = json.load(f)\n",
    "    rows.append({\n",
    "        'target': tgt_name,\n",
    "        'auc': m['auc'],\n",
    "        'pr_auc': m['pr_auc'],\n",
    "        'brier': m['brier'],\n",
    "        'threshold': m['threshold'],\n",
    "        'f1_at_threshold': m['f1_at_threshold'],\n",
    "        'precision_at_threshold': m['precision_at_threshold'],\n",
    "        'recall_at_threshold': m['recall_at_threshold'],\n",
    "        'baseline_val_auc': baseline_results.get(tgt_name)\n",
    "    })\n",
    "summary_df = pd.DataFrame(rows)\n",
    "print(summary_df.sort_values('target'))\n",
    "summary_df.to_csv(ARTIFACTS_DIR / 'metrics_summary.csv', index=False)\n",
    "print('Wrote metrics_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42bb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare full (train+val) vs base calibrated models on test\n",
    "from joblib import load as jobload\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "comparison_rows = []\n",
    "missing_any = False\n",
    "for tgt_name, label_col in TARGET_SPECS:\n",
    "    y_all = labels_df[label_col].astype(int).to_numpy()\n",
    "    y_te_t = y_all[idx_te]\n",
    "    full_path = ARTIFACTS_DIR / f'model_full_{tgt_name}.joblib'\n",
    "    base_path = ARTIFACTS_DIR / f'model_{tgt_name}.joblib'\n",
    "    iso_path = ARTIFACTS_DIR / f'isotonic_{tgt_name}.joblib'\n",
    "    if not (full_path.exists() and base_path.exists() and iso_path.exists()):\n",
    "        print('Skipping', tgt_name, 'missing one of required artifacts')\n",
    "        missing_any = True\n",
    "        continue\n",
    "    full_model = jobload(full_path)\n",
    "    base_model = jobload(base_path)\n",
    "    iso = jobload(iso_path)\n",
    "    raw_full = full_model.predict_proba(X_te)[:,1]\n",
    "    raw_base = base_model.predict_proba(X_te)[:,1]\n",
    "    cal_base = iso.transform(raw_base)\n",
    "    row = {\n",
    "        'target': tgt_name,\n",
    "        'full_raw_auc': roc_auc_score(y_te_t, raw_full),\n",
    "        'full_raw_pr_auc': average_precision_score(y_te_t, raw_full),\n",
    "        'base_raw_auc': roc_auc_score(y_te_t, raw_base),\n",
    "        'base_cal_auc': roc_auc_score(y_te_t, cal_base),\n",
    "        'base_cal_pr_auc': average_precision_score(y_te_t, cal_base),\n",
    "        'base_cal_brier': brier_score_loss(y_te_t, cal_base)\n",
    "    }\n",
    "    comparison_rows.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "if len(comparison_df):\n",
    "    display(comparison_df.sort_values('target'))\n",
    "    comparison_df.to_csv(ARTIFACTS_DIR / 'full_vs_calibrated_comparison.csv', index=False)\n",
    "    print('Wrote full_vs_calibrated_comparison.csv')\n",
    "else:\n",
    "    print('No comparison rows generated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd5b50",
   "metadata": {},
   "source": [
    "### Full vs Calibrated Model Comparison\n",
    "Compares raw discrimination of `model_full_*` (trained on train+val) against raw and calibrated outputs of the train-only `model_*` + isotonic. This helps quantify the trade-off (if any) from using the calibration protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge comparison with metrics summary (enriched)\n",
    "import pandas as pd, json\n",
    "summary_path = ARTIFACTS_DIR / 'metrics_summary.csv'\n",
    "comp_path = ARTIFACTS_DIR / 'full_vs_calibrated_comparison.csv'\n",
    "if summary_path.exists() and comp_path.exists():\n",
    "    summary_df = pd.read_csv(summary_path)\n",
    "    comp_df = pd.read_csv(comp_path)\n",
    "    merged = summary_df.merge(comp_df, on='target', how='left')\n",
    "    merged.to_csv(ARTIFACTS_DIR / 'metrics_summary_enriched.csv', index=False)\n",
    "    print('Wrote metrics_summary_enriched.csv')\n",
    "    display(merged.sort_values('target'))\n",
    "else:\n",
    "    print('Missing one of summary or comparison CSV; skip enrichment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88049aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC Difference Significance (Bootstrap between full_raw and base_cal)\n",
    "import numpy as np, pandas as pd\n",
    "from joblib import load as jobload\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "RESULTS = []\n",
    "N_BOOT = 2000  # increase for tighter CI (runtime ~ O(N_BOOT))\n",
    "RNG = np.random.default_rng(42)\n",
    "\n",
    "for tgt_name, label_col in TARGET_SPECS:\n",
    "    full_path = ARTIFACTS_DIR / f'model_full_{tgt_name}.joblib'\n",
    "    base_path = ARTIFACTS_DIR / f'model_{tgt_name}.joblib'\n",
    "    iso_path = ARTIFACTS_DIR / f'isotonic_{tgt_name}.joblib'\n",
    "    if not (full_path.exists() and base_path.exists() and iso_path.exists()):\n",
    "        print('Skip significance for', tgt_name, '(missing artifacts)')\n",
    "        continue\n",
    "    y_all = labels_df[label_col].astype(int).to_numpy()\n",
    "    y_te_t = y_all[idx_te]\n",
    "    full_model = jobload(full_path)\n",
    "    base_model = jobload(base_path)\n",
    "    iso = jobload(iso_path)\n",
    "    raw_full = full_model.predict_proba(X_te)[:,1]\n",
    "    cal_base = iso.transform(base_model.predict_proba(X_te)[:,1])\n",
    "    auc_full = roc_auc_score(y_te_t, raw_full)\n",
    "    auc_cal = roc_auc_score(y_te_t, cal_base)\n",
    "    diff = auc_full - auc_cal\n",
    "    n = len(y_te_t)\n",
    "    # Bootstrap\n",
    "    diffs = np.empty(N_BOOT)\n",
    "    for b in range(N_BOOT):\n",
    "        idx = RNG.integers(0, n, size=n)\n",
    "        y_b = y_te_t[idx]\n",
    "        rf_b = raw_full[idx]\n",
    "        cb_b = cal_base[idx]\n",
    "        try:\n",
    "            diffs[b] = roc_auc_score(y_b, rf_b) - roc_auc_score(y_b, cb_b)\n",
    "        except ValueError:\n",
    "            # In rare case bootstrap sample has only one class\n",
    "            diffs[b] = np.nan\n",
    "    diffs = diffs[~np.isnan(diffs)]\n",
    "    if len(diffs) < N_BOOT * 0.9:\n",
    "        print('Warning: many degenerate bootstrap samples for', tgt_name)\n",
    "    lower, upper = np.percentile(diffs, [2.5, 97.5])\n",
    "    # Two-sided p-value: proportion of bootstrap diffs with opposite sign or more extreme\n",
    "    if diff >= 0:\n",
    "        p = (np.sum(diffs <= 0) + 1) / (len(diffs) + 1)\n",
    "    else:\n",
    "        p = (np.sum(diffs >= 0) + 1) / (len(diffs) + 1)\n",
    "    RESULTS.append({\n",
    "        'target': tgt_name,\n",
    "        'auc_full_raw': auc_full,\n",
    "        'auc_base_cal': auc_cal,\n",
    "        'diff_full_minus_cal': diff,\n",
    "        'ci_95_lower': lower,\n",
    "        'ci_95_upper': upper,\n",
    "        'approx_p_two_sided': p * 2 if p * 2 <= 1 else 1.0,\n",
    "        'n_test': n,\n",
    "        'n_boot_effective': int(len(diffs))\n",
    "    })\n",
    "\n",
    "if RESULTS:\n",
    "    sig_df = pd.DataFrame(RESULTS).sort_values('target')\n",
    "    display(sig_df)\n",
    "    sig_df.to_csv(ARTIFACTS_DIR / 'auc_diff_significance.csv', index=False)\n",
    "    print('Wrote auc_diff_significance.csv')\n",
    "else:\n",
    "    print('No targets processed for significance analysis.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df132db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leakage Audit: heuristic checks for potential target leakage across features\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('\\n=== Leakage Audit Start ===')\n",
    "\n",
    "# 1. Basic presence checks\n",
    "if 'feature_df' not in globals():\n",
    "    raise RuntimeError('feature_df not found in notebook namespace.')\n",
    "\n",
    "suspicious_substrings = [\n",
    "    'los', 'readmit', 'readmission', 'mort', 'death', 'dod', 'deathtime',\n",
    "    'discharge', 'outcome', 'label', 'future'\n",
    "]\n",
    "pattern = re.compile('|'.join([re.escape(s) for s in suspicious_substrings]), re.IGNORECASE)\n",
    "\n",
    "all_features = list(feature_df.columns)\n",
    "matched_features = [f for f in all_features if pattern.search(f)]\n",
    "print(f'Total features: {len(all_features)} | Suspicious-matched: {len(matched_features)}')\n",
    "\n",
    "# Exclude obviously safe tokens that create false positives (tune if needed)\n",
    "false_positive_tokens = {'mortality_risk_score_placeholder'}\n",
    "matched_features = [f for f in matched_features if f.lower() not in false_positive_tokens]\n",
    "\n",
    "print('Sample suspicious feature names (up to 25):')\n",
    "for name in matched_features[:25]:\n",
    "    print('  -', name)\n",
    "\n",
    "# 2. Per-target single-feature AUCs (very high AUC alone may indicate leakage)\n",
    "per_target_auc = {}\n",
    "if 'labels_df' in globals():\n",
    "    for tgt_name, label_col in [('readmission','readmission_label'), ('mortality','mortality_label'), ('prolonged_los','prolonged_los_label')]:\n",
    "        if label_col not in labels_df.columns:  # skip missing targets\n",
    "            continue\n",
    "        y = labels_df.set_index('subject_id')[label_col].reindex(feature_df.index).astype(float).values\n",
    "        auc_rows = []\n",
    "        for f in matched_features:\n",
    "            x = feature_df[f].values\n",
    "            # Skip constant\n",
    "            if np.nanstd(x) < 1e-9:\n",
    "                continue\n",
    "            try:\n",
    "                # For continuous feature use raw values; sklearn will treat >2 class distribution\n",
    "                auc = roc_auc_score(y, x)\n",
    "                auc_rows.append((f, auc))\n",
    "            except ValueError:\n",
    "                # Probably only one class present in y subset or feature has too few distinct values\n",
    "                continue\n",
    "        # Sort by AUC descending\n",
    "        auc_rows.sort(key=lambda r: r[1], reverse=True)\n",
    "        per_target_auc[tgt_name] = auc_rows[:20]\n",
    "\n",
    "print('\\nTop suspicious-feature single-variable AUCs (first 20 per target):')\n",
    "for tgt, rows in per_target_auc.items():\n",
    "    if not rows:\n",
    "        print(f'  {tgt}: (no evaluable suspicious features)')\n",
    "        continue\n",
    "    print(f'  {tgt}:')\n",
    "    for f, auc in rows:\n",
    "        flag = ' **LEAK?**' if auc >= 0.9 else ''\n",
    "        print(f'    {f}: AUC={auc:.3f}{flag}')\n",
    "\n",
    "# 3. Variance & near-deterministic checks for prolonged LOS leakage (features containing \"los\")\n",
    "los_feats = [f for f in matched_features if 'los' in f.lower()]\n",
    "los_flags = []\n",
    "for f in los_feats:\n",
    "    x = feature_df[f].values\n",
    "    unique_vals = np.unique(x[~pd.isna(x)])\n",
    "    if len(unique_vals) <= 2:\n",
    "        # Binary LOS-derived feature: check overlap with target\n",
    "        if 'prolonged_los_label' in labels_df.columns:\n",
    "            y_pl = labels_df.set_index('subject_id')['prolonged_los_label'].reindex(feature_df.index).values\n",
    "            try:\n",
    "                auc_bin = roc_auc_score(y_pl, x)\n",
    "            except ValueError:\n",
    "                auc_bin = float('nan')\n",
    "            if (np.array_equal(x, y_pl) or np.array_equal(1 - x, y_pl)):\n",
    "                los_flags.append((f, 'IDENTICAL_TO_TARGET'))\n",
    "            elif auc_bin >= 0.9:\n",
    "                los_flags.append((f, f'HIGH_AUC_{auc_bin:.3f}'))\n",
    "    else:\n",
    "        # Continuous LOS-like feature: if max value >> 48 early-window hours, could leak realized LOS\n",
    "        if np.nanmax(x) > 72:  # heuristic threshold (48h window + slack)\n",
    "            los_flags.append((f, f'MAX_GT_72({np.nanmax(x):.1f})'))\n",
    "\n",
    "if los_flags:\n",
    "    print('\\nPotential LOS leakage flags:')\n",
    "    for f, reason in los_flags:\n",
    "        print(f'  - {f}: {reason}')\n",
    "else:\n",
    "    print('\\nNo LOS leakage heuristics triggered.')\n",
    "\n",
    "# 4. Remove known non-leak columns from matched list if we want to persist a clean report (optional)\n",
    "report = {\n",
    "    'n_total_features': len(all_features),\n",
    "    'n_suspicious_matched': len(matched_features),\n",
    "    'top_single_feature_auc_per_target': {\n",
    "        tgt: [{'feature': f, 'auc': float(a)} for f,a in rows] for tgt, rows in per_target_auc.items()\n",
    "    },\n",
    "    'los_flags': [{'feature': f, 'reason': r} for f,r in los_flags],\n",
    "    'suspicious_sample': matched_features[:50]\n",
    "}\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'leakage_audit.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print('\\nLeakage audit report written to', (ARTIFACTS_DIR / 'leakage_audit.json').resolve())\n",
    "print('=== Leakage Audit Complete ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f4bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended Feature Importance & Suspicious Feature Surfacing\n",
    "import json, re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from joblib import load as jobload\n",
    "\n",
    "print('\\n=== Feature Importance & Suspicion Scan ===')\n",
    "if 'feature_df' not in globals():\n",
    "    raise RuntimeError('feature_df not in scope; run earlier cells first.')\n",
    "\n",
    "# Map f0..fN to actual column names (training used raw numpy arrays)\n",
    "feature_names = list(feature_df.columns)\n",
    "name_map = {f'f{i}': feature_names[i] for i in range(len(feature_names))}\n",
    "\n",
    "importance_rows = []\n",
    "for tgt_name, _lbl in [('readmission','readmission_label'), ('mortality','mortality_label'), ('prolonged_los','prolonged_los_label')]:\n",
    "    model_path = ARTIFACTS_DIR / f'model_{tgt_name}.joblib'  # calibrated base model (train-only)\n",
    "    if not model_path.exists():\n",
    "        print(f'Skip {tgt_name}: model file missing -> {model_path}')\n",
    "        continue\n",
    "    model = jobload(model_path)\n",
    "    try:\n",
    "        booster = model.get_booster()\n",
    "    except Exception as e:\n",
    "        print(f'Skip {tgt_name}: cannot extract booster ({e})')\n",
    "        continue\n",
    "    gain_scores = booster.get_score(importance_type='gain')  # dict fidx -> gain\n",
    "    total_gain = sum(gain_scores.values()) or 1.0\n",
    "    for f_idx, gain in gain_scores.items():\n",
    "        col_name = name_map.get(f_idx, f_idx)\n",
    "        importance_rows.append({\n",
    "            'target': tgt_name,\n",
    "            'xgb_feature': f_idx,\n",
    "            'feature_name': col_name,\n",
    "            'gain': gain,\n",
    "            'gain_frac': gain / total_gain\n",
    "        })\n",
    "\n",
    "imp_df = pd.DataFrame(importance_rows)\n",
    "if not imp_df.empty:\n",
    "    imp_df['rank_within_target'] = imp_df.groupby('target')['gain'].rank(ascending=False, method='first')\n",
    "    imp_df_sorted = imp_df.sort_values(['target','rank_within_target'])\n",
    "    imp_out_csv = ARTIFACTS_DIR / 'feature_importance_gain.csv'\n",
    "    imp_df_sorted.to_csv(imp_out_csv, index=False)\n",
    "    print('Wrote feature importance CSV ->', imp_out_csv.resolve())\n",
    "else:\n",
    "    print('No importance data collected.')\n",
    "\n",
    "# Suspicion scan (reuse pattern similar to leakage audit)\n",
    "suspicious_substrings = [ 'los', 'readmit', 'readmission', 'mort', 'death', 'dod', 'discharge', 'outcome', 'label', 'future' ]\n",
    "pattern = re.compile('|'.join([re.escape(s) for s in suspicious_substrings]), re.IGNORECASE)\n",
    "imp_df_sorted['suspicious_name_match'] = imp_df_sorted['feature_name'].apply(lambda x: bool(pattern.search(str(x)))) if not imp_df.empty else []\n",
    "\n",
    "# Flag features that are BOTH suspicious name matches AND high relative importance (top 5% by gain within target)\n",
    "flags = []\n",
    "if not imp_df.empty:\n",
    "    for tgt, grp in imp_df_sorted.groupby('target'):\n",
    "        cutoff_rank = max(1, int(len(grp) * 0.05))\n",
    "        top_grp = grp.nsmallest(cutoff_rank, 'rank_within_target')\n",
    "        flagged = top_grp[top_grp['suspicious_name_match']]\n",
    "        for _, row in flagged.iterrows():\n",
    "            flags.append({\n",
    "                'target': tgt,\n",
    "                'feature_name': row['feature_name'],\n",
    "                'gain_frac': float(row['gain_frac']),\n",
    "                'rank': int(row['rank_within_target'])\n",
    "            })\n",
    "\n",
    "report = {\n",
    "    'n_features': len(feature_names),\n",
    "    'n_importance_rows': int(len(imp_df)),\n",
    "    'suspicion_flags': flags,\n",
    "    'suspicious_name_pattern': suspicious_substrings,\n",
    "}\n",
    "with open(ARTIFACTS_DIR / 'feature_importance_suspicious.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print('Suspicion report written ->', (ARTIFACTS_DIR / 'feature_importance_suspicious.json').resolve())\n",
    "\n",
    "# Display concise summary\n",
    "if flags:\n",
    "    print('\\nHigh-importance suspicious feature candidates:')\n",
    "    for f in flags[:40]:\n",
    "        print(f\"  [{f['target']}] {f['feature_name']} | gain_frac={f['gain_frac']:.4f} | rank={f['rank']}\")\n",
    "else:\n",
    "    print('No high-importance suspicious feature names flagged.')\n",
    "print('=== Feature Importance Scan Complete ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Feature Importances (Top 20 per target) with Suspicion Highlight\n",
    "import pandas as pd, json, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "imp_csv = ARTIFACTS_DIR / 'feature_importance_gain.csv'\n",
    "flags_json = ARTIFACTS_DIR / 'feature_importance_suspicious.json'\n",
    "\n",
    "if not imp_csv.exists():\n",
    "    raise FileNotFoundError(f'Missing importance file: {imp_csv}. Run previous cell first.')\n",
    "\n",
    "imp_df = pd.read_csv(imp_csv)\n",
    "flagged = set()\n",
    "if flags_json.exists():\n",
    "    try:\n",
    "        with open(flags_json) as f:\n",
    "            rep = json.load(f)\n",
    "        for item in rep.get('suspicion_flags', []):\n",
    "            flagged.add((item['target'], item['feature_name']))\n",
    "    except Exception as e:\n",
    "        print('Could not parse suspicion flags JSON:', e)\n",
    "\n",
    "# Ensure consistent ordering by gain within each target\n",
    "imp_df['gain_rank'] = imp_df.groupby('target')['gain'].rank(ascending=False, method='first')\n",
    "\n",
    "TARGETS = sorted(imp_df['target'].unique())\n",
    "fig_rows = len(TARGETS)\n",
    "fig, axes = plt.subplots(fig_rows, 1, figsize=(10, 4*fig_rows), constrained_layout=True)\n",
    "if fig_rows == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "img_paths = []\n",
    "for ax, tgt in zip(axes, TARGETS):\n",
    "    subset = imp_df[imp_df['target']==tgt].nsmallest(20, 'gain_rank')\n",
    "    subset = subset.sort_values('gain', ascending=True)  # for horizontal bar\n",
    "    colors = ['crimson' if (tgt, fn) in flagged else 'steelblue' for fn in subset['feature_name']]\n",
    "    ax.barh(subset['feature_name'], subset['gain'], color=colors, alpha=0.85)\n",
    "    ax.set_title(f'Top 20 Gain Features: {tgt}')\n",
    "    ax.set_xlabel('Gain')\n",
    "    for i,(g,f) in enumerate(zip(subset['gain'], subset['feature_name'])):\n",
    "        ax.text(g*1.01, i, f'{g:.2f}', va='center', fontsize=8)\n",
    "    # Legend once\n",
    "    if any(c=='crimson' for c in colors):\n",
    "        from matplotlib.lines import Line2D\n",
    "        legend_elems = [Line2D([0],[0], color='steelblue', lw=6, label='Normal'), Line2D([0],[0], color='crimson', lw=6, label='Suspicious')]\n",
    "        ax.legend(handles=legend_elems, loc='lower right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save per-target PNGs\n",
    "for tgt in TARGETS:\n",
    "    fig_t, ax_t = plt.subplots(figsize=(6,6))\n",
    "    subset = imp_df[imp_df['target']==tgt].nsmallest(20, 'gain_rank')\n",
    "    subset = subset.sort_values('gain', ascending=True)\n",
    "    colors = ['crimson' if (tgt, fn) in flagged else 'steelblue' for fn in subset['feature_name']]\n",
    "    ax_t.barh(subset['feature_name'], subset['gain'], color=colors, alpha=0.9)\n",
    "    ax_t.set_title(f'Top 20 Gain Features: {tgt}')\n",
    "    ax_t.set_xlabel('Gain')\n",
    "    plt.tight_layout()\n",
    "    out_path = ARTIFACTS_DIR / f'feature_importance_top20_{tgt}.png'\n",
    "    fig_t.savefig(out_path, dpi=150)\n",
    "    img_paths.append(out_path)\n",
    "    plt.close(fig_t)\n",
    "\n",
    "print('Saved feature importance plots:')\n",
    "for p in img_paths:\n",
    "    print('  -', p.resolve())\n",
    "\n",
    "# Display flagged suspicious features table (if any)\n",
    "if flagged:\n",
    "    flagged_rows = []\n",
    "    for tgt, fn in flagged:\n",
    "        row = imp_df[(imp_df['target']==tgt) & (imp_df['feature_name']==fn)]\n",
    "        if not row.empty:\n",
    "            flagged_rows.append(row.iloc[0])\n",
    "    if flagged_rows:\n",
    "        flagged_df = pd.DataFrame(flagged_rows).sort_values(['target','gain_rank'])\n",
    "        display(flagged_df[['target','feature_name','gain','gain_frac','gain_rank']])\n",
    "    else:\n",
    "        print('No flagged features present in top importance list (flags may refer to excluded ranks).')\n",
    "else:\n",
    "    print('No suspicious high-importance features flagged in prior step.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw vs Schema-Aligned Variance Diagnostics (updated to avoid undefined DATA_DIR)\n",
    "import json, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure artifact_dir exists (set in earlier setup cell)\n",
    "try:\n",
    "    artifact_dir\n",
    "except NameError:\n",
    "    raise RuntimeError('artifact_dir not defined; run setup cell first.')\n",
    "\n",
    "feat_cols_path = artifact_dir / 'feature_columns.json'\n",
    "if not feat_cols_path.exists():\n",
    "    raise FileNotFoundError(f'Missing feature_columns.json at {feat_cols_path}')\n",
    "\n",
    "with open(feat_cols_path, 'r') as f:\n",
    "    model_feature_order = json.load(f)\n",
    "print(f'Model schema feature count: {len(model_feature_order)}')\n",
    "\n",
    "# Prefer already loaded pruned_features_df; raw_features_df likely unavailable\n",
    "raw_df = raw_features_df if 'raw_features_df' in globals() else None\n",
    "pruned_df = pruned_features_df if 'pruned_features_df' in globals() else None\n",
    "\n",
    "# Helper\n",
    "import math\n",
    "\n",
    "def describe_variance(df: pd.DataFrame, label: str):\n",
    "    if df is None or df.empty:\n",
    "        print(f'{label}: empty')\n",
    "        return None\n",
    "    numeric = df.select_dtypes(include=[np.number])\n",
    "    if numeric.empty:\n",
    "        print(f'{label}: no numeric columns')\n",
    "        return None\n",
    "    variances = numeric.var(axis=0, ddof=0)\n",
    "    zero_var = (variances == 0).sum()\n",
    "    frac_variable = 1 - zero_var / len(variances)\n",
    "    print(f'{label}: cols={len(variances)} variable={len(variances)-zero_var} ({frac_variable:.2%}) zero_var={zero_var}')\n",
    "    print('  variance quantiles:')\n",
    "    print(variances.quantile([0,0.01,0.05,0.25,0.5,0.75,0.95,0.99,1]))\n",
    "    return variances\n",
    "\n",
    "raw_var = describe_variance(raw_df, 'RAW')\n",
    "pruned_var = describe_variance(pruned_df, 'PRUNED')\n",
    "\n",
    "aligned_df = None\n",
    "if pruned_df is not None and not pruned_df.empty:\n",
    "    aligned_df = pd.DataFrame(index=pruned_df.index)\n",
    "    for col in model_feature_order:\n",
    "        if col in pruned_df.columns:\n",
    "            aligned_df[col] = pruned_df[col]\n",
    "        else:\n",
    "            aligned_df[col] = 0.0\n",
    "    aligned_var = describe_variance(aligned_df, 'ALIGNED')\n",
    "else:\n",
    "    print('Skipping aligned variance (no pruned_df)')\n",
    "\n",
    "if pruned_df is not None and not pruned_df.empty:\n",
    "    pruned_cols = set(pruned_df.columns)\n",
    "    model_cols = set(model_feature_order)\n",
    "    inter = pruned_cols & model_cols\n",
    "    jaccard = len(inter) / len(pruned_cols | model_cols) if (pruned_cols | model_cols) else 1.0\n",
    "    print(f'Schema overlap: pruned={len(pruned_cols)} model={len(model_cols)} intersection={len(inter)} jaccard={jaccard:.4f}')\n",
    "    if jaccard < 0.95:\n",
    "        print('[ALERT] Low Jaccard similarity -> stale model schema likely.')\n",
    "    missing_in_pruned = [c for c in model_feature_order if c not in pruned_cols]\n",
    "    if missing_in_pruned:\n",
    "        print(f'Model expects {len(missing_in_pruned)} missing columns (first 10): {missing_in_pruned[:10]}')\n",
    "\n",
    "if aligned_df is not None:\n",
    "    variable_frac = (aligned_df.var(axis=0, ddof=0) > 0).mean()\n",
    "    if variable_frac < 0.10:\n",
    "        print(f'[ALERT] Aligned matrix variable fraction only {variable_frac:.2%} (would trigger scoring guard).')\n",
    "    else:\n",
    "        print(f'Aligned matrix variable fraction healthy: {variable_frac:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db941ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust setup for variance diagnostics\n",
    "import os, json, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Attempt to find feature_columns.json anywhere under repo (bounded search)\n",
    "search_roots = [Path('.'), Path('..'), Path('../..')]\n",
    "found_paths = []\n",
    "for root in search_roots:\n",
    "    try:\n",
    "        for p in root.glob('**/feature_columns.json'):\n",
    "            # Skip virtual env / hidden dirs if any\n",
    "            parts = {part.lower() for part in p.parts}\n",
    "            if any(x in parts for x in ['.venv', 'venv', '.git']):\n",
    "                continue\n",
    "            found_paths.append(p)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not found_paths:\n",
    "    raise FileNotFoundError('feature_columns.json not found in search roots.')\n",
    "\n",
    "# Prefer one inside project/models if present\n",
    "chosen = None\n",
    "for p in found_paths:\n",
    "    if 'project' in [q.lower() for q in p.parts] and 'models' in [q.lower() for q in p.parts]:\n",
    "        chosen = p\n",
    "        break\n",
    "if chosen is None:\n",
    "    chosen = found_paths[0]\n",
    "\n",
    "artifact_dir = chosen.parent\n",
    "print('Using artifact_dir =', artifact_dir)\n",
    "\n",
    "# Load pruned feature matrix if available\n",
    "pruned_features_df = None\n",
    "full_parquet = artifact_dir / 'features_full.parquet'\n",
    "if full_parquet.exists():\n",
    "    try:\n",
    "        pruned_features_df = pd.read_parquet(full_parquet)\n",
    "        print('Loaded pruned_features_df shape:', pruned_features_df.shape)\n",
    "    except Exception as e:\n",
    "        print('Failed loading features_full.parquet:', e)\n",
    "else:\n",
    "    print('features_full.parquet not found; proceeding without pruned_features_df.')\n",
    "\n",
    "raw_features_df = None  # Usually not persisted; left as None unless previously defined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
