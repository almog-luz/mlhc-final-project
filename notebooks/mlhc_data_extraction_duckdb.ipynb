{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c371067",
   "metadata": {},
   "source": [
    "# Data Extraction (DuckDB)\n",
    "\n",
    "Objective: materialize cohort and 48h modality slices required for feature engineering.\n",
    "\n",
    "## Workflow\n",
    "1. Configure database path and connectors\n",
    "2. Load or derive subject ID cohort\n",
    "3. Extract first admissions (LOS >=54h filter inline)\n",
    "4. Extract all admissions (for readmission logic if needed later)\n",
    "5. Pull demographics\n",
    "6. Pull 0â€“48h vitals, labs, prescriptions, procedures\n",
    "\n",
    "## Notes\n",
    "- All timing windows use hour deltas from admission start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe53f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment & Imports (local)\n",
    "import os, sys, pathlib\n",
    "from importlib.util import find_spec\n",
    "\n",
    "# Add project root to path if notebook is under notebooks/\n",
    "ROOT = pathlib.Path(__file__).resolve().parents[1] if '__file__' in globals() else pathlib.Path.cwd().parents[0]\n",
    "if str(ROOT) not in sys.path: sys.path.insert(0, str(ROOT))\n",
    "\n",
    "missing = []\n",
    "for pkg in ['duckdb','pandas','matplotlib']:\n",
    "    if find_spec(pkg) is None: missing.append(pkg)\n",
    "if missing:\n",
    "    print('Missing packages detected:', missing)\n",
    "    print('Install them (example): pip install ' + ' '.join(missing))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import duckdb\n",
    "except ModuleNotFoundError:\n",
    "    raise RuntimeError('duckdb not installed. Please install and re-run.')\n",
    "\n",
    "print('Imports ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fde18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & configuration\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "import duckdb, json, math, sys, datetime\n",
    "from importlib import reload\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'project' / 'artifacts'\n",
    "DB_PATH = DATA_DIR / 'mimiciii.duckdb'  # Adjust if your filename differs\n",
    "SUBJECT_SAMPLE_LIMIT = None  # e.g., 500 for a quick run; set None for full\n",
    "RANDOM_SEED = 42\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('DuckDB path exists:', DB_PATH.exists())\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fdb89d",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Initialize paths and imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62da08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_path = DATA_DIR / 'initial_cohort.csv'\n",
    "if not cohort_path.exists():\n",
    "    raise FileNotFoundError(f'Missing cohort file: {cohort_path}')\n",
    "cohort = pd.read_csv(cohort_path)\n",
    "if 'subject_id' not in cohort.columns:\n",
    "    raise ValueError('`subject_id` column missing in cohort CSV')\n",
    "cohort = cohort.dropna(subset=['subject_id'])\n",
    "cohort['subject_id'] = cohort['subject_id'].astype(int)\n",
    "if SUBJECT_SAMPLE_LIMIT is not None:\n",
    "    cohort = cohort.sample(n=min(SUBJECT_SAMPLE_LIMIT, len(cohort)), random_state=RANDOM_SEED)\n",
    "print('Cohort size (after optional sample):', len(cohort))\n",
    "cohort.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccbbfce",
   "metadata": {},
   "source": [
    "## 2. First Admissions\n",
    "Retrieve first admission rows per subject and compute LOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(str(DB_PATH))\n",
    "print('Connected to DuckDB.')\n",
    "# (Optional) list tables to verify schema\n",
    "try:\n",
    "    tbls = con.execute(\"SHOW TABLES\").fetchdf()\n",
    "    print('Available tables:', tbls['name'].tolist())\n",
    "except Exception as e:\n",
    "    print('Could not list tables:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d57b2c",
   "metadata": {},
   "source": [
    "## 3. All Admissions Reference\n",
    "Load all admissions for downstream readmission logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified extraction + feature build (replaces manual admissions + modality cells)\n",
    "import importlib\n",
    "import project.pipelines as pc\n",
    "pc = importlib.reload(pc)\n",
    "from project.pipelines import run_training_side_pipeline\n",
    "\n",
    "subject_ids_all = cohort['subject_id'].astype(int).tolist()\n",
    "print('Subjects in cohort (pre optional sample):', len(subject_ids_all))\n",
    "\n",
    "debug = True\n",
    "\n",
    "features, labels, dbg = run_training_side_pipeline(\n",
    "        con,\n",
    "        cohort_subject_ids=subject_ids_all,\n",
    "        debug=True,\n",
    "    )\n",
    "\n",
    "if debug:\n",
    "    print('--- Pipeline Debug Stats ---')\n",
    "    for k, v in dbg.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "print('Unified features shape:', features.shape)\n",
    "features.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769be70b",
   "metadata": {},
   "source": [
    "## 4. Demographics\n",
    "Pull patient-level gender, DOB, death date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist artifacts from unified feature matrix with schema guard (no mode flags)\n",
    "import json, math\n",
    "if features is None or features.empty:\n",
    "    raise RuntimeError('No features produced; cannot persist artifacts.')\n",
    "\n",
    "# Pre-persist variance guard (stronger than downstream):\n",
    "_nuniq = features.nunique(dropna=True)\n",
    "_zero_var = (_nuniq <= 1)\n",
    "zero_var_count = int(_zero_var.sum())\n",
    "zero_frac = zero_var_count / len(_nuniq) if len(_nuniq) else 0\n",
    "print(f'[pre-persist-guard] columns={len(_nuniq)} zero/constant={zero_var_count} ({zero_frac:.2%})')\n",
    "if zero_var_count == len(_nuniq):\n",
    "    raise RuntimeError('Refusing to persist: all features are constant (flat matrix).')\n",
    "if zero_frac > 0.50:\n",
    "    raise RuntimeError(f'Refusing to persist: >50% ({zero_frac:.1%}) of columns constant; investigate extraction/pruning path.')\n",
    "\n",
    "feat_path = ARTIFACTS_DIR / 'features_full.parquet'\n",
    "prov_path = ARTIFACTS_DIR / 'feature_provenance.json'\n",
    "cols_path = ARTIFACTS_DIR / 'feature_columns.json'\n",
    "legacy_cols_path = (PROJECT_ROOT / 'project' / 'artifacts2' / 'feature_columns.json')\n",
    "\n",
    "from project.features import build_feature_provenance\n",
    "\n",
    "prev_cols = None\n",
    "if cols_path.exists():\n",
    "    try:\n",
    "        prev_cols = json.loads(cols_path.read_text())\n",
    "    except Exception:\n",
    "        prev_cols = None\n",
    "legacy_cols = None\n",
    "if legacy_cols_path.exists():\n",
    "    try:\n",
    "        legacy_cols = json.loads(legacy_cols_path.read_text())\n",
    "    except Exception:\n",
    "        legacy_cols = None\n",
    "\n",
    "current_cols = list(features.columns)\n",
    "print(f'Current feature count: {len(current_cols)}')\n",
    "if legacy_cols is not None:\n",
    "    inter = len(set(current_cols) & set(legacy_cols))\n",
    "    jacc = inter / len(set(current_cols) | set(legacy_cols))\n",
    "    print(f'Legacy comparison -> intersection={inter} jaccard={jacc:.4f} legacy_count={len(legacy_cols)}')\n",
    "    if len(current_cols) < 0.9 * len(legacy_cols):\n",
    "        raise RuntimeError('Refusing to overwrite artifacts: feature schema size <90% of legacy (bug likely).')\n",
    "\n",
    "numeric_only = [c for c in current_cols if str(c).isdigit()]\n",
    "if numeric_only:\n",
    "    print('Dropping numeric-only columns:', numeric_only)\n",
    "    features = features.drop(columns=numeric_only, errors='ignore')\n",
    "    current_cols = [c for c in current_cols if c not in numeric_only]\n",
    "\n",
    "# Final variance snapshot after any drops\n",
    "_nuniq2 = features.nunique(dropna=True)\n",
    "print(f'[pre-persist-guard] post-clean constant columns: {int((_nuniq2<=1).sum())} / {len(_nuniq2)}')\n",
    "\n",
    "features.to_parquet(feat_path)\n",
    "prov = build_feature_provenance(features)\n",
    "prov_path.write_text(json.dumps(prov, indent=2))\n",
    "cols_path.write_text(json.dumps(current_cols, indent=2))\n",
    "print('Saved:')\n",
    "for p in [feat_path, prov_path, cols_path]:\n",
    "    print('  -', p.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist labels (parallels feature artifact persistence)\n",
    "if labels is None or labels.empty:\n",
    "    raise RuntimeError('No labels dataframe returned from pipeline; cannot persist.')\n",
    "\n",
    "labels_path = ARTIFACTS_DIR / 'labels.csv'\n",
    "labels.to_csv(labels_path, index=False)\n",
    "print('Saved labels to:', labels_path.resolve())\n",
    "\n",
    "# If debug metadata about labels was returned, persist it\n",
    "if isinstance(dbg, dict) and 'labels_meta' in dbg:\n",
    "    import json\n",
    "    meta_path = ARTIFACTS_DIR / 'labels_meta.json'\n",
    "    try:\n",
    "        meta_path.write_text(json.dumps(dbg['labels_meta'], indent=2))\n",
    "        print('Saved labels meta to:', meta_path.resolve())\n",
    "    except Exception as e:\n",
    "        print('WARNING: could not persist labels_meta.json ->', e)\n",
    "\n",
    "# Quick prevalence / integrity summary\n",
    "id_cols = {'subject_id','hadm_id'}\n",
    "print('Label columns summary:')\n",
    "for col in labels.columns:\n",
    "    if col in id_cols:\n",
    "        continue\n",
    "    series = labels[col]\n",
    "    # Binary prevalence if values are 0/1\n",
    "    if series.dropna().isin([0,1]).all():\n",
    "        prev = float(series.mean()) if len(series) else float('nan')\n",
    "        print(f'  {col}: prevalence={prev:.4f} (n={len(series)})')\n",
    "    else:\n",
    "        print(f'  {col}: non-binary or mixed values; describe -> min={series.min()} max={series.max()} nonnull={series.notna().sum()}')\n",
    "print('Labels persistence complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc4ff39",
   "metadata": {},
   "source": [
    "## 5. Summary Snapshots\n",
    "Preview shape and sample rows for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature schema parity check: ensure saved feature_columns.json matches in-memory ordering\n",
    "import json\n",
    "schema_path = ARTIFACTS_DIR / 'feature_columns.json'\n",
    "if schema_path.exists():\n",
    "    saved_cols = json.loads(schema_path.read_text())\n",
    "    if list(features.columns) != saved_cols:\n",
    "        raise AssertionError('Feature column ordering drift detected between in-memory features and saved schema.')\n",
    "    else:\n",
    "        print('Parity check passed: feature column order matches saved schema.')\n",
    "else:\n",
    "    print('WARNING: feature_columns.json missing; parity check skipped.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
